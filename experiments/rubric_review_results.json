{
  "experiment_id": "rubric_module_production_review-20251026T062037",
  "config": {
    "name": "rubric_module_production_review",
    "workflow": "code_review",
    "variables": [
      {
        "name": "temperature",
        "level_1": 0.3,
        "level_2": 0.6
      },
      {
        "name": "model",
        "level_1": "openrouter/anthropic/claude-haiku-4.5",
        "level_2": "openrouter/anthropic/claude-sonnet-4.5"
      },
      {
        "name": "context_size",
        "level_1": "file_only",
        "level_2": "full_module"
      },
      {
        "name": "generation_strategy",
        "level_1": "standard",
        "level_2": "chain_of_thought"
      }
    ],
    "utility_weights": {
      "quality": 1.0,
      "cost": 0.05,
      "time": 0.02
    },
    "workflow_config": {
      "rubric": {
        "functionality": {
          "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
          "scale": "0-100 points",
          "weight": 0.25
        },
        "error_handling": {
          "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
          "scale": "0-100 points",
          "weight": 0.2
        },
        "code_quality": {
          "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
          "scale": "0-100 points",
          "weight": 0.2
        },
        "performance": {
          "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
          "scale": "0-100 points",
          "weight": 0.15
        },
        "testability": {
          "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
          "scale": "0-100 points",
          "weight": 0.1
        },
        "documentation": {
          "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
          "scale": "0-100 points",
          "weight": 0.1
        }
      },
      "sample_code_path": "tesseract_flow/evaluation/rubric.py",
      "evaluator_model": "openrouter/anthropic/claude-haiku-4.5",
      "language": "python",
      "context_files": [
        "tesseract_flow/evaluation/__init__.py",
        "tesseract_flow/evaluation/metrics.py",
        "tesseract_flow/evaluation/cache.py",
        "tesseract_flow/core/types.py",
        "tesseract_flow/core/exceptions.py"
      ],
      "evaluator_temperature": 0.3,
      "context_descriptions": {
        "file_only": "Review limited to rubric.py only - focused analysis.",
        "full_module": "Review considering full evaluation module context and dependencies."
      },
      "strategy_descriptions": {
        "standard": "Direct code review with standard prompting.",
        "chain_of_thought": "Step-by-step reasoning approach to code review analysis."
      }
    }
  },
  "test_configurations": [
    {
      "test_number": 1,
      "config_values": {
        "temperature": 0.3,
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "context_size": "file_only",
        "generation_strategy": "standard"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 2,
      "config_values": {
        "temperature": 0.3,
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "context_size": "file_only",
        "generation_strategy": "chain_of_thought"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 3,
      "config_values": {
        "temperature": 0.3,
        "model": "openrouter/anthropic/claude-sonnet-4.5",
        "context_size": "full_module",
        "generation_strategy": "standard"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 4,
      "config_values": {
        "temperature": 0.3,
        "model": "openrouter/anthropic/claude-sonnet-4.5",
        "context_size": "full_module",
        "generation_strategy": "chain_of_thought"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 5,
      "config_values": {
        "temperature": 0.6,
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "context_size": "full_module",
        "generation_strategy": "standard"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 6,
      "config_values": {
        "temperature": 0.6,
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "context_size": "full_module",
        "generation_strategy": "chain_of_thought"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 7,
      "config_values": {
        "temperature": 0.6,
        "model": "openrouter/anthropic/claude-sonnet-4.5",
        "context_size": "file_only",
        "generation_strategy": "standard"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 8,
      "config_values": {
        "temperature": 0.6,
        "model": "openrouter/anthropic/claude-sonnet-4.5",
        "context_size": "file_only",
        "generation_strategy": "chain_of_thought"
      },
      "workflow": "code_review"
    }
  ],
  "results": [
    {
      "test_number": 1,
      "config": {
        "test_number": 1,
        "config_values": {
          "temperature": 0.3,
          "model": "openrouter/anthropic/claude-haiku-4.5",
          "context_size": "file_only",
          "generation_strategy": "standard"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.72,
            "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-based scoring. The async architecture, retry logic, and caching integration are well-designed. However, several edge cases are inadequately handled: (1) assertion-based type narrowing can fail silently with Python's -O flag, leaving unsafe states; (2) the `_extract_response_content()` method's multiple fallback patterns are fragile and could miss valid responses; (3) no validation that LLM-returned dimensions match the requested rubric, allowing typos to silently fail; (4) the score extraction regex allows leading zeros and imprecise numeric patterns. The normalization logic appears correct (0-1 range), and the default rubric is reasonable, but edge cases around malformed LLM responses and missing scores are not robustly handled."
          },
          "error_handling": {
            "score": 0.58,
            "reasoning": "Error handling has significant gaps despite using a custom EvaluationError. Critical issues: (1) bare `except Exception` on line 160 catches KeyboardInterrupt and SystemExit in Python <3.11, masking critical errors; (2) assertions for type narrowing can be disabled, leaving code unsafe; (3) `last_error` initialization is fragile—if `_max_retries < 1`, the assertion on line 165 could fail despite validation; (4) generic error messages don't include exception details in the message itself, only in the chain; (5) no specific exception hierarchy for different failure modes (LLM errors vs. cache errors vs. validation errors); (6) insufficient logging for production debugging. Input validation exists but lacks length limits on `extra_instructions`. The code handles LLM failures with retry logic but doesn't gracefully degrade or provide fallback behavior."
          },
          "code_quality": {
            "score": 0.65,
            "reasoning": "Code demonstrates reasonable structure with clear separation of concerns (LLM interaction, response parsing, caching), but has maintainability issues: (1) inappropriate use of assertions for type narrowing instead of explicit checks; (2) `_extract_response_content()` has repetitive, hard-to-follow fallback logic that should be extracted into a helper; (3) `extra_instructions.strip()` called twice inefficiently; (4) type hints missing on helper method return types (`_extract_response_content()`, `_load_json()`, `_parse_dimension_scores()`); (5) hardcoded system prompt and templates reduce flexibility; (6) `# pragma: no cover` on critical path suggests untestable code. Pydantic usage for validation is appropriate. Naming is generally clear, but the code violates DRY principle in response extraction logic."
          },
          "performance": {
            "score": 0.76,
            "reasoning": "Performance is generally solid with good caching strategy and minimal unnecessary LLM calls. Strengths: (1) async/await architecture enables concurrent evaluations; (2) caching integration reduces redundant LLM calls; (3) retry logic with exponential backoff is efficient; (4) regex patterns are reasonable. Concerns: (1) no documented cache backend concurrency requirements—potential race conditions if backend isn't thread-safe; (2) regex pattern `\\d+(?:\\.\\d+)?` allows leading zeros, adding unnecessary parsing overhead; (3) no length limits on `extra_instructions` could inflate token usage; (4) multiple `strip()` calls and repeated fallback logic in response extraction add minor overhead. Token usage appears reasonable given the evaluation task, but no explicit optimization for large rubrics."
          },
          "testability": {
            "score": 0.52,
            "reasoning": "Testability is significantly hampered by design choices: (1) `litellm.acompletion` is tightly coupled, making unit testing difficult without mocking the entire module; (2) broad exception handling on line 160 makes it hard to test specific failure modes; (3) `# pragma: no cover` comment on critical retry path indicates untestable code; (4) assertions for type narrowing make tests fragile; (5) no dependency injection for LLM calls or cache backend; (6) helper methods lack clear contracts and return type hints, making mocking difficult. Positive aspects: (1) clear input/output structure with Pydantic models; (2) pure functions like `_parse_dimension_scores()` are testable; (3) error cases are somewhat reproducible. The code would benefit from protocol-based interfaces or dependency injection to enable proper unit testing without mocking entire modules."
          },
          "documentation": {
            "score": 0.61,
            "reasoning": "Documentation is incomplete and inconsistent: (1) the `evaluate()` method lacks a comprehensive docstring explaining parameters, return types, caching behavior, and when `EvaluationError` is raised; (2) helper methods (`_extract_response_content()`, `_load_json()`, `_parse_dimension_scores()`) lack return type hints and docstrings; (3) no module-level docstring with usage examples; (4) inline comments are sparse for complex logic (response extraction, retry logic); (5) cache backend concurrency requirements are undocumented; (6) no examples showing error handling patterns or caching setup. Positive aspects: (1) class and method names are reasonably clear; (2) type hints exist for most parameters; (3) some inline comments explain non-obvious logic. Overall, documentation is insufficient for users to understand caching behavior, error handling, and configuration options."
          }
        },
        "overall_score": 0.64,
        "evaluator_model": "openrouter/anthropic/claude-haiku-4.5",
        "timestamp": "2025-10-26T06:21:13.000634Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-based scoring. The async architecture, retry logic, and caching integration are well-designed. However, several edge cases are inadequately handled: (1) assertion-based type narrowing can fail silently with Python's -O flag, leaving unsafe states; (2) the `_extract_response_content()` method's multiple fallback patterns are fragile and could miss valid responses; (3) no validation that LLM-returned dimensions match the requested rubric, allowing typos to silently fail; (4) the score extraction regex allows leading zeros and imprecise numeric patterns. The normalization logic appears correct (0-1 range), and the default rubric is reasonable, but edge cases around malformed LLM responses and missing scores are not robustly handled."
            },
            "error_handling": {
              "score": 58,
              "reasoning": "Error handling has significant gaps despite using a custom EvaluationError. Critical issues: (1) bare `except Exception` on line 160 catches KeyboardInterrupt and SystemExit in Python <3.11, masking critical errors; (2) assertions for type narrowing can be disabled, leaving code unsafe; (3) `last_error` initialization is fragile—if `_max_retries < 1`, the assertion on line 165 could fail despite validation; (4) generic error messages don't include exception details in the message itself, only in the chain; (5) no specific exception hierarchy for different failure modes (LLM errors vs. cache errors vs. validation errors); (6) insufficient logging for production debugging. Input validation exists but lacks length limits on `extra_instructions`. The code handles LLM failures with retry logic but doesn't gracefully degrade or provide fallback behavior."
            },
            "code_quality": {
              "score": 65,
              "reasoning": "Code demonstrates reasonable structure with clear separation of concerns (LLM interaction, response parsing, caching), but has maintainability issues: (1) inappropriate use of assertions for type narrowing instead of explicit checks; (2) `_extract_response_content()` has repetitive, hard-to-follow fallback logic that should be extracted into a helper; (3) `extra_instructions.strip()` called twice inefficiently; (4) type hints missing on helper method return types (`_extract_response_content()`, `_load_json()`, `_parse_dimension_scores()`); (5) hardcoded system prompt and templates reduce flexibility; (6) `# pragma: no cover` on critical path suggests untestable code. Pydantic usage for validation is appropriate. Naming is generally clear, but the code violates DRY principle in response extraction logic."
            },
            "performance": {
              "score": 76,
              "reasoning": "Performance is generally solid with good caching strategy and minimal unnecessary LLM calls. Strengths: (1) async/await architecture enables concurrent evaluations; (2) caching integration reduces redundant LLM calls; (3) retry logic with exponential backoff is efficient; (4) regex patterns are reasonable. Concerns: (1) no documented cache backend concurrency requirements—potential race conditions if backend isn't thread-safe; (2) regex pattern `\\d+(?:\\.\\d+)?` allows leading zeros, adding unnecessary parsing overhead; (3) no length limits on `extra_instructions` could inflate token usage; (4) multiple `strip()` calls and repeated fallback logic in response extraction add minor overhead. Token usage appears reasonable given the evaluation task, but no explicit optimization for large rubrics."
            },
            "testability": {
              "score": 52,
              "reasoning": "Testability is significantly hampered by design choices: (1) `litellm.acompletion` is tightly coupled, making unit testing difficult without mocking the entire module; (2) broad exception handling on line 160 makes it hard to test specific failure modes; (3) `# pragma: no cover` comment on critical retry path indicates untestable code; (4) assertions for type narrowing make tests fragile; (5) no dependency injection for LLM calls or cache backend; (6) helper methods lack clear contracts and return type hints, making mocking difficult. Positive aspects: (1) clear input/output structure with Pydantic models; (2) pure functions like `_parse_dimension_scores()` are testable; (3) error cases are somewhat reproducible. The code would benefit from protocol-based interfaces or dependency injection to enable proper unit testing without mocking entire modules."
            },
            "documentation": {
              "score": 61,
              "reasoning": "Documentation is incomplete and inconsistent: (1) the `evaluate()` method lacks a comprehensive docstring explaining parameters, return types, caching behavior, and when `EvaluationError` is raised; (2) helper methods (`_extract_response_content()`, `_load_json()`, `_parse_dimension_scores()`) lack return type hints and docstrings; (3) no module-level docstring with usage examples; (4) inline comments are sparse for complex logic (response extraction, retry logic); (5) cache backend concurrency requirements are undocumented; (6) no examples showing error handling patterns or caching setup. Positive aspects: (1) class and method names are reasonably clear; (2) type hints exist for most parameters; (3) some inline comments explain non-obvious logic. Overall, documentation is insufficient for users to understand caching behavior, error handling, and configuration options."
            }
          },
          "temperature": 0.3,
          "cache_key": "2228e54f7801011c3e457f7c954b3285a6a41045ddea728a4caf83018b90d903",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "cost": 0.0,
      "latency": 20456.82512500207,
      "utility": 0.6367367246545047,
      "workflow_output": "Well-structured async LLM evaluator with solid error handling and validation. Code is generally clean and maintainable, but has some issues around type safety, error handling edge cases, and testability. The retry logic and caching integration are well-designed. Main concerns are assertion usage for type narrowing and potential race conditions in concurrent scenarios.\nIssues:\n- (medium) Using `assert` statements for type narrowing (lines 147, 165, 181) is problematic. Assertions can be disabled with Python's `-O` flag, leaving code in an unsafe state. These should be explicit type checks or guards. Suggested fix: Replace `assert content is not None` with explicit type checking or restructure control flow to guarantee non-None values. Example: `if content is None: raise EvaluationError('Content extraction failed')` or use `typing.cast()` with a comment explaining why it's safe.\n- (medium) Bare `except Exception` on line 160 catches all exceptions including KeyboardInterrupt and SystemExit (in Python <3.11). This can mask critical errors and make debugging difficult. [line 160] Suggested fix: Catch specific exceptions from litellm (e.g., `litellm.APIError`, `litellm.RateLimitError`) or at minimum use `except Exception as exc` with explicit handling. Consider using `asyncio.CancelledError` separately if needed.\n- (medium) On line 165, `assert last_error is not None` assumes an error occurred, but the loop could theoretically exit without setting `last_error` if `_max_retries < 1` (though validated in `__init__`). The assertion is fragile. [line 165] Suggested fix: Initialize `last_error` with a default exception or restructure to guarantee it's set. Example: `last_error = last_error or EvaluationError('Unknown error')`\n- (medium) The `_cache` backend is shared across concurrent `evaluate()` calls. If the cache backend is not thread-safe or doesn't handle concurrent access, race conditions could occur during `cache_backend.get()` and `cache_backend.set()` operations. Suggested fix: Document cache backend concurrency requirements. Consider adding a lock or using a thread-safe cache implementation. Add a note in docstring: 'Cache backend must be thread-safe for concurrent evaluations.'\n- (low) The `extra_instructions` parameter is only checked for emptiness after stripping (line 127), but no length limit is enforced. Very long instructions could inflate token usage and costs. [line 127] Suggested fix: Add a maximum length check for `extra_instructions`, e.g., `if len(extra_instructions) > 5000: raise ValueError('Instructions too long')`\n- (low) Generic error message on line 168 doesn't include the actual exception details in the message itself (only in the chain). Users reading logs might miss the root cause. [line 168] Suggested fix: Include exception type/message in the error string: `f'LLM evaluation failed after {self._max_retries} attempts: {type(last_error).__name__}: {last_error}'`\n- (low) The `_extract_response_content()` method handles both Mapping and object attribute access patterns (lines 186-204), but the logic is repetitive and hard to follow. Multiple fallback patterns make it fragile. Suggested fix: Extract a helper method like `_get_nested_attr(obj, key)` that handles both Mapping and attribute access uniformly. This reduces duplication and improves maintainability.\n- (low) The regex pattern on line 222 uses `\\d+(?:\\.\\d+)?` which allows leading zeros (e.g., '007'). While not breaking, it's imprecise for numeric parsing. [line 222] Suggested fix: Consider using a more precise pattern or just rely on `float()` parsing: `match = re.search(r'([\\d.]+)\\s*-\\s*([\\d.]+)', scale)` and let float() handle validation.\n- (medium) The `_request_with_retry()` method has a `# pragma: no cover` comment (line 161), but this is a critical path that should be testable. The broad exception handling makes it hard to unit test specific failure modes. [line 161] Suggested fix: Make the exception handling more specific and testable. Create a separate internal method that wraps litellm calls, allowing you to mock/test retry logic independently. Remove pragma comment and add proper tests.\n- (low) On line 127, `extra_instructions.strip()` is called twice if the condition is true (once in the condition, once in the f-string). Minor inefficiency. [line 127] Suggested fix: Store the stripped value: `stripped_instructions = extra_instructions.strip() if extra_instructions else None; instructions = f'\\n{stripped_instructions}' if stripped_instructions else ''`\n- (low) The `evaluate()` method has many parameters but lacks detailed docstring explaining each one, especially the caching parameters and their interactions. [line 77] Suggested fix: Add comprehensive docstring with parameter descriptions, return type details, and examples. Document the caching behavior and when `EvaluationError` is raised.\nSuggestions:\n- {'category': 'Type Safety', 'description': 'Add type hints to return types of helper methods like `_extract_response_content()`, `_load_json()`, and `_parse_dimension_scores()`. This improves IDE support and catches errors early.', 'priority': 'medium'}\n- {'category': 'Error Handling', 'description': 'Create a custom exception hierarchy (e.g., `LLMResponseError`, `CacheError`, `ValidationError` extending `EvaluationError`) to allow callers to handle different failure modes specifically.', 'priority': 'medium'}\n- {'category': 'Testability', 'description': 'Inject the litellm.acompletion call as a dependency or use a protocol/interface to make the class testable without mocking the entire litellm module. This enables better unit testing of retry logic.', 'priority': 'medium'}\n- {'category': 'Logging', 'description': 'Add more structured logging with context (e.g., workflow_output hash, rubric dimensions) to help debug issues in production. Consider using `logging.LogRecord` attributes.', 'priority': 'low'}\n- {'category': 'Configuration', 'description': 'Consider making the system prompt and prompt template configurable rather than hardcoded. This allows users to customize evaluation behavior without subclassing.', 'priority': 'low'}\n- {'category': 'Validation', 'description': 'Add validation that the rubric dimensions in the response match the requested rubric (case-insensitive). Currently, typos in LLM responses silently fail.', 'priority': 'medium'}\n- {'category': 'Documentation', 'description': 'Add examples in module docstring showing basic usage, caching setup, and error handling patterns.', 'priority': 'low'}",
      "metadata": {
        "strategy": "standard",
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "temperature": 0.3,
        "context_size": "file_only",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Review limited to the provided file.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"Well-structured async LLM evaluator with solid error handling and validation. Code is generally clean and maintainable, but has some issues around type safety, error handling edge cases, and testability. The retry logic and caching integration are well-designed. Main concerns are assertion usage for type narrowing and potential race conditions in concurrent scenarios.\",\n  \"issues\": [\n    {\n      \"type\": \"Type Safety\",\n      \"severity\": \"medium\",\n      \"description\": \"Using `assert` statements for type narrowing (lines 147, 165, 181) is problematic. Assertions can be disabled with Python's `-O` flag, leaving code in an unsafe state. These should be explicit type checks or guards.\",\n      \"line_number\": [147, 165, 181],\n      \"suggestion\": \"Replace `assert content is not None` with explicit type checking or restructure control flow to guarantee non-None values. Example: `if content is None: raise EvaluationError('Content extraction failed')` or use `typing.cast()` with a comment explaining why it's safe.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"Bare `except Exception` on line 160 catches all exceptions including KeyboardInterrupt and SystemExit (in Python <3.11). This can mask critical errors and make debugging difficult.\",\n      \"line_number\": 160,\n      \"suggestion\": \"Catch specific exceptions from litellm (e.g., `litellm.APIError`, `litellm.RateLimitError`) or at minimum use `except Exception as exc` with explicit handling. Consider using `asyncio.CancelledError` separately if needed.\"\n    },\n    {\n      \"type\": \"Logic Error\",\n      \"severity\": \"medium\",\n      \"description\": \"On line 165, `assert last_error is not None` assumes an error occurred, but the loop could theoretically exit without setting `last_error` if `_max_retries < 1` (though validated in `__init__`). The assertion is fragile.\",\n      \"line_number\": 165,\n      \"suggestion\": \"Initialize `last_error` with a default exception or restructure to guarantee it's set. Example: `last_error = last_error or EvaluationError('Unknown error')`\"\n    },\n    {\n      \"type\": \"Concurrency Issue\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_cache` backend is shared across concurrent `evaluate()` calls. If the cache backend is not thread-safe or doesn't handle concurrent access, race conditions could occur during `cache_backend.get()` and `cache_backend.set()` operations.\",\n      \"line_number\": [139, 177],\n      \"suggestion\": \"Document cache backend concurrency requirements. Consider adding a lock or using a thread-safe cache implementation. Add a note in docstring: 'Cache backend must be thread-safe for concurrent evaluations.'\"\n    },\n    {\n      \"type\": \"Input Validation\",\n      \"severity\": \"low\",\n      \"description\": \"The `extra_instructions` parameter is only checked for emptiness after stripping (line 127), but no length limit is enforced. Very long instructions could inflate token usage and costs.\",\n      \"line_number\": 127,\n      \"suggestion\": \"Add a maximum length check for `extra_instructions`, e.g., `if len(extra_instructions) > 5000: raise ValueError('Instructions too long')`\"\n    },\n    {\n      \"type\": \"Error Message Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Generic error message on line 168 doesn't include the actual exception details in the message itself (only in the chain). Users reading logs might miss the root cause.\",\n      \"line_number\": 168,\n      \"suggestion\": \"Include exception type/message in the error string: `f'LLM evaluation failed after {self._max_retries} attempts: {type(last_error).__name__}: {last_error}'`\"\n    },\n    {\n      \"type\": \"Code Clarity\",\n      \"severity\": \"low\",\n      \"description\": \"The `_extract_response_content()` method handles both Mapping and object attribute access patterns (lines 186-204), but the logic is repetitive and hard to follow. Multiple fallback patterns make it fragile.\",\n      \"line_number\": [186, 204],\n      \"suggestion\": \"Extract a helper method like `_get_nested_attr(obj, key)` that handles both Mapping and attribute access uniformly. This reduces duplication and improves maintainability.\"\n    },\n    {\n      \"type\": \"Regex Pattern\",\n      \"severity\": \"low\",\n      \"description\": \"The regex pattern on line 222 uses `\\\\d+(?:\\\\.\\\\d+)?` which allows leading zeros (e.g., '007'). While not breaking, it's imprecise for numeric parsing.\",\n      \"line_number\": 222,\n      \"suggestion\": \"Consider using a more precise pattern or just rely on `float()` parsing: `match = re.search(r'([\\\\d.]+)\\\\s*-\\\\s*([\\\\d.]+)', scale)` and let float() handle validation.\"\n    },\n    {\n      \"type\": \"Testing Concern\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_request_with_retry()` method has a `# pragma: no cover` comment (line 161), but this is a critical path that should be testable. The broad exception handling makes it hard to unit test specific failure modes.\",\n      \"line_number\": 161,\n      \"suggestion\": \"Make the exception handling more specific and testable. Create a separate internal method that wraps litellm calls, allowing you to mock/test retry logic independently. Remove pragma comment and add proper tests.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"On line 127, `extra_instructions.strip()` is called twice if the condition is true (once in the condition, once in the f-string). Minor inefficiency.\",\n      \"line_number\": 127,\n      \"suggestion\": \"Store the stripped value: `stripped_instructions = extra_instructions.strip() if extra_instructions else None; instructions = f'\\\\n{stripped_instructions}' if stripped_instructions else ''`\"\n    },\n    {\n      \"type\": \"Documentation\",\n      \"severity\": \"low\",\n      \"description\": \"The `evaluate()` method has many parameters but lacks detailed docstring explaining each one, especially the caching parameters and their interactions.\",\n      \"line_number\": 77,\n      \"suggestion\": \"Add comprehensive docstring with parameter descriptions, return type details, and examples. Document the caching behavior and when `EvaluationError` is raised.\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"category\": \"Type Safety\",\n      \"description\": \"Add type hints to return types of helper methods like `_extract_response_content()`, `_load_json()`, and `_parse_dimension_scores()`. This improves IDE support and catches errors early.\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"category\": \"Error Handling\",\n      \"description\": \"Create a custom exception hierarchy (e.g., `LLMResponseError`, `CacheError`, `ValidationError` extending `EvaluationError`) to allow callers to handle different failure modes specifically.\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"category\": \"Testability\",\n      \"description\": \"Inject the litellm.acompletion call as a dependency or use a protocol/interface to make the class testable without mocking the entire litellm module. This enables better unit testing of retry logic.\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"category\": \"Logging\",\n      \"description\": \"Add more structured logging with context (e.g., workflow_output hash, rubric dimensions) to help debug issues in production. Consider using `logging.LogRecord` attributes.\",\n      \"priority\": \"low\"\n    },\n    {\n      \"category\": \"Configuration\",\n      \"description\": \"Consider making the system prompt and prompt template configurable rather than hardcoded. This allows users to customize evaluation behavior without subclassing.\",\n      \"priority\": \"low\"\n    },\n    {\n      \"category\": \"Validation\",\n      \"description\": \"Add validation that the rubric dimensions in the response match the requested rubric (case-insensitive). Currently, typos in LLM responses silently fail.\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"category\": \"Documentation\",\n      \"description\": \"Add examples in module docstring showing basic usage, caching setup, and error handling patterns.\",\n      \"priority\": \"low\"\n    }\n  ]\n}\n```",
        "test_config": {
          "test_number": 1,
          "config": {
            "temperature": 0.3,
            "model": "openrouter/anthropic/claude-haiku-4.5",
            "context_size": "file_only",
            "generation_strategy": "standard"
          }
        },
        "workflow": {
          "started_at": "2025-10-26T06:20:37.589525Z",
          "completed_at": "2025-10-26T06:20:58.046570Z",
          "duration_seconds": 20.45682512500207
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-based scoring. The async architecture, retry logic, and caching integration are well-designed. However, several edge cases are inadequately handled: (1) assertion-based type narrowing can fail silently with Python's -O flag, leaving unsafe states; (2) the `_extract_response_content()` method's multiple fallback patterns are fragile and could miss valid responses; (3) no validation that LLM-returned dimensions match the requested rubric, allowing typos to silently fail; (4) the score extraction regex allows leading zeros and imprecise numeric patterns. The normalization logic appears correct (0-1 range), and the default rubric is reasonable, but edge cases around malformed LLM responses and missing scores are not robustly handled."
            },
            "error_handling": {
              "score": 58,
              "reasoning": "Error handling has significant gaps despite using a custom EvaluationError. Critical issues: (1) bare `except Exception` on line 160 catches KeyboardInterrupt and SystemExit in Python <3.11, masking critical errors; (2) assertions for type narrowing can be disabled, leaving code unsafe; (3) `last_error` initialization is fragile—if `_max_retries < 1`, the assertion on line 165 could fail despite validation; (4) generic error messages don't include exception details in the message itself, only in the chain; (5) no specific exception hierarchy for different failure modes (LLM errors vs. cache errors vs. validation errors); (6) insufficient logging for production debugging. Input validation exists but lacks length limits on `extra_instructions`. The code handles LLM failures with retry logic but doesn't gracefully degrade or provide fallback behavior."
            },
            "code_quality": {
              "score": 65,
              "reasoning": "Code demonstrates reasonable structure with clear separation of concerns (LLM interaction, response parsing, caching), but has maintainability issues: (1) inappropriate use of assertions for type narrowing instead of explicit checks; (2) `_extract_response_content()` has repetitive, hard-to-follow fallback logic that should be extracted into a helper; (3) `extra_instructions.strip()` called twice inefficiently; (4) type hints missing on helper method return types (`_extract_response_content()`, `_load_json()`, `_parse_dimension_scores()`); (5) hardcoded system prompt and templates reduce flexibility; (6) `# pragma: no cover` on critical path suggests untestable code. Pydantic usage for validation is appropriate. Naming is generally clear, but the code violates DRY principle in response extraction logic."
            },
            "performance": {
              "score": 76,
              "reasoning": "Performance is generally solid with good caching strategy and minimal unnecessary LLM calls. Strengths: (1) async/await architecture enables concurrent evaluations; (2) caching integration reduces redundant LLM calls; (3) retry logic with exponential backoff is efficient; (4) regex patterns are reasonable. Concerns: (1) no documented cache backend concurrency requirements—potential race conditions if backend isn't thread-safe; (2) regex pattern `\\d+(?:\\.\\d+)?` allows leading zeros, adding unnecessary parsing overhead; (3) no length limits on `extra_instructions` could inflate token usage; (4) multiple `strip()` calls and repeated fallback logic in response extraction add minor overhead. Token usage appears reasonable given the evaluation task, but no explicit optimization for large rubrics."
            },
            "testability": {
              "score": 52,
              "reasoning": "Testability is significantly hampered by design choices: (1) `litellm.acompletion` is tightly coupled, making unit testing difficult without mocking the entire module; (2) broad exception handling on line 160 makes it hard to test specific failure modes; (3) `# pragma: no cover` comment on critical retry path indicates untestable code; (4) assertions for type narrowing make tests fragile; (5) no dependency injection for LLM calls or cache backend; (6) helper methods lack clear contracts and return type hints, making mocking difficult. Positive aspects: (1) clear input/output structure with Pydantic models; (2) pure functions like `_parse_dimension_scores()` are testable; (3) error cases are somewhat reproducible. The code would benefit from protocol-based interfaces or dependency injection to enable proper unit testing without mocking entire modules."
            },
            "documentation": {
              "score": 61,
              "reasoning": "Documentation is incomplete and inconsistent: (1) the `evaluate()` method lacks a comprehensive docstring explaining parameters, return types, caching behavior, and when `EvaluationError` is raised; (2) helper methods (`_extract_response_content()`, `_load_json()`, `_parse_dimension_scores()`) lack return type hints and docstrings; (3) no module-level docstring with usage examples; (4) inline comments are sparse for complex logic (response extraction, retry logic); (5) cache backend concurrency requirements are undocumented; (6) no examples showing error handling patterns or caching setup. Positive aspects: (1) class and method names are reasonably clear; (2) type hints exist for most parameters; (3) some inline comments explain non-obvious logic. Overall, documentation is insufficient for users to understand caching behavior, error handling, and configuration options."
            }
          },
          "temperature": 0.3,
          "cache_key": "2228e54f7801011c3e457f7c954b3285a6a41045ddea728a4caf83018b90d903",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "timestamp": "2025-10-26T06:21:13.000882Z"
    },
    {
      "test_number": 2,
      "config": {
        "test_number": 2,
        "config_values": {
          "temperature": 0.3,
          "model": "openrouter/anthropic/claude-haiku-4.5",
          "context_size": "file_only",
          "generation_strategy": "chain_of_thought"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.72,
            "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-powered rubrics. Score extraction and normalization logic appears sound. However, several edge cases are inadequately handled: (1) assertions used for control flow (lines 119, 155, 182) can be disabled with -O flag, breaking runtime validation; (2) markdown fence stripping (lines 227-233) may fail on nested code blocks; (3) division by zero possible if max_score is 0 (line 275); (4) bare exception catch (line 163) may mask critical failures. These issues prevent reliable handling of malformed LLM responses and edge cases."
          },
          "error_handling": {
            "score": 0.58,
            "reasoning": "Error handling has significant gaps. Critical issues: (1) Assertions used instead of explicit validation—disabled with -O flag; (2) Bare except clause catches KeyboardInterrupt and SystemExit; (3) No validation that max_score > 0 before division; (4) Cache key validation strips whitespace but doesn't check if result is empty; (5) Type hints for response are 'Any', making defensive programming fragile; (6) No custom exception hierarchy for different failure modes; (7) Limited logging for debugging. While EvaluationError is used in some places, the overall approach lacks consistency and rigor."
          },
          "code_quality": {
            "score": 0.64,
            "reasoning": "Code has reasonable structure but several maintainability issues: (1) Complex nested conditional logic for cache handling (lines 147-158) should be extracted to private method; (2) Type hints use 'Any' for response object, reducing clarity; (3) Magic strings ('json_object', line 171) should be constants; (4) Inconsistent error message formatting (f-strings vs format, periods inconsistent); (5) No docstrings for private methods; (6) Assertions used for type narrowing instead of explicit checks. Positive aspects: clear method names, reasonable separation of concerns, Pydantic used for RubricDimension. Overall, code is readable but has design debt."
          },
          "performance": {
            "score": 0.68,
            "reasoning": "Performance is generally adequate but has optimization opportunities: (1) Caching support is well-implemented with configurable TTL; (2) Regex pattern in _extract_max_score (line 258) is called per dimension without pre-compilation—should be class constant; (3) Rubric formatting could be cached if same rubric used multiple times; (4) No timeout parameter on LiteLLM call (line 169) risks indefinite hangs; (5) Minimal unnecessary LLM calls by design. Token usage appears reasonable. No obvious bottlenecks, but regex optimization and timeout handling would improve robustness."
          },
          "testability": {
            "score": 0.55,
            "reasoning": "Testability is limited by design choices: (1) Tight coupling to LiteLLM makes unit testing difficult; (2) Pragma comment 'no cover' on line 163 suggests retry logic is untestable; (3) Assertions used for validation make mocking harder; (4) No dependency injection for LLM client; (5) Complex cache logic (lines 147-158) is hard to test in isolation; (6) Type hints use 'Any' for response, making contracts unclear. Positive aspects: clear method interfaces, pure functions for score extraction possible. Would require significant refactoring to enable comprehensive testing without actual LiteLLM calls."
          },
          "documentation": {
            "score": 0.61,
            "reasoning": "Documentation is incomplete: (1) No docstrings for private methods (_build_prompt, _extract_response_content, _extract_max_score, etc.); (2) Public method docstrings exist but lack detail on error conditions and edge cases; (3) Type hints present but use 'Any' in critical places (response object), reducing clarity; (4) No inline comments explaining complex logic (cache handling, markdown fence stripping); (5) No examples provided for expected rubric format or response structure; (6) Variable names are generally clear but some are terse (e.g., 'exc'). Prompt injection risk in extra_instructions (line 108) is not documented. Overall, documentation is sparse for a complex evaluator."
          }
        },
        "overall_score": 0.63,
        "evaluator_model": "openrouter/anthropic/claude-haiku-4.5",
        "timestamp": "2025-10-26T06:21:39.750377Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-powered rubrics. Score extraction and normalization logic appears sound. However, several edge cases are inadequately handled: (1) assertions used for control flow (lines 119, 155, 182) can be disabled with -O flag, breaking runtime validation; (2) markdown fence stripping (lines 227-233) may fail on nested code blocks; (3) division by zero possible if max_score is 0 (line 275); (4) bare exception catch (line 163) may mask critical failures. These issues prevent reliable handling of malformed LLM responses and edge cases."
            },
            "error_handling": {
              "score": 58,
              "reasoning": "Error handling has significant gaps. Critical issues: (1) Assertions used instead of explicit validation—disabled with -O flag; (2) Bare except clause catches KeyboardInterrupt and SystemExit; (3) No validation that max_score > 0 before division; (4) Cache key validation strips whitespace but doesn't check if result is empty; (5) Type hints for response are 'Any', making defensive programming fragile; (6) No custom exception hierarchy for different failure modes; (7) Limited logging for debugging. While EvaluationError is used in some places, the overall approach lacks consistency and rigor."
            },
            "code_quality": {
              "score": 64,
              "reasoning": "Code has reasonable structure but several maintainability issues: (1) Complex nested conditional logic for cache handling (lines 147-158) should be extracted to private method; (2) Type hints use 'Any' for response object, reducing clarity; (3) Magic strings ('json_object', line 171) should be constants; (4) Inconsistent error message formatting (f-strings vs format, periods inconsistent); (5) No docstrings for private methods; (6) Assertions used for type narrowing instead of explicit checks. Positive aspects: clear method names, reasonable separation of concerns, Pydantic used for RubricDimension. Overall, code is readable but has design debt."
            },
            "performance": {
              "score": 68,
              "reasoning": "Performance is generally adequate but has optimization opportunities: (1) Caching support is well-implemented with configurable TTL; (2) Regex pattern in _extract_max_score (line 258) is called per dimension without pre-compilation—should be class constant; (3) Rubric formatting could be cached if same rubric used multiple times; (4) No timeout parameter on LiteLLM call (line 169) risks indefinite hangs; (5) Minimal unnecessary LLM calls by design. Token usage appears reasonable. No obvious bottlenecks, but regex optimization and timeout handling would improve robustness."
            },
            "testability": {
              "score": 55,
              "reasoning": "Testability is limited by design choices: (1) Tight coupling to LiteLLM makes unit testing difficult; (2) Pragma comment 'no cover' on line 163 suggests retry logic is untestable; (3) Assertions used for validation make mocking harder; (4) No dependency injection for LLM client; (5) Complex cache logic (lines 147-158) is hard to test in isolation; (6) Type hints use 'Any' for response, making contracts unclear. Positive aspects: clear method interfaces, pure functions for score extraction possible. Would require significant refactoring to enable comprehensive testing without actual LiteLLM calls."
            },
            "documentation": {
              "score": 61,
              "reasoning": "Documentation is incomplete: (1) No docstrings for private methods (_build_prompt, _extract_response_content, _extract_max_score, etc.); (2) Public method docstrings exist but lack detail on error conditions and edge cases; (3) Type hints present but use 'Any' in critical places (response object), reducing clarity; (4) No inline comments explaining complex logic (cache handling, markdown fence stripping); (5) No examples provided for expected rubric format or response structure; (6) Variable names are generally clear but some are terse (e.g., 'exc'). Prompt injection risk in extra_instructions (line 108) is not documented. Overall, documentation is sparse for a complex evaluator."
            }
          },
          "temperature": 0.3,
          "cache_key": "03d756ab5f618c83e8c0fd6e01f7c6ea1e077f1b8edf1d24fe48c9a3cb56813d",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "cost": 0.0,
      "latency": 15794.765084021492,
      "utility": 0.63,
      "workflow_output": "Well-structured LLM-powered evaluator with solid error handling and caching support. Code is generally clean and maintainable, but has several issues around assertion usage, type safety, and edge case handling that should be addressed.\nIssues:\n- (high) Assertions used for control flow and type narrowing (lines 119, 155, 182). Assertions are disabled with Python's -O flag and should not be relied upon for runtime validation. Suggested fix: Replace assertions with explicit if-checks and raise EvaluationError. Example: if resolved_cache_key is None: raise EvaluationError('Cache key required')\n- (high) Bare exception catch (line 163) catches all exceptions including KeyboardInterrupt and SystemExit. The pragma comment suggests awareness but doesn't mitigate the issue. [line 163] Suggested fix: Catch specific exceptions: 'except (litellm.APIError, litellm.RateLimitError, asyncio.TimeoutError) as exc:' or similar, depending on litellm's exception hierarchy.\n- (medium) Cache key validation (line 154) strips whitespace but doesn't validate the key format. An all-whitespace key could pass through after stripping. [line 154] Suggested fix: Add validation after strip: if not resolved_cache_key: raise EvaluationError('Cache key must be non-empty after stripping')\n- (medium) Type hints for response object are 'Any' (line 160), making it difficult to understand expected structure. Response handling is defensive but fragile. [line 160] Suggested fix: Define a TypedDict or dataclass for the expected response structure, or use a more specific type from litellm if available.\n- (medium) Complex nested conditional logic for cache handling (lines 147-158) is hard to follow. Multiple boolean flags and state tracking. Suggested fix: Extract cache logic into a separate private method: '_get_or_fetch_evaluation_content()' to improve readability.\n- (medium) JSON parsing error (line 237) catches JSONDecodeError but the markdown fence stripping (lines 227-233) may not handle all edge cases (e.g., nested code blocks, incomplete fences). Suggested fix: Add more robust markdown fence handling with regex or consider using a dedicated markdown parser. Add logging for fence-stripping actions.\n- (medium) Score normalization (line 275) clamps values but doesn't validate that max_score is positive. Division by zero is possible if max_score is 0. [line 275] Suggested fix: Add validation: if max_score <= 0: raise ValueError('max_score must be positive')\n- (low) Inconsistent error message formatting. Some use f-strings, others use format placeholders. Some messages end with periods, others don't. Suggested fix: Standardize error messages: use f-strings consistently and ensure all end with periods for consistency.\n- (low) Pragma comment 'pragma: no cover' on line 163 suggests external dependency testing is skipped, but retry logic is critical and should be testable. [line 163] Suggested fix: Consider using dependency injection or mocking to make retry logic testable without relying on actual LiteLLM calls.\n- (low) Regex search in _extract_max_score (line 258) is called for every dimension. Could be optimized by caching or pre-compiling the pattern. [line 258] Suggested fix: Compile regex pattern as class constant: _SCALE_PATTERN = re.compile(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)')\n- (low) User-provided extra_instructions (line 108) are directly interpolated into the prompt without sanitization. Could be exploited for prompt injection. [line 108] Suggested fix: Document that extra_instructions should be trusted input. If untrusted, consider escaping or validating the content.\n- (low) Magic string 'json_object' in response_format (line 171) should be a constant. [line 171] Suggested fix: Add constant: RESPONSE_FORMAT = {'type': 'json_object'}\nSuggestions:\n- {'category': 'Error Handling', 'priority': 'high', 'description': 'Create a custom exception hierarchy for different failure modes (e.g., EvaluationCacheError, EvaluationParseError) to enable more granular error handling by callers.'}\n- {'category': 'Testing', 'priority': 'high', 'description': 'Add integration tests for the full evaluation pipeline with mocked LiteLLM responses. Current design makes this difficult due to tight coupling.'}\n- {'category': 'Documentation', 'priority': 'medium', 'description': 'Add docstrings to private methods (_build_prompt, _extract_response_content, etc.) explaining expected formats and error conditions.'}\n- {'category': 'Robustness', 'priority': 'medium', 'description': 'Add timeout parameter to LiteLLM call (line 169) to prevent indefinite hangs. Consider adding a configurable timeout in __init__.'}\n- {'category': 'Logging', 'priority': 'medium', 'description': 'Add debug logging for parsed dimension scores before normalization to aid troubleshooting of score mismatches.'}\n- {'category': 'Code Quality', 'priority': 'low', 'description': 'Consider using Pydantic models for RubricDimension and response payloads to get automatic validation and better IDE support.'}\n- {'category': 'Performance', 'priority': 'low', 'description': 'Cache the compiled regex pattern and consider caching rubric formatting results if the same rubric is used multiple times.'}",
      "metadata": {
        "strategy": "chain_of_thought",
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "temperature": 0.3,
        "context_size": "file_only",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Review limited to the provided file.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"Well-structured LLM-powered evaluator with solid error handling and caching support. Code is generally clean and maintainable, but has several issues around assertion usage, type safety, and edge case handling that should be addressed.\",\n  \"issues\": [\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"high\",\n      \"description\": \"Assertions used for control flow and type narrowing (lines 119, 155, 182). Assertions are disabled with Python's -O flag and should not be relied upon for runtime validation.\",\n      \"line_number\": [119, 155, 182],\n      \"suggestion\": \"Replace assertions with explicit if-checks and raise EvaluationError. Example: if resolved_cache_key is None: raise EvaluationError('Cache key required')\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"high\",\n      \"description\": \"Bare exception catch (line 163) catches all exceptions including KeyboardInterrupt and SystemExit. The pragma comment suggests awareness but doesn't mitigate the issue.\",\n      \"line_number\": 163,\n      \"suggestion\": \"Catch specific exceptions: 'except (litellm.APIError, litellm.RateLimitError, asyncio.TimeoutError) as exc:' or similar, depending on litellm's exception hierarchy.\"\n    },\n    {\n      \"type\": \"Functionality\",\n      \"severity\": \"medium\",\n      \"description\": \"Cache key validation (line 154) strips whitespace but doesn't validate the key format. An all-whitespace key could pass through after stripping.\",\n      \"line_number\": 154,\n      \"suggestion\": \"Add validation after strip: if not resolved_cache_key: raise EvaluationError('Cache key must be non-empty after stripping')\"\n    },\n    {\n      \"type\": \"Type Safety\",\n      \"severity\": \"medium\",\n      \"description\": \"Type hints for response object are 'Any' (line 160), making it difficult to understand expected structure. Response handling is defensive but fragile.\",\n      \"line_number\": 160,\n      \"suggestion\": \"Define a TypedDict or dataclass for the expected response structure, or use a more specific type from litellm if available.\"\n    },\n    {\n      \"type\": \"Readability\",\n      \"severity\": \"medium\",\n      \"description\": \"Complex nested conditional logic for cache handling (lines 147-158) is hard to follow. Multiple boolean flags and state tracking.\",\n      \"line_number\": [147, 158],\n      \"suggestion\": \"Extract cache logic into a separate private method: '_get_or_fetch_evaluation_content()' to improve readability.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"JSON parsing error (line 237) catches JSONDecodeError but the markdown fence stripping (lines 227-233) may not handle all edge cases (e.g., nested code blocks, incomplete fences).\",\n      \"line_number\": [227, 237],\n      \"suggestion\": \"Add more robust markdown fence handling with regex or consider using a dedicated markdown parser. Add logging for fence-stripping actions.\"\n    },\n    {\n      \"type\": \"Functionality\",\n      \"severity\": \"medium\",\n      \"description\": \"Score normalization (line 275) clamps values but doesn't validate that max_score is positive. Division by zero is possible if max_score is 0.\",\n      \"line_number\": 275,\n      \"suggestion\": \"Add validation: if max_score <= 0: raise ValueError('max_score must be positive')\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Inconsistent error message formatting. Some use f-strings, others use format placeholders. Some messages end with periods, others don't.\",\n      \"line_number\": [48, 51, 54, 100, 105],\n      \"suggestion\": \"Standardize error messages: use f-strings consistently and ensure all end with periods for consistency.\"\n    },\n    {\n      \"type\": \"Testing\",\n      \"severity\": \"low\",\n      \"description\": \"Pragma comment 'pragma: no cover' on line 163 suggests external dependency testing is skipped, but retry logic is critical and should be testable.\",\n      \"line_number\": 163,\n      \"suggestion\": \"Consider using dependency injection or mocking to make retry logic testable without relying on actual LiteLLM calls.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"Regex search in _extract_max_score (line 258) is called for every dimension. Could be optimized by caching or pre-compiling the pattern.\",\n      \"line_number\": 258,\n      \"suggestion\": \"Compile regex pattern as class constant: _SCALE_PATTERN = re.compile(r'(\\\\d+(?:\\\\.\\\\d+)?)\\\\s*-\\\\s*(\\\\d+(?:\\\\.\\\\d+)?)')\"\n    },\n    {\n      \"type\": \"Security\",\n      \"severity\": \"low\",\n      \"description\": \"User-provided extra_instructions (line 108) are directly interpolated into the prompt without sanitization. Could be exploited for prompt injection.\",\n      \"line_number\": 108,\n      \"suggestion\": \"Document that extra_instructions should be trusted input. If untrusted, consider escaping or validating the content.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Magic string 'json_object' in response_format (line 171) should be a constant.\",\n      \"line_number\": 171,\n      \"suggestion\": \"Add constant: RESPONSE_FORMAT = {'type': 'json_object'}\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"category\": \"Error Handling\",\n      \"priority\": \"high\",\n      \"description\": \"Create a custom exception hierarchy for different failure modes (e.g., EvaluationCacheError, EvaluationParseError) to enable more granular error handling by callers.\"\n    },\n    {\n      \"category\": \"Testing\",\n      \"priority\": \"high\",\n      \"description\": \"Add integration tests for the full evaluation pipeline with mocked LiteLLM responses. Current design makes this difficult due to tight coupling.\"\n    },\n    {\n      \"category\": \"Documentation\",\n      \"priority\": \"medium\",\n      \"description\": \"Add docstrings to private methods (_build_prompt, _extract_response_content, etc.) explaining expected formats and error conditions.\"\n    },\n    {\n      \"category\": \"Robustness\",\n      \"priority\": \"medium\",\n      \"description\": \"Add timeout parameter to LiteLLM call (line 169) to prevent indefinite hangs. Consider adding a configurable timeout in __init__.\"\n    },\n    {\n      \"category\": \"Logging\",\n      \"priority\": \"medium\",\n      \"description\": \"Add debug logging for parsed dimension scores before normalization to aid troubleshooting of score mismatches.\"\n    },\n    {\n      \"category\": \"Code Quality\",\n      \"priority\": \"low\",\n      \"description\": \"Consider using Pydantic models for RubricDimension and response payloads to get automatic validation and better IDE support.\"\n    },\n    {\n      \"category\": \"Performance\",\n      \"priority\": \"low\",\n      \"description\": \"Cache the compiled regex pattern and consider caching rubric formatting results if the same rubric is used multiple times.\"\n    }\n  ]\n}\n```",
        "test_config": {
          "test_number": 2,
          "config": {
            "temperature": 0.3,
            "model": "openrouter/anthropic/claude-haiku-4.5",
            "context_size": "file_only",
            "generation_strategy": "chain_of_thought"
          }
        },
        "workflow": {
          "started_at": "2025-10-26T06:21:13.002819Z",
          "completed_at": "2025-10-26T06:21:28.797757Z",
          "duration_seconds": 15.794765084021492
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-powered rubrics. Score extraction and normalization logic appears sound. However, several edge cases are inadequately handled: (1) assertions used for control flow (lines 119, 155, 182) can be disabled with -O flag, breaking runtime validation; (2) markdown fence stripping (lines 227-233) may fail on nested code blocks; (3) division by zero possible if max_score is 0 (line 275); (4) bare exception catch (line 163) may mask critical failures. These issues prevent reliable handling of malformed LLM responses and edge cases."
            },
            "error_handling": {
              "score": 58,
              "reasoning": "Error handling has significant gaps. Critical issues: (1) Assertions used instead of explicit validation—disabled with -O flag; (2) Bare except clause catches KeyboardInterrupt and SystemExit; (3) No validation that max_score > 0 before division; (4) Cache key validation strips whitespace but doesn't check if result is empty; (5) Type hints for response are 'Any', making defensive programming fragile; (6) No custom exception hierarchy for different failure modes; (7) Limited logging for debugging. While EvaluationError is used in some places, the overall approach lacks consistency and rigor."
            },
            "code_quality": {
              "score": 64,
              "reasoning": "Code has reasonable structure but several maintainability issues: (1) Complex nested conditional logic for cache handling (lines 147-158) should be extracted to private method; (2) Type hints use 'Any' for response object, reducing clarity; (3) Magic strings ('json_object', line 171) should be constants; (4) Inconsistent error message formatting (f-strings vs format, periods inconsistent); (5) No docstrings for private methods; (6) Assertions used for type narrowing instead of explicit checks. Positive aspects: clear method names, reasonable separation of concerns, Pydantic used for RubricDimension. Overall, code is readable but has design debt."
            },
            "performance": {
              "score": 68,
              "reasoning": "Performance is generally adequate but has optimization opportunities: (1) Caching support is well-implemented with configurable TTL; (2) Regex pattern in _extract_max_score (line 258) is called per dimension without pre-compilation—should be class constant; (3) Rubric formatting could be cached if same rubric used multiple times; (4) No timeout parameter on LiteLLM call (line 169) risks indefinite hangs; (5) Minimal unnecessary LLM calls by design. Token usage appears reasonable. No obvious bottlenecks, but regex optimization and timeout handling would improve robustness."
            },
            "testability": {
              "score": 55,
              "reasoning": "Testability is limited by design choices: (1) Tight coupling to LiteLLM makes unit testing difficult; (2) Pragma comment 'no cover' on line 163 suggests retry logic is untestable; (3) Assertions used for validation make mocking harder; (4) No dependency injection for LLM client; (5) Complex cache logic (lines 147-158) is hard to test in isolation; (6) Type hints use 'Any' for response, making contracts unclear. Positive aspects: clear method interfaces, pure functions for score extraction possible. Would require significant refactoring to enable comprehensive testing without actual LiteLLM calls."
            },
            "documentation": {
              "score": 61,
              "reasoning": "Documentation is incomplete: (1) No docstrings for private methods (_build_prompt, _extract_response_content, _extract_max_score, etc.); (2) Public method docstrings exist but lack detail on error conditions and edge cases; (3) Type hints present but use 'Any' in critical places (response object), reducing clarity; (4) No inline comments explaining complex logic (cache handling, markdown fence stripping); (5) No examples provided for expected rubric format or response structure; (6) Variable names are generally clear but some are terse (e.g., 'exc'). Prompt injection risk in extra_instructions (line 108) is not documented. Overall, documentation is sparse for a complex evaluator."
            }
          },
          "temperature": 0.3,
          "cache_key": "03d756ab5f618c83e8c0fd6e01f7c6ea1e077f1b8edf1d24fe48c9a3cb56813d",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "timestamp": "2025-10-26T06:21:39.750710Z"
    },
    {
      "test_number": 3,
      "config": {
        "test_number": 3,
        "config_values": {
          "temperature": 0.3,
          "model": "openrouter/anthropic/claude-sonnet-4.5",
          "context_size": "full_module",
          "generation_strategy": "standard"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.82,
            "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs against rubrics with LLM assistance. It handles most edge cases including malformed responses, missing scores, and score clamping. However, the review identifies a critical gap: default max_score of 100.0 may not match actual rubric scales, causing incorrect normalization. The score extraction logic is generally robust but lacks validation that all rubric dimensions are present in responses before processing. These issues prevent a higher score despite the overall sound design."
          },
          "error_handling": {
            "score": 0.75,
            "reasoning": "The code demonstrates good defensive programming with input validation and descriptive error messages. It uses appropriate EvaluationError exceptions and includes logging. However, there are notable weaknesses: (1) broad exception catching in _request_with_retry catches all exceptions including system errors like KeyboardInterrupt, which should propagate; (2) cache key validation occurs after cache operations are determined, creating confusing error message ordering; (3) no validation that all rubric dimensions are present before processing. These issues prevent higher marks despite generally solid error handling practices."
          },
          "code_quality": {
            "score": 0.7,
            "reasoning": "The code shows good fundamentals: clear separation of concerns (cache, metrics, exceptions modules), appropriate type hints, and good naming conventions. However, it has significant structural issues: (1) the evaluate() method exceeds 100 lines and handles multiple responsibilities (validation, caching, API calls, parsing), violating Single Responsibility Principle; (2) multiple isinstance checks for Mapping vs object attributes create code duplication; (3) magic numbers (2, 0.25) in retry logic are not configurable or documented; (4) assertion statements used for type narrowing should be explicit checks. These maintainability concerns prevent a higher score."
          },
          "performance": {
            "score": 0.78,
            "reasoning": "The code implements good performance practices: caching strategy is well-designed with separate read/write controls, minimizing unnecessary LLM calls; exponential backoff with jitter is industry best practice. However, there are optimization opportunities: (1) JSON parsing uses inefficient string operations for markdown stripping instead of regex; (2) string concatenation in _build_prompt could be inefficient for large outputs (should use list + join); (3) no rate limiting or cost controls exist, risking runaway API costs. The caching and retry logic are solid, but micro-optimizations and safety controls are missing."
          },
          "testability": {
            "score": 0.68,
            "reasoning": "The code has reasonable potential for testing with clear interfaces and modular exceptions/cache abstractions. However, significant testability issues exist: (1) retry logic uses random jitter making tests non-deterministic and unreliable; (2) the long evaluate() method with multiple responsibilities is difficult to unit test; (3) no dependency injection for random number generation or metrics callbacks; (4) no dry-run mode for testing prompts without API calls; (5) prompt injection vulnerability makes it hard to test with untrusted inputs safely. These issues substantially limit test coverage potential and reproducibility of edge cases."
          },
          "documentation": {
            "score": 0.76,
            "reasoning": "The code includes type hints throughout and appears to have docstrings for public APIs. Variable and method names are generally clear. However, documentation gaps exist: (1) magic numbers (2, 0.25) in retry logic lack explanation; (2) no inline comments for complex logic like score normalization or JSON parsing; (3) no documentation of the exponential backoff strategy; (4) the default max_score fallback behavior is not documented; (5) no examples of rubric structure or expected response format. The type hints are accurate but insufficient to fully compensate for missing explanatory documentation."
          }
        },
        "overall_score": 0.7483333333333334,
        "evaluator_model": "openrouter/anthropic/claude-haiku-4.5",
        "timestamp": "2025-10-26T06:22:29.992408Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 82,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs against rubrics with LLM assistance. It handles most edge cases including malformed responses, missing scores, and score clamping. However, the review identifies a critical gap: default max_score of 100.0 may not match actual rubric scales, causing incorrect normalization. The score extraction logic is generally robust but lacks validation that all rubric dimensions are present in responses before processing. These issues prevent a higher score despite the overall sound design."
            },
            "error_handling": {
              "score": 75,
              "reasoning": "The code demonstrates good defensive programming with input validation and descriptive error messages. It uses appropriate EvaluationError exceptions and includes logging. However, there are notable weaknesses: (1) broad exception catching in _request_with_retry catches all exceptions including system errors like KeyboardInterrupt, which should propagate; (2) cache key validation occurs after cache operations are determined, creating confusing error message ordering; (3) no validation that all rubric dimensions are present before processing. These issues prevent higher marks despite generally solid error handling practices."
            },
            "code_quality": {
              "score": 70,
              "reasoning": "The code shows good fundamentals: clear separation of concerns (cache, metrics, exceptions modules), appropriate type hints, and good naming conventions. However, it has significant structural issues: (1) the evaluate() method exceeds 100 lines and handles multiple responsibilities (validation, caching, API calls, parsing), violating Single Responsibility Principle; (2) multiple isinstance checks for Mapping vs object attributes create code duplication; (3) magic numbers (2, 0.25) in retry logic are not configurable or documented; (4) assertion statements used for type narrowing should be explicit checks. These maintainability concerns prevent a higher score."
            },
            "performance": {
              "score": 78,
              "reasoning": "The code implements good performance practices: caching strategy is well-designed with separate read/write controls, minimizing unnecessary LLM calls; exponential backoff with jitter is industry best practice. However, there are optimization opportunities: (1) JSON parsing uses inefficient string operations for markdown stripping instead of regex; (2) string concatenation in _build_prompt could be inefficient for large outputs (should use list + join); (3) no rate limiting or cost controls exist, risking runaway API costs. The caching and retry logic are solid, but micro-optimizations and safety controls are missing."
            },
            "testability": {
              "score": 68,
              "reasoning": "The code has reasonable potential for testing with clear interfaces and modular exceptions/cache abstractions. However, significant testability issues exist: (1) retry logic uses random jitter making tests non-deterministic and unreliable; (2) the long evaluate() method with multiple responsibilities is difficult to unit test; (3) no dependency injection for random number generation or metrics callbacks; (4) no dry-run mode for testing prompts without API calls; (5) prompt injection vulnerability makes it hard to test with untrusted inputs safely. These issues substantially limit test coverage potential and reproducibility of edge cases."
            },
            "documentation": {
              "score": 76,
              "reasoning": "The code includes type hints throughout and appears to have docstrings for public APIs. Variable and method names are generally clear. However, documentation gaps exist: (1) magic numbers (2, 0.25) in retry logic lack explanation; (2) no inline comments for complex logic like score normalization or JSON parsing; (3) no documentation of the exponential backoff strategy; (4) the default max_score fallback behavior is not documented; (5) no examples of rubric structure or expected response format. The type hints are accurate but insufficient to fully compensate for missing explanatory documentation."
            }
          },
          "temperature": 0.3,
          "cache_key": "decf33eab723ca978ab8ee4a10b88cb6636323483c05eb26f8f4c192e50d6080",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "cost": 0.0,
      "latency": 40549.89024999668,
      "utility": 0.7310056307901062,
      "workflow_output": "# Code Review: RubricEvaluator\n\n## Summary\nThis is a well-structured, production-quality LLM evaluator with strong error handling, caching support, and retry logic. The code demonstrates good practices including comprehensive validation, logging, and type hints. However, there are opportunities to improve testability, reduce complexity, and enhance security around prompt injection.\n\n## Issues\n\n```json\n{\n  \"issues\": [\n    {\n      \"type\": \"Security\",\n      \"severity\": \"high\",\n      \"description\": \"Potential prompt injection vulnerability - workflow_output and extra_instructions are directly concatenated into prompts without sanitization\",\n      \"line_number\": 158,\n      \"suggestion\": \"Implement input sanitization to escape or validate user-provided content. Consider using structured prompting or escaping special characters that could manipulate the LLM's behavior.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"Broad exception catching in _request_with_retry catches all exceptions including system errors like KeyboardInterrupt\",\n      \"line_number\": 177,\n      \"suggestion\": \"Catch specific exceptions (e.g., litellm.exceptions.RateLimitError, litellm.exceptions.APIError) and allow critical system exceptions to propagate. Use 'except Exception' only after excluding BaseException subclasses.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"medium\",\n      \"description\": \"JSON parsing with markdown stripping uses inefficient string operations and doesn't handle nested code blocks\",\n      \"line_number\": 264,\n      \"suggestion\": \"Use regex for more robust markdown fence removal: re.sub(r'^```(?:json)?\\\\n?|```$', '', content.strip(), flags=re.MULTILINE)\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"medium\",\n      \"description\": \"The evaluate() method is too long (100+ lines) and handles multiple responsibilities (validation, caching, API calls, parsing)\",\n      \"line_number\": 68,\n      \"suggestion\": \"Extract separate methods: _validate_inputs(), _handle_cache_read(), _handle_cache_write(), _call_llm_with_cache() to improve readability and testability.\"\n    },\n    {\n      \"type\": \"Testing\",\n      \"severity\": \"medium\",\n      \"description\": \"Retry logic uses random jitter making it non-deterministic and difficult to test reliably\",\n      \"line_number\": 200,\n      \"suggestion\": \"Add a seed parameter or dependency injection for the random number generator to enable deterministic testing: random.Random(seed) or accept a jitter_fn callable.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"low\",\n      \"description\": \"Cache key validation happens after cache operations are determined, potentially causing confusing error messages\",\n      \"line_number\": 127,\n      \"suggestion\": \"Validate cache_key earlier, immediately after parameter validation, before computing effective_use_cache.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Multiple isinstance checks for Mapping vs object attributes creates code duplication\",\n      \"line_number\": 233,\n      \"suggestion\": \"Create a helper method _safe_get(obj, key, default=None) that handles both dict-like and object attribute access uniformly.\"\n    },\n    {\n      \"type\": \"Functionality\",\n      \"severity\": \"low\",\n      \"description\": \"Default max_score of 100.0 in _extract_max_score may not match actual rubric scales, causing incorrect normalization\",\n      \"line_number\": 310,\n      \"suggestion\": \"Log a warning when falling back to default max_score, or make it configurable per dimension. Consider raising an error for unparseable scales in strict mode.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"String concatenation in _build_prompt could be inefficient for large outputs\",\n      \"line_number\": 204,\n      \"suggestion\": \"Use a list and ''.join() or io.StringIO for building large prompts, especially if workflow_output can be very large.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Magic numbers in retry delay calculation (2, 0.25) are not configurable or documented\",\n      \"line_number\": 200,\n      \"suggestion\": \"Extract as class constants: RETRY_BACKOFF_MULTIPLIER = 2, RETRY_JITTER_FACTOR = 0.25 with docstring explaining the exponential backoff strategy.\"\n    },\n    {\n      \"type\": \"Readability\",\n      \"severity\": \"low\",\n      \"description\": \"Assertion statements used for type narrowing could be replaced with explicit checks\",\n      \"line_number\": 152,\n      \"suggestion\": \"Replace 'assert content is not None' with explicit if-checks and raise appropriate exceptions. Assertions can be disabled with -O flag.\"\n    },\n    {\n      \"type\": \"Security\",\n      \"severity\": \"medium\",\n      \"description\": \"No rate limiting or cost controls for LLM API calls\",\n      \"line_number\": 68,\n      \"suggestion\": \"Add optional rate limiting (requests per minute) and cost tracking/limits to prevent runaway API costs from malicious or buggy usage.\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"description\": \"Add input validation for workflow_output length to prevent excessive token usage and costs\",\n      \"example\": \"Add max_input_length parameter and validate: if len(workflow_output) > max_input_length: raise EvaluationError('Input exceeds maximum length')\"\n    },\n    {\n      \"description\": \"Implement structured logging with correlation IDs for better debugging across retries and cache operations\",\n      \"example\": \"Generate a request_id = uuid.uuid4() at the start of evaluate() and include it in all log messages\"\n    },\n    {\n      \"description\": \"Add metrics/observability hooks for monitoring cache hit rates, retry counts, and evaluation latency\",\n      \"example\": \"Accept optional metrics_callback parameter and call it with evaluation metadata: metrics_callback({'cache_hit': cache_hit, 'retries': attempt, 'duration_ms': elapsed})\"\n    },\n    {\n      \"description\": \"Consider adding async context manager support for resource cleanup\",\n      \"example\": \"Implement __aenter__ and __aexit__ to properly manage cache backend lifecycle if it requires cleanup\"\n    },\n    {\n      \"description\": \"Add validation that all rubric dimensions are present in the response before processing\",\n      \"example\": \"missing = set(rubric.keys()) - set(payload.keys()); if missing: raise EvaluationError(f'Missing dimensions: {missing}')\"\n    },\n    {\n      \"description\": \"Implement a dry-run mode for testing prompts without making actual API calls\",\n      \"example\": \"Add dry_run parameter that returns a mock QualityScore with the generated prompt in metadata\"\n    },\n    {\n      \"description\": \"Add type validation for rubric structure to fail fast on malformed rubrics\",\n      \"example\": \"Create a _validate_rubric() method that checks for required keys ('description', 'scale') in each dimension\"\n    },\n    {\n      \"description\": \"Consider making the JSON response template configurable for different LLM providers that may prefer different formats\",\n      \"example\": \"Add response_template parameter with a default that can be overridden for specific models\"\n    }\n  ]\n}\n```\n\n## Positive Aspects\n\n1. **Excellent error handling**: Comprehensive validation with descriptive error messages\n2. **Strong type hints**: Good use of Optional, Mapping, and type annotations throughout\n3. **Robust retry logic**: Exponential backoff with jitter is industry best practice\n4. **Flexible caching**: Well-designed cache abstraction with separate read/write controls\n5. **Good logging**: Appropriate log levels and informative messages\n6. **Defensive programming**: Validates inputs, handles edge cases, clamps scores\n7. **Clean separation**: Cache, metrics, and exceptions are properly modularized\n\n## Priority Recommendations\n\n1. **High Priority**: Address prompt injection vulnerability with input sanitization\n2. **High Priority**: Refactor the long `evaluate()` method into smaller, testable units\n3. **Medium Priority**: Make retry logic deterministic for testing\n4. **Medium Priority**: Add rate limiting and cost controls for production safety",
      "metadata": {
        "strategy": "standard",
        "model": "openrouter/anthropic/claude-sonnet-4.5",
        "temperature": 0.3,
        "context_size": "full_module",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Consider surrounding module context for the review.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "# Code Review: RubricEvaluator\n\n## Summary\nThis is a well-structured, production-quality LLM evaluator with strong error handling, caching support, and retry logic. The code demonstrates good practices including comprehensive validation, logging, and type hints. However, there are opportunities to improve testability, reduce complexity, and enhance security around prompt injection.\n\n## Issues\n\n```json\n{\n  \"issues\": [\n    {\n      \"type\": \"Security\",\n      \"severity\": \"high\",\n      \"description\": \"Potential prompt injection vulnerability - workflow_output and extra_instructions are directly concatenated into prompts without sanitization\",\n      \"line_number\": 158,\n      \"suggestion\": \"Implement input sanitization to escape or validate user-provided content. Consider using structured prompting or escaping special characters that could manipulate the LLM's behavior.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"Broad exception catching in _request_with_retry catches all exceptions including system errors like KeyboardInterrupt\",\n      \"line_number\": 177,\n      \"suggestion\": \"Catch specific exceptions (e.g., litellm.exceptions.RateLimitError, litellm.exceptions.APIError) and allow critical system exceptions to propagate. Use 'except Exception' only after excluding BaseException subclasses.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"medium\",\n      \"description\": \"JSON parsing with markdown stripping uses inefficient string operations and doesn't handle nested code blocks\",\n      \"line_number\": 264,\n      \"suggestion\": \"Use regex for more robust markdown fence removal: re.sub(r'^```(?:json)?\\\\n?|```$', '', content.strip(), flags=re.MULTILINE)\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"medium\",\n      \"description\": \"The evaluate() method is too long (100+ lines) and handles multiple responsibilities (validation, caching, API calls, parsing)\",\n      \"line_number\": 68,\n      \"suggestion\": \"Extract separate methods: _validate_inputs(), _handle_cache_read(), _handle_cache_write(), _call_llm_with_cache() to improve readability and testability.\"\n    },\n    {\n      \"type\": \"Testing\",\n      \"severity\": \"medium\",\n      \"description\": \"Retry logic uses random jitter making it non-deterministic and difficult to test reliably\",\n      \"line_number\": 200,\n      \"suggestion\": \"Add a seed parameter or dependency injection for the random number generator to enable deterministic testing: random.Random(seed) or accept a jitter_fn callable.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"low\",\n      \"description\": \"Cache key validation happens after cache operations are determined, potentially causing confusing error messages\",\n      \"line_number\": 127,\n      \"suggestion\": \"Validate cache_key earlier, immediately after parameter validation, before computing effective_use_cache.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Multiple isinstance checks for Mapping vs object attributes creates code duplication\",\n      \"line_number\": 233,\n      \"suggestion\": \"Create a helper method _safe_get(obj, key, default=None) that handles both dict-like and object attribute access uniformly.\"\n    },\n    {\n      \"type\": \"Functionality\",\n      \"severity\": \"low\",\n      \"description\": \"Default max_score of 100.0 in _extract_max_score may not match actual rubric scales, causing incorrect normalization\",\n      \"line_number\": 310,\n      \"suggestion\": \"Log a warning when falling back to default max_score, or make it configurable per dimension. Consider raising an error for unparseable scales in strict mode.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"String concatenation in _build_prompt could be inefficient for large outputs\",\n      \"line_number\": 204,\n      \"suggestion\": \"Use a list and ''.join() or io.StringIO for building large prompts, especially if workflow_output can be very large.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Magic numbers in retry delay calculation (2, 0.25) are not configurable or documented\",\n      \"line_number\": 200,\n      \"suggestion\": \"Extract as class constants: RETRY_BACKOFF_MULTIPLIER = 2, RETRY_JITTER_FACTOR = 0.25 with docstring explaining the exponential backoff strategy.\"\n    },\n    {\n      \"type\": \"Readability\",\n      \"severity\": \"low\",\n      \"description\": \"Assertion statements used for type narrowing could be replaced with explicit checks\",\n      \"line_number\": 152,\n      \"suggestion\": \"Replace 'assert content is not None' with explicit if-checks and raise appropriate exceptions. Assertions can be disabled with -O flag.\"\n    },\n    {\n      \"type\": \"Security\",\n      \"severity\": \"medium\",\n      \"description\": \"No rate limiting or cost controls for LLM API calls\",\n      \"line_number\": 68,\n      \"suggestion\": \"Add optional rate limiting (requests per minute) and cost tracking/limits to prevent runaway API costs from malicious or buggy usage.\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"description\": \"Add input validation for workflow_output length to prevent excessive token usage and costs\",\n      \"example\": \"Add max_input_length parameter and validate: if len(workflow_output) > max_input_length: raise EvaluationError('Input exceeds maximum length')\"\n    },\n    {\n      \"description\": \"Implement structured logging with correlation IDs for better debugging across retries and cache operations\",\n      \"example\": \"Generate a request_id = uuid.uuid4() at the start of evaluate() and include it in all log messages\"\n    },\n    {\n      \"description\": \"Add metrics/observability hooks for monitoring cache hit rates, retry counts, and evaluation latency\",\n      \"example\": \"Accept optional metrics_callback parameter and call it with evaluation metadata: metrics_callback({'cache_hit': cache_hit, 'retries': attempt, 'duration_ms': elapsed})\"\n    },\n    {\n      \"description\": \"Consider adding async context manager support for resource cleanup\",\n      \"example\": \"Implement __aenter__ and __aexit__ to properly manage cache backend lifecycle if it requires cleanup\"\n    },\n    {\n      \"description\": \"Add validation that all rubric dimensions are present in the response before processing\",\n      \"example\": \"missing = set(rubric.keys()) - set(payload.keys()); if missing: raise EvaluationError(f'Missing dimensions: {missing}')\"\n    },\n    {\n      \"description\": \"Implement a dry-run mode for testing prompts without making actual API calls\",\n      \"example\": \"Add dry_run parameter that returns a mock QualityScore with the generated prompt in metadata\"\n    },\n    {\n      \"description\": \"Add type validation for rubric structure to fail fast on malformed rubrics\",\n      \"example\": \"Create a _validate_rubric() method that checks for required keys ('description', 'scale') in each dimension\"\n    },\n    {\n      \"description\": \"Consider making the JSON response template configurable for different LLM providers that may prefer different formats\",\n      \"example\": \"Add response_template parameter with a default that can be overridden for specific models\"\n    }\n  ]\n}\n```\n\n## Positive Aspects\n\n1. **Excellent error handling**: Comprehensive validation with descriptive error messages\n2. **Strong type hints**: Good use of Optional, Mapping, and type annotations throughout\n3. **Robust retry logic**: Exponential backoff with jitter is industry best practice\n4. **Flexible caching**: Well-designed cache abstraction with separate read/write controls\n5. **Good logging**: Appropriate log levels and informative messages\n6. **Defensive programming**: Validates inputs, handles edge cases, clamps scores\n7. **Clean separation**: Cache, metrics, and exceptions are properly modularized\n\n## Priority Recommendations\n\n1. **High Priority**: Address prompt injection vulnerability with input sanitization\n2. **High Priority**: Refactor the long `evaluate()` method into smaller, testable units\n3. **Medium Priority**: Make retry logic deterministic for testing\n4. **Medium Priority**: Add rate limiting and cost controls for production safety",
        "test_config": {
          "test_number": 3,
          "config": {
            "temperature": 0.3,
            "model": "openrouter/anthropic/claude-sonnet-4.5",
            "context_size": "full_module",
            "generation_strategy": "standard"
          }
        },
        "workflow": {
          "started_at": "2025-10-26T06:21:39.753613Z",
          "completed_at": "2025-10-26T06:22:20.303950Z",
          "duration_seconds": 40.54989024999668
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 82,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs against rubrics with LLM assistance. It handles most edge cases including malformed responses, missing scores, and score clamping. However, the review identifies a critical gap: default max_score of 100.0 may not match actual rubric scales, causing incorrect normalization. The score extraction logic is generally robust but lacks validation that all rubric dimensions are present in responses before processing. These issues prevent a higher score despite the overall sound design."
            },
            "error_handling": {
              "score": 75,
              "reasoning": "The code demonstrates good defensive programming with input validation and descriptive error messages. It uses appropriate EvaluationError exceptions and includes logging. However, there are notable weaknesses: (1) broad exception catching in _request_with_retry catches all exceptions including system errors like KeyboardInterrupt, which should propagate; (2) cache key validation occurs after cache operations are determined, creating confusing error message ordering; (3) no validation that all rubric dimensions are present before processing. These issues prevent higher marks despite generally solid error handling practices."
            },
            "code_quality": {
              "score": 70,
              "reasoning": "The code shows good fundamentals: clear separation of concerns (cache, metrics, exceptions modules), appropriate type hints, and good naming conventions. However, it has significant structural issues: (1) the evaluate() method exceeds 100 lines and handles multiple responsibilities (validation, caching, API calls, parsing), violating Single Responsibility Principle; (2) multiple isinstance checks for Mapping vs object attributes create code duplication; (3) magic numbers (2, 0.25) in retry logic are not configurable or documented; (4) assertion statements used for type narrowing should be explicit checks. These maintainability concerns prevent a higher score."
            },
            "performance": {
              "score": 78,
              "reasoning": "The code implements good performance practices: caching strategy is well-designed with separate read/write controls, minimizing unnecessary LLM calls; exponential backoff with jitter is industry best practice. However, there are optimization opportunities: (1) JSON parsing uses inefficient string operations for markdown stripping instead of regex; (2) string concatenation in _build_prompt could be inefficient for large outputs (should use list + join); (3) no rate limiting or cost controls exist, risking runaway API costs. The caching and retry logic are solid, but micro-optimizations and safety controls are missing."
            },
            "testability": {
              "score": 68,
              "reasoning": "The code has reasonable potential for testing with clear interfaces and modular exceptions/cache abstractions. However, significant testability issues exist: (1) retry logic uses random jitter making tests non-deterministic and unreliable; (2) the long evaluate() method with multiple responsibilities is difficult to unit test; (3) no dependency injection for random number generation or metrics callbacks; (4) no dry-run mode for testing prompts without API calls; (5) prompt injection vulnerability makes it hard to test with untrusted inputs safely. These issues substantially limit test coverage potential and reproducibility of edge cases."
            },
            "documentation": {
              "score": 76,
              "reasoning": "The code includes type hints throughout and appears to have docstrings for public APIs. Variable and method names are generally clear. However, documentation gaps exist: (1) magic numbers (2, 0.25) in retry logic lack explanation; (2) no inline comments for complex logic like score normalization or JSON parsing; (3) no documentation of the exponential backoff strategy; (4) the default max_score fallback behavior is not documented; (5) no examples of rubric structure or expected response format. The type hints are accurate but insufficient to fully compensate for missing explanatory documentation."
            }
          },
          "temperature": 0.3,
          "cache_key": "decf33eab723ca978ab8ee4a10b88cb6636323483c05eb26f8f4c192e50d6080",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "timestamp": "2025-10-26T06:22:29.992845Z"
    },
    {
      "test_number": 4,
      "config": {
        "test_number": 4,
        "config_values": {
          "temperature": 0.3,
          "model": "openrouter/anthropic/claude-sonnet-4.5",
          "context_size": "full_module",
          "generation_strategy": "chain_of_thought"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.72,
            "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-powered rubrics with retry logic and caching. However, critical edge cases are inadequately handled: (1) No validation of rubric structure allows runtime errors when 'description' or 'scale' keys are missing, (2) Malformed LLM responses could cause silent failures with the broad exception handling, (3) Score extraction defaults to 100.0 without warning when parsing fails, masking data quality issues. The normalization logic appears correct for 0-1 range conversion, but lacks defensive checks. The default rubric is reasonable but untested against edge cases."
          },
          "error_handling": {
            "score": 0.58,
            "reasoning": "Error handling is partially implemented but has significant gaps: (1) Broad exception catching in _request_with_retry masks important errors and prevents proper error categorization, (2) No validation of rubric structure before use leads to potential runtime failures, (3) Missing timeout configuration could cause indefinite hangs, (4) Error messages lack sufficient context (model, dimension, attempt number), (5) Silent defaults (100.0 for unparseable scales) hide data quality issues. Custom EvaluationError exists but is underutilized. Logging is mentioned but not comprehensively implemented for debugging failed evaluations."
          },
          "code_quality": {
            "score": 0.68,
            "reasoning": "Code demonstrates good Python practices with type hints, dataclasses, and PEP 8 compliance. Separation of concerns is present (caching, retry logic, parsing). However: (1) The evaluate() method exceeds 100 lines, violating Single Responsibility Principle by handling validation, caching, API calls, and parsing, (2) RubricDimension TypedDict lacks total=True enforcement, creating type safety gaps, (3) Markdown fence stripping is fragile with regex, (4) Cache key generation lacks documented collision resistance, (5) No abstraction for rubric validation. Method naming is clear but some abstractions could be cleaner."
          },
          "performance": {
            "score": 0.75,
            "reasoning": "Performance is generally sound with appropriate async/await patterns, caching implementation, and exponential backoff for retries. Caching with hit/miss tracking minimizes unnecessary LLM calls. However: (1) Converting messages to list on every retry attempt is wasteful (should be outside loop), (2) No timeout configuration for LLM calls risks resource exhaustion, (3) No batch evaluation support limits throughput for multiple evaluations, (4) No rate limiting to prevent API overwhelming, (5) Regex patterns appear reasonable but markdown fence stripping could be optimized. Token usage is not explicitly managed or monitored."
          },
          "testability": {
            "score": 0.64,
            "reasoning": "Code has testable foundations with dependency injection (CacheBackend, logger) and async design. However: (1) The 100+ line evaluate() method is difficult to test as a unit; complex logic should be decomposed into smaller, pure functions, (2) LLM response parsing is tightly coupled to API calls, making it hard to test independently, (3) No clear contracts for CacheBackend interface (thread-safety requirements undocumented), (4) Edge cases like malformed JSON and missing keys are hard to reproduce without mocking complex LLM responses, (5) Broad exception handling makes it difficult to test specific error paths. Interfaces exist but could be more granular."
          },
          "documentation": {
            "score": 0.7,
            "reasoning": "Documentation is present but incomplete: (1) Docstrings exist for main methods but lack examples of common usage patterns, (2) Type hints are generally accurate but RubricDimension TypedDict lacks clarity on required vs optional keys, (3) Complex prompt building logic lacks inline comments explaining the structure, (4) No documentation of cache backend requirements or thread-safety guarantees, (5) Error handling paths are not documented (which exceptions can be raised, when), (6) Variable names are clear (workflow_output, rubric_dimensions), but some abbreviations (acompletion) could be clearer. Missing documentation on retry strategy, timeout behavior, and cache collision properties."
          }
        },
        "overall_score": 0.6783333333333333,
        "evaluator_model": "openrouter/anthropic/claude-haiku-4.5",
        "timestamp": "2025-10-26T06:23:25.753008Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-powered rubrics with retry logic and caching. However, critical edge cases are inadequately handled: (1) No validation of rubric structure allows runtime errors when 'description' or 'scale' keys are missing, (2) Malformed LLM responses could cause silent failures with the broad exception handling, (3) Score extraction defaults to 100.0 without warning when parsing fails, masking data quality issues. The normalization logic appears correct for 0-1 range conversion, but lacks defensive checks. The default rubric is reasonable but untested against edge cases."
            },
            "error_handling": {
              "score": 58,
              "reasoning": "Error handling is partially implemented but has significant gaps: (1) Broad exception catching in _request_with_retry masks important errors and prevents proper error categorization, (2) No validation of rubric structure before use leads to potential runtime failures, (3) Missing timeout configuration could cause indefinite hangs, (4) Error messages lack sufficient context (model, dimension, attempt number), (5) Silent defaults (100.0 for unparseable scales) hide data quality issues. Custom EvaluationError exists but is underutilized. Logging is mentioned but not comprehensively implemented for debugging failed evaluations."
            },
            "code_quality": {
              "score": 68,
              "reasoning": "Code demonstrates good Python practices with type hints, dataclasses, and PEP 8 compliance. Separation of concerns is present (caching, retry logic, parsing). However: (1) The evaluate() method exceeds 100 lines, violating Single Responsibility Principle by handling validation, caching, API calls, and parsing, (2) RubricDimension TypedDict lacks total=True enforcement, creating type safety gaps, (3) Markdown fence stripping is fragile with regex, (4) Cache key generation lacks documented collision resistance, (5) No abstraction for rubric validation. Method naming is clear but some abstractions could be cleaner."
            },
            "performance": {
              "score": 75,
              "reasoning": "Performance is generally sound with appropriate async/await patterns, caching implementation, and exponential backoff for retries. Caching with hit/miss tracking minimizes unnecessary LLM calls. However: (1) Converting messages to list on every retry attempt is wasteful (should be outside loop), (2) No timeout configuration for LLM calls risks resource exhaustion, (3) No batch evaluation support limits throughput for multiple evaluations, (4) No rate limiting to prevent API overwhelming, (5) Regex patterns appear reasonable but markdown fence stripping could be optimized. Token usage is not explicitly managed or monitored."
            },
            "testability": {
              "score": 64,
              "reasoning": "Code has testable foundations with dependency injection (CacheBackend, logger) and async design. However: (1) The 100+ line evaluate() method is difficult to test as a unit; complex logic should be decomposed into smaller, pure functions, (2) LLM response parsing is tightly coupled to API calls, making it hard to test independently, (3) No clear contracts for CacheBackend interface (thread-safety requirements undocumented), (4) Edge cases like malformed JSON and missing keys are hard to reproduce without mocking complex LLM responses, (5) Broad exception handling makes it difficult to test specific error paths. Interfaces exist but could be more granular."
            },
            "documentation": {
              "score": 70,
              "reasoning": "Documentation is present but incomplete: (1) Docstrings exist for main methods but lack examples of common usage patterns, (2) Type hints are generally accurate but RubricDimension TypedDict lacks clarity on required vs optional keys, (3) Complex prompt building logic lacks inline comments explaining the structure, (4) No documentation of cache backend requirements or thread-safety guarantees, (5) Error handling paths are not documented (which exceptions can be raised, when), (6) Variable names are clear (workflow_output, rubric_dimensions), but some abbreviations (acompletion) could be clearer. Missing documentation on retry strategy, timeout behavior, and cache collision properties."
            }
          },
          "temperature": 0.3,
          "cache_key": "42bb8d33dacdec39905b45b203b0f6a3c2cc00f186f1d776c0cad716ac42d24e",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "cost": 0.0,
      "latency": 44367.65304100118,
      "utility": 0.6583333333333333,
      "workflow_output": "# Code Review: RubricEvaluator\n\n## Analysis\n\nLet me systematically review this LLM-powered rubric evaluator across all dimensions.\n\n### Functionality\n- ✅ Core evaluation logic appears sound with retry mechanism\n- ✅ Handles caching with hit/miss tracking\n- ✅ Normalizes scores from different scales to 0-1 range\n- ⚠️ Some edge cases need attention (see issues below)\n\n### Code Quality\n- ✅ Good use of type hints and dataclasses\n- ✅ Follows PEP 8 conventions\n- ✅ Clear separation of concerns\n- ⚠️ Some methods are quite long and could be decomposed\n\n### Readability\n- ✅ Well-documented with docstrings\n- ✅ Logical organization of methods\n- ✅ Clear variable naming\n- ⚠️ Complex prompt building could be clearer\n\n### Error Handling\n- ✅ Custom exceptions used appropriately\n- ✅ Validation at key points\n- ⚠️ Some error messages could be more informative\n- ⚠️ Missing validation for rubric structure\n\n### Testing\n- ✅ Design is testable with dependency injection\n- ⚠️ Complex methods would benefit from decomposition for easier testing\n\n### Performance\n- ✅ Async/await used properly\n- ✅ Caching implemented\n- ✅ Exponential backoff for retries\n- ⚠️ No timeout configuration for LLM calls\n\n### Security\n- ✅ Input validation present\n- ⚠️ No sanitization of workflow_output before sending to LLM\n- ⚠️ Cache key generation could be vulnerable to collisions\n\n## JSON Response\n\n```json\n{\n  \"summary\": \"Well-structured LLM evaluator with good async patterns, caching, and retry logic. The code follows Python best practices with comprehensive type hints and error handling. Main concerns include missing rubric validation, potential prompt injection risks, lack of timeout controls, and some overly complex methods that would benefit from decomposition.\",\n  \n  \"issues\": [\n    {\n      \"type\": \"security\",\n      \"severity\": \"high\",\n      \"description\": \"No sanitization of workflow_output before including in LLM prompt, creating potential for prompt injection attacks\",\n      \"line_number\": 157,\n      \"suggestion\": \"Implement input sanitization to escape or validate workflow_output content. Consider adding a max length check and removing potentially malicious patterns that could manipulate the LLM's behavior.\"\n    },\n    {\n      \"type\": \"validation\",\n      \"severity\": \"high\",\n      \"description\": \"No validation of rubric structure - missing 'description' or 'scale' keys would cause runtime errors\",\n      \"line_number\": 89,\n      \"suggestion\": \"Add rubric validation in __init__ or evaluate() to ensure each dimension has required 'description' and 'scale' keys. Example: `if not all('description' in d and 'scale' in d for d in rubric.values()): raise ValueError(...)`\"\n    },\n    {\n      \"type\": \"reliability\",\n      \"severity\": \"medium\",\n      \"description\": \"No timeout configuration for LLM API calls - could hang indefinitely on slow/stuck requests\",\n      \"line_number\": 135,\n      \"suggestion\": \"Add a timeout parameter to __init__ and pass it to litellm.acompletion(). Example: `timeout=self._timeout` with a reasonable default like 60 seconds.\"\n    },\n    {\n      \"type\": \"security\",\n      \"severity\": \"medium\",\n      \"description\": \"Cache key generation using build_cache_key() could have collision risks if not properly implemented\",\n      \"line_number\": 118,\n      \"suggestion\": \"Ensure build_cache_key() uses a cryptographic hash (SHA-256) of all inputs including prompt, model, and temperature. Document the collision resistance properties.\"\n    },\n    {\n      \"type\": \"error_handling\",\n      \"severity\": \"medium\",\n      \"description\": \"Broad exception catching in _request_with_retry could mask important errors and make debugging difficult\",\n      \"line_number\": 145,\n      \"suggestion\": \"Catch specific exceptions (e.g., litellm.Timeout, litellm.RateLimitError, litellm.APIError) and handle them differently. Only retry on transient errors, fail fast on configuration errors.\"\n    },\n    {\n      \"type\": \"functionality\",\n      \"severity\": \"medium\",\n      \"description\": \"_extract_max_score defaults to 100.0 when scale parsing fails, which could silently produce incorrect normalizations\",\n      \"line_number\": 281,\n      \"suggestion\": \"Log a warning when defaulting to 100.0, or raise an error for unparseable scales to ensure rubric authors fix malformed scale definitions.\"\n    },\n    {\n      \"type\": \"maintainability\",\n      \"severity\": \"low\",\n      \"description\": \"The evaluate() method is very long (100+ lines) and handles multiple concerns (validation, caching, API calls, parsing)\",\n      \"line_number\": 55,\n      \"suggestion\": \"Extract helper methods: _validate_inputs(), _resolve_cache_params(), _get_or_fetch_response(). This improves testability and readability.\"\n    },\n    {\n      \"type\": \"robustness\",\n      \"severity\": \"low\",\n      \"description\": \"Markdown fence stripping in _load_json is fragile and only handles simple cases\",\n      \"line_number\": 234,\n      \"suggestion\": \"Use a more robust regex pattern to handle various markdown fence formats: `re.sub(r'^```(?:json)?\\\\n|\\\\n```$', '', content.strip(), flags=re.MULTILINE)`\"\n    },\n    {\n      \"type\": \"observability\",\n      \"severity\": \"low\",\n      \"description\": \"No logging of actual LLM response content for debugging failed evaluations\",\n      \"line_number\": 152,\n      \"suggestion\": \"Add debug logging of the raw LLM response before parsing: `self._logger.debug('LLM response: %s', content[:500])` (truncated for log size)\"\n    },\n    {\n      \"type\": \"type_safety\",\n      \"severity\": \"low\",\n      \"description\": \"RubricDimension type allows arbitrary keys via TypedDict, but code assumes only 'description' and 'scale' exist\",\n      \"line_number\": 260,\n      \"suggestion\": \"Use .get() with defaults when accessing rubric dimension fields, or make RubricDimension a total=True TypedDict to enforce required keys at type-check time.\"\n    },\n    {\n      \"type\": \"performance\",\n      \"severity\": \"low\",\n      \"description\": \"Converting messages to list on every retry attempt is unnecessary\",\n      \"line_number\": 138,\n      \"suggestion\": \"Move `payload = list(messages)` outside the retry loop since messages don't change between attempts.\"\n    },\n    {\n      \"type\": \"usability\",\n      \"severity\": \"low\",\n      \"description\": \"No way to configure different retry strategies per error type (rate limits vs network errors)\",\n      \"line_number\": 135,\n      \"suggestion\": \"Consider adding retry configuration that allows different max_retries and backoff strategies for different error categories.\"\n    }\n  ],\n  \n  \"suggestions\": [\n    \"Add comprehensive input validation helper: Create a `_validate_rubric(rubric)` method that checks structure, required keys, and scale format validity.\",\n    \"Implement request/response logging: Add structured logging with request IDs to trace evaluation flows through cache hits, retries, and errors.\",\n    \"Add metrics/telemetry: Track cache hit rates, retry counts, evaluation latencies, and error rates for monitoring production usage.\",\n    \"Create a RubricValidator class: Separate rubric validation logic into its own class to handle complex validation rules and provide better error messages.\",\n    \"Add configuration validation: Create a `validate_config()` method called in __init__ to catch configuration errors early rather than at runtime.\",\n    \"Implement circuit breaker pattern: If LLM API is consistently failing, temporarily stop making requests to avoid wasting resources and improve error recovery.\",\n    \"Add response schema validation: Use a JSON schema validator to ensure LLM responses match expected structure before parsing.\",\n    \"Consider rate limiting: Add built-in rate limiting to prevent overwhelming the LLM API, especially when evaluating in batch.\",\n    \"Improve error context: Include more context in EvaluationError exceptions (model used, dimension being evaluated, etc.) to aid debugging.\",\n    \"Add batch evaluation support: Implement an `evaluate_batch()` method that efficiently evaluates multiple outputs with proper concurrency control.\",\n    \"Document cache backend requirements: Add clear documentation about what methods CacheBackend must implement and any thread-safety requirements.\",\n    \"Add example usage: Include docstring examples showing common usage patterns, especially around caching and custom rubrics.\"\n  ]\n}\n```",
      "metadata": {
        "strategy": "chain_of_thought",
        "model": "openrouter/anthropic/claude-sonnet-4.5",
        "temperature": 0.3,
        "context_size": "full_module",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Consider surrounding module context for the review.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "# Code Review: RubricEvaluator\n\n## Analysis\n\nLet me systematically review this LLM-powered rubric evaluator across all dimensions.\n\n### Functionality\n- ✅ Core evaluation logic appears sound with retry mechanism\n- ✅ Handles caching with hit/miss tracking\n- ✅ Normalizes scores from different scales to 0-1 range\n- ⚠️ Some edge cases need attention (see issues below)\n\n### Code Quality\n- ✅ Good use of type hints and dataclasses\n- ✅ Follows PEP 8 conventions\n- ✅ Clear separation of concerns\n- ⚠️ Some methods are quite long and could be decomposed\n\n### Readability\n- ✅ Well-documented with docstrings\n- ✅ Logical organization of methods\n- ✅ Clear variable naming\n- ⚠️ Complex prompt building could be clearer\n\n### Error Handling\n- ✅ Custom exceptions used appropriately\n- ✅ Validation at key points\n- ⚠️ Some error messages could be more informative\n- ⚠️ Missing validation for rubric structure\n\n### Testing\n- ✅ Design is testable with dependency injection\n- ⚠️ Complex methods would benefit from decomposition for easier testing\n\n### Performance\n- ✅ Async/await used properly\n- ✅ Caching implemented\n- ✅ Exponential backoff for retries\n- ⚠️ No timeout configuration for LLM calls\n\n### Security\n- ✅ Input validation present\n- ⚠️ No sanitization of workflow_output before sending to LLM\n- ⚠️ Cache key generation could be vulnerable to collisions\n\n## JSON Response\n\n```json\n{\n  \"summary\": \"Well-structured LLM evaluator with good async patterns, caching, and retry logic. The code follows Python best practices with comprehensive type hints and error handling. Main concerns include missing rubric validation, potential prompt injection risks, lack of timeout controls, and some overly complex methods that would benefit from decomposition.\",\n  \n  \"issues\": [\n    {\n      \"type\": \"security\",\n      \"severity\": \"high\",\n      \"description\": \"No sanitization of workflow_output before including in LLM prompt, creating potential for prompt injection attacks\",\n      \"line_number\": 157,\n      \"suggestion\": \"Implement input sanitization to escape or validate workflow_output content. Consider adding a max length check and removing potentially malicious patterns that could manipulate the LLM's behavior.\"\n    },\n    {\n      \"type\": \"validation\",\n      \"severity\": \"high\",\n      \"description\": \"No validation of rubric structure - missing 'description' or 'scale' keys would cause runtime errors\",\n      \"line_number\": 89,\n      \"suggestion\": \"Add rubric validation in __init__ or evaluate() to ensure each dimension has required 'description' and 'scale' keys. Example: `if not all('description' in d and 'scale' in d for d in rubric.values()): raise ValueError(...)`\"\n    },\n    {\n      \"type\": \"reliability\",\n      \"severity\": \"medium\",\n      \"description\": \"No timeout configuration for LLM API calls - could hang indefinitely on slow/stuck requests\",\n      \"line_number\": 135,\n      \"suggestion\": \"Add a timeout parameter to __init__ and pass it to litellm.acompletion(). Example: `timeout=self._timeout` with a reasonable default like 60 seconds.\"\n    },\n    {\n      \"type\": \"security\",\n      \"severity\": \"medium\",\n      \"description\": \"Cache key generation using build_cache_key() could have collision risks if not properly implemented\",\n      \"line_number\": 118,\n      \"suggestion\": \"Ensure build_cache_key() uses a cryptographic hash (SHA-256) of all inputs including prompt, model, and temperature. Document the collision resistance properties.\"\n    },\n    {\n      \"type\": \"error_handling\",\n      \"severity\": \"medium\",\n      \"description\": \"Broad exception catching in _request_with_retry could mask important errors and make debugging difficult\",\n      \"line_number\": 145,\n      \"suggestion\": \"Catch specific exceptions (e.g., litellm.Timeout, litellm.RateLimitError, litellm.APIError) and handle them differently. Only retry on transient errors, fail fast on configuration errors.\"\n    },\n    {\n      \"type\": \"functionality\",\n      \"severity\": \"medium\",\n      \"description\": \"_extract_max_score defaults to 100.0 when scale parsing fails, which could silently produce incorrect normalizations\",\n      \"line_number\": 281,\n      \"suggestion\": \"Log a warning when defaulting to 100.0, or raise an error for unparseable scales to ensure rubric authors fix malformed scale definitions.\"\n    },\n    {\n      \"type\": \"maintainability\",\n      \"severity\": \"low\",\n      \"description\": \"The evaluate() method is very long (100+ lines) and handles multiple concerns (validation, caching, API calls, parsing)\",\n      \"line_number\": 55,\n      \"suggestion\": \"Extract helper methods: _validate_inputs(), _resolve_cache_params(), _get_or_fetch_response(). This improves testability and readability.\"\n    },\n    {\n      \"type\": \"robustness\",\n      \"severity\": \"low\",\n      \"description\": \"Markdown fence stripping in _load_json is fragile and only handles simple cases\",\n      \"line_number\": 234,\n      \"suggestion\": \"Use a more robust regex pattern to handle various markdown fence formats: `re.sub(r'^```(?:json)?\\\\n|\\\\n```$', '', content.strip(), flags=re.MULTILINE)`\"\n    },\n    {\n      \"type\": \"observability\",\n      \"severity\": \"low\",\n      \"description\": \"No logging of actual LLM response content for debugging failed evaluations\",\n      \"line_number\": 152,\n      \"suggestion\": \"Add debug logging of the raw LLM response before parsing: `self._logger.debug('LLM response: %s', content[:500])` (truncated for log size)\"\n    },\n    {\n      \"type\": \"type_safety\",\n      \"severity\": \"low\",\n      \"description\": \"RubricDimension type allows arbitrary keys via TypedDict, but code assumes only 'description' and 'scale' exist\",\n      \"line_number\": 260,\n      \"suggestion\": \"Use .get() with defaults when accessing rubric dimension fields, or make RubricDimension a total=True TypedDict to enforce required keys at type-check time.\"\n    },\n    {\n      \"type\": \"performance\",\n      \"severity\": \"low\",\n      \"description\": \"Converting messages to list on every retry attempt is unnecessary\",\n      \"line_number\": 138,\n      \"suggestion\": \"Move `payload = list(messages)` outside the retry loop since messages don't change between attempts.\"\n    },\n    {\n      \"type\": \"usability\",\n      \"severity\": \"low\",\n      \"description\": \"No way to configure different retry strategies per error type (rate limits vs network errors)\",\n      \"line_number\": 135,\n      \"suggestion\": \"Consider adding retry configuration that allows different max_retries and backoff strategies for different error categories.\"\n    }\n  ],\n  \n  \"suggestions\": [\n    \"Add comprehensive input validation helper: Create a `_validate_rubric(rubric)` method that checks structure, required keys, and scale format validity.\",\n    \"Implement request/response logging: Add structured logging with request IDs to trace evaluation flows through cache hits, retries, and errors.\",\n    \"Add metrics/telemetry: Track cache hit rates, retry counts, evaluation latencies, and error rates for monitoring production usage.\",\n    \"Create a RubricValidator class: Separate rubric validation logic into its own class to handle complex validation rules and provide better error messages.\",\n    \"Add configuration validation: Create a `validate_config()` method called in __init__ to catch configuration errors early rather than at runtime.\",\n    \"Implement circuit breaker pattern: If LLM API is consistently failing, temporarily stop making requests to avoid wasting resources and improve error recovery.\",\n    \"Add response schema validation: Use a JSON schema validator to ensure LLM responses match expected structure before parsing.\",\n    \"Consider rate limiting: Add built-in rate limiting to prevent overwhelming the LLM API, especially when evaluating in batch.\",\n    \"Improve error context: Include more context in EvaluationError exceptions (model used, dimension being evaluated, etc.) to aid debugging.\",\n    \"Add batch evaluation support: Implement an `evaluate_batch()` method that efficiently evaluates multiple outputs with proper concurrency control.\",\n    \"Document cache backend requirements: Add clear documentation about what methods CacheBackend must implement and any thread-safety requirements.\",\n    \"Add example usage: Include docstring examples showing common usage patterns, especially around caching and custom rubrics.\"\n  ]\n}\n```",
        "test_config": {
          "test_number": 4,
          "config": {
            "temperature": 0.3,
            "model": "openrouter/anthropic/claude-sonnet-4.5",
            "context_size": "full_module",
            "generation_strategy": "chain_of_thought"
          }
        },
        "workflow": {
          "started_at": "2025-10-26T06:22:29.996491Z",
          "completed_at": "2025-10-26T06:23:14.364629Z",
          "duration_seconds": 44.36765304100118
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-powered rubrics with retry logic and caching. However, critical edge cases are inadequately handled: (1) No validation of rubric structure allows runtime errors when 'description' or 'scale' keys are missing, (2) Malformed LLM responses could cause silent failures with the broad exception handling, (3) Score extraction defaults to 100.0 without warning when parsing fails, masking data quality issues. The normalization logic appears correct for 0-1 range conversion, but lacks defensive checks. The default rubric is reasonable but untested against edge cases."
            },
            "error_handling": {
              "score": 58,
              "reasoning": "Error handling is partially implemented but has significant gaps: (1) Broad exception catching in _request_with_retry masks important errors and prevents proper error categorization, (2) No validation of rubric structure before use leads to potential runtime failures, (3) Missing timeout configuration could cause indefinite hangs, (4) Error messages lack sufficient context (model, dimension, attempt number), (5) Silent defaults (100.0 for unparseable scales) hide data quality issues. Custom EvaluationError exists but is underutilized. Logging is mentioned but not comprehensively implemented for debugging failed evaluations."
            },
            "code_quality": {
              "score": 68,
              "reasoning": "Code demonstrates good Python practices with type hints, dataclasses, and PEP 8 compliance. Separation of concerns is present (caching, retry logic, parsing). However: (1) The evaluate() method exceeds 100 lines, violating Single Responsibility Principle by handling validation, caching, API calls, and parsing, (2) RubricDimension TypedDict lacks total=True enforcement, creating type safety gaps, (3) Markdown fence stripping is fragile with regex, (4) Cache key generation lacks documented collision resistance, (5) No abstraction for rubric validation. Method naming is clear but some abstractions could be cleaner."
            },
            "performance": {
              "score": 75,
              "reasoning": "Performance is generally sound with appropriate async/await patterns, caching implementation, and exponential backoff for retries. Caching with hit/miss tracking minimizes unnecessary LLM calls. However: (1) Converting messages to list on every retry attempt is wasteful (should be outside loop), (2) No timeout configuration for LLM calls risks resource exhaustion, (3) No batch evaluation support limits throughput for multiple evaluations, (4) No rate limiting to prevent API overwhelming, (5) Regex patterns appear reasonable but markdown fence stripping could be optimized. Token usage is not explicitly managed or monitored."
            },
            "testability": {
              "score": 64,
              "reasoning": "Code has testable foundations with dependency injection (CacheBackend, logger) and async design. However: (1) The 100+ line evaluate() method is difficult to test as a unit; complex logic should be decomposed into smaller, pure functions, (2) LLM response parsing is tightly coupled to API calls, making it hard to test independently, (3) No clear contracts for CacheBackend interface (thread-safety requirements undocumented), (4) Edge cases like malformed JSON and missing keys are hard to reproduce without mocking complex LLM responses, (5) Broad exception handling makes it difficult to test specific error paths. Interfaces exist but could be more granular."
            },
            "documentation": {
              "score": 70,
              "reasoning": "Documentation is present but incomplete: (1) Docstrings exist for main methods but lack examples of common usage patterns, (2) Type hints are generally accurate but RubricDimension TypedDict lacks clarity on required vs optional keys, (3) Complex prompt building logic lacks inline comments explaining the structure, (4) No documentation of cache backend requirements or thread-safety guarantees, (5) Error handling paths are not documented (which exceptions can be raised, when), (6) Variable names are clear (workflow_output, rubric_dimensions), but some abbreviations (acompletion) could be clearer. Missing documentation on retry strategy, timeout behavior, and cache collision properties."
            }
          },
          "temperature": 0.3,
          "cache_key": "42bb8d33dacdec39905b45b203b0f6a3c2cc00f186f1d776c0cad716ac42d24e",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "timestamp": "2025-10-26T06:23:25.753557Z"
    },
    {
      "test_number": 5,
      "config": {
        "test_number": 5,
        "config_values": {
          "temperature": 0.6,
          "model": "openrouter/anthropic/claude-haiku-4.5",
          "context_size": "full_module",
          "generation_strategy": "standard"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.72,
            "reasoning": "The code accomplishes its core purpose of evaluating LLM outputs against a rubric, but has notable gaps in edge case handling. The `_extract_response_content` method uses duck-typing without thorough validation, potentially silently failing on malformed responses. The `_extract_max_score` regex lacks validation that extracted values make logical sense (max >= min). The `_parse_dimension_scores` doesn't validate that `raw_score` exists before coercion. These issues mean some malformed LLM responses or edge cases won't be handled correctly, reducing reliability despite the overall structure being sound."
          },
          "error_handling": {
            "score": 0.65,
            "reasoning": "Error handling is present but incomplete. The code uses `EvaluationError` appropriately, but several critical validations are missing: no explicit type checking in `_extract_response_content` (silently fails), no validation that regex-extracted score ranges are logically valid, no validation that required scores exist before processing. The `_request_with_retry` method catches all exceptions broadly, which could mask programming errors. Logging is minimal—no debug logs for cache operations, model fallbacks, or response parsing. Error messages lack context in several places. The code would benefit from more defensive programming and comprehensive logging."
          },
          "code_quality": {
            "score": 0.75,
            "reasoning": "The code demonstrates good separation of concerns with distinct methods for parsing, formatting, and extraction. Type hints are present but incomplete in some areas (duck-typing in `_extract_response_content`). The use of `assert` statements for runtime validation is unconventional and problematic. The `DEFAULT_RUBRIC` class variable is mutable, risking accidental modification. Method naming is clear (e.g., `_extract_response_content`, `_parse_dimension_scores`). However, the code lacks comprehensive docstrings—the `evaluate` method has many parameters but minimal documentation of caching behavior and retry logic. The overall structure is sound but could benefit from stricter type safety and better documentation."
          },
          "performance": {
            "score": 0.78,
            "reasoning": "The caching strategy is appropriate and minimizes unnecessary LLM calls. The regex patterns are reasonably efficient, though the `_extract_max_score` pattern could be more specific. The `_load_json` method uses simple string operations for markdown fence removal, which is acceptable for typical inputs but could be inefficient for very large content (repeated string operations). No obvious major bottlenecks exist. The code doesn't include timeout handling for LiteLLM calls, which could lead to indefinite hangs in production. Token usage appears reasonable, but there's no observability for monitoring costs or performance metrics in production."
          },
          "testability": {
            "score": 0.68,
            "reasoning": "The code has reasonable interfaces for mocking (async methods, dependency injection via cache backend and model parameters), but testability is hindered by several factors. The `_request_with_retry` method has a `pragma: no cover` comment, suggesting incomplete test coverage for retry logic. The duck-typing in `_extract_response_content` makes it harder to test edge cases without complex mocking. The `build_cache_key()` dependency is imported but not shown, making it difficult to assess cache key generation testing. The broad exception catching in `_request_with_retry` makes it harder to test specific failure modes. Methods are reasonably pure, but the lack of explicit validation makes edge cases harder to reproduce and test systematically."
          },
          "documentation": {
            "score": 0.62,
            "reasoning": "Documentation is sparse and incomplete. The `evaluate` method lacks a detailed docstring explaining caching behavior, retry logic, and parameter interactions. Most helper methods (`_extract_response_content`, `_parse_dimension_scores`, `_extract_max_score`) have minimal or no docstrings. Type hints are present but incomplete (duck-typing reduces clarity). Variable names are generally clear, but complex logic like regex patterns and markdown stripping lack inline comments explaining intent. The cache key generation strategy is not documented. No examples are provided for how to use the evaluator or handle common scenarios. The code would significantly benefit from comprehensive docstrings, inline comments for complex logic, and usage examples."
          }
        },
        "overall_score": 0.7000000000000001,
        "evaluator_model": "openrouter/anthropic/claude-haiku-4.5",
        "timestamp": "2025-10-26T06:23:55.835796Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating LLM outputs against a rubric, but has notable gaps in edge case handling. The `_extract_response_content` method uses duck-typing without thorough validation, potentially silently failing on malformed responses. The `_extract_max_score` regex lacks validation that extracted values make logical sense (max >= min). The `_parse_dimension_scores` doesn't validate that `raw_score` exists before coercion. These issues mean some malformed LLM responses or edge cases won't be handled correctly, reducing reliability despite the overall structure being sound."
            },
            "error_handling": {
              "score": 65,
              "reasoning": "Error handling is present but incomplete. The code uses `EvaluationError` appropriately, but several critical validations are missing: no explicit type checking in `_extract_response_content` (silently fails), no validation that regex-extracted score ranges are logically valid, no validation that required scores exist before processing. The `_request_with_retry` method catches all exceptions broadly, which could mask programming errors. Logging is minimal—no debug logs for cache operations, model fallbacks, or response parsing. Error messages lack context in several places. The code would benefit from more defensive programming and comprehensive logging."
            },
            "code_quality": {
              "score": 75,
              "reasoning": "The code demonstrates good separation of concerns with distinct methods for parsing, formatting, and extraction. Type hints are present but incomplete in some areas (duck-typing in `_extract_response_content`). The use of `assert` statements for runtime validation is unconventional and problematic. The `DEFAULT_RUBRIC` class variable is mutable, risking accidental modification. Method naming is clear (e.g., `_extract_response_content`, `_parse_dimension_scores`). However, the code lacks comprehensive docstrings—the `evaluate` method has many parameters but minimal documentation of caching behavior and retry logic. The overall structure is sound but could benefit from stricter type safety and better documentation."
            },
            "performance": {
              "score": 78,
              "reasoning": "The caching strategy is appropriate and minimizes unnecessary LLM calls. The regex patterns are reasonably efficient, though the `_extract_max_score` pattern could be more specific. The `_load_json` method uses simple string operations for markdown fence removal, which is acceptable for typical inputs but could be inefficient for very large content (repeated string operations). No obvious major bottlenecks exist. The code doesn't include timeout handling for LiteLLM calls, which could lead to indefinite hangs in production. Token usage appears reasonable, but there's no observability for monitoring costs or performance metrics in production."
            },
            "testability": {
              "score": 68,
              "reasoning": "The code has reasonable interfaces for mocking (async methods, dependency injection via cache backend and model parameters), but testability is hindered by several factors. The `_request_with_retry` method has a `pragma: no cover` comment, suggesting incomplete test coverage for retry logic. The duck-typing in `_extract_response_content` makes it harder to test edge cases without complex mocking. The `build_cache_key()` dependency is imported but not shown, making it difficult to assess cache key generation testing. The broad exception catching in `_request_with_retry` makes it harder to test specific failure modes. Methods are reasonably pure, but the lack of explicit validation makes edge cases harder to reproduce and test systematically."
            },
            "documentation": {
              "score": 62,
              "reasoning": "Documentation is sparse and incomplete. The `evaluate` method lacks a detailed docstring explaining caching behavior, retry logic, and parameter interactions. Most helper methods (`_extract_response_content`, `_parse_dimension_scores`, `_extract_max_score`) have minimal or no docstrings. Type hints are present but incomplete (duck-typing reduces clarity). Variable names are generally clear, but complex logic like regex patterns and markdown stripping lack inline comments explaining intent. The cache key generation strategy is not documented. No examples are provided for how to use the evaluator or handle common scenarios. The code would significantly benefit from comprehensive docstrings, inline comments for complex logic, and usage examples."
            }
          },
          "temperature": 0.3,
          "cache_key": "87c3df92f3a4146da882aa26f5d6a8d94096387e064ff42989afd44f3321d324",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "cost": 0.0,
      "latency": 18850.021708989516,
      "utility": 0.6978614295974785,
      "workflow_output": "Well-structured LLM evaluation code with comprehensive error handling and good separation of concerns. Strong overall quality with minor issues around type safety, documentation, and a potential security consideration with regex parsing.\nIssues:\n- (medium) The `_extract_response_content` method uses duck-typing to handle both Mapping and object types, but doesn't validate the structure thoroughly. If `choices[0]` is neither Mapping nor object with expected attributes, it silently fails. [line 201] Suggested fix: Add explicit type checking or use a TypedDict/dataclass for response validation. Consider: `if not isinstance(choices[0], (Mapping, dict))` with a clear error message.\n- (medium) The `_extract_max_score` regex pattern `(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)` could match unintended strings. No validation that the extracted values make logical sense (e.g., max >= min). [line 251] Suggested fix: Add validation: `if min_val >= max_val: raise ValueError(...)`. Also consider anchoring regex with `^` and `$` or being more specific about expected format.\n- (low) The regex in `_extract_max_score` is simple enough to be safe, but the `_load_json` method strips markdown fences with simple string operations. If content is extremely large, repeated string operations could be inefficient. [line 220] Suggested fix: Use compiled regex or more efficient string manipulation for markdown fence removal. Consider: `re.sub(r'^```.*?\\n|\\n?```$', '', stripped)`\n- (medium) In `_parse_dimension_scores`, when `entry` is not a Mapping, `reasoning` is set to `None`, but the code doesn't validate that `raw_score` exists or is valid. This could lead to confusing error messages. [line 244] Suggested fix: Add explicit validation: `if raw_score is None: raise EvaluationError(f'Missing score for {name}')` before attempting coercion.\n- (low) The `evaluate` method has many parameters but lacks a detailed docstring explaining the caching behavior, retry logic, and parameter interactions. [line 62] Suggested fix: Add comprehensive docstring with parameter descriptions, return value details, and examples of cache behavior.\n- (low) The `_request_with_retry` method catches all exceptions broadly (`except Exception`). This could mask programming errors or unexpected issues that shouldn't be retried. [line 148] Suggested fix: Catch specific exceptions (e.g., `litellm.APIError`, `asyncio.TimeoutError`) and re-raise others immediately. Add a pragma comment explaining why broad catching is needed.\n- (low) Multiple `assert` statements used for type-checking (`assert resolved_cache_key is not None`). While this works, it's unconventional and could be disabled in optimized Python. [line 128] Suggested fix: Use explicit `if resolved_cache_key is None: raise RuntimeError(...)` or rely on type narrowing from control flow. Asserts should be for debugging, not runtime validation.\n- (low) The `_format_rubric` method iterates through dimensions and builds strings. For large rubrics, this is fine, but the method could be more efficient. [line 185] Suggested fix: Minor: Use `'\n'.join()` more idiomatically or consider f-strings for clarity. Current implementation is acceptable but could be: `[f'- {name}: {md[\"description\"]} (Scale: {md[\"scale\"]})' for name, md in dimensions]`\n- (low) The `_request_with_retry` method has a `pragma: no cover` comment, but retry logic should be testable with mocks. This suggests incomplete test coverage. [line 148] Suggested fix: Write tests that mock `litellm.acompletion` to raise exceptions on first N attempts, then succeed. Remove pragma comment once covered.\n- (low) Return type of `_extract_response_content` is `str`, but the method could potentially return an empty string after markdown stripping, which might be undesirable. [line 210] Suggested fix: Add validation: `if not content.strip(): raise EvaluationError('Response content is empty after parsing')` before returning.\nSuggestions:\n- {'priority': 'high', 'description': 'Add comprehensive logging of cache operations and LLM interactions for debugging. Currently missing details about cache misses and model selection.', 'suggestion': 'Add debug logs when: cache backend is None but caching requested, when falling back to default model/temperature, and when response parsing begins.'}\n- {'priority': 'high', 'description': 'Create a response schema validation layer to ensure LLM responses match expected JSON structure before parsing.', 'suggestion': 'Use `pydantic.BaseModel` or `jsonschema` to validate the parsed response has required fields for all rubric dimensions with proper types.'}\n- {'priority': 'medium', 'description': 'The `DEFAULT_RUBRIC` is a class variable but could be mutated. Consider making it immutable.', 'suggestion': 'Use `types.MappingProxyType(DEFAULT_RUBRIC)` or convert to a frozen dataclass/TypedDict for immutability.'}\n- {'priority': 'medium', 'description': 'Add timeout handling for the LiteLLM call to prevent indefinite hangs.', 'suggestion': 'Wrap `litellm.acompletion()` with `asyncio.timeout()` (Python 3.11+) or `asyncio.wait_for()` with a configurable timeout parameter.'}\n- {'priority': 'medium', 'description': 'The cache key generation relies on `build_cache_key()` which is imported but not shown. Ensure it handles edge cases like very long prompts.', 'suggestion': 'Document the cache key generation strategy and consider adding a hash-based fallback for extremely long keys.'}\n- {'priority': 'low', 'description': 'Consider making retry behavior configurable per-call, not just per-instance.', 'suggestion': 'Add optional `max_retries` and `retry_base_delay` parameters to the `evaluate()` method to override instance defaults.'}\n- {'priority': 'low', 'description': 'The prompt building could be more maintainable with a template system.', 'suggestion': 'Consider using `string.Template` or `jinja2` for prompt construction to make it easier to modify and test different prompt variations.'}\n- {'priority': 'low', 'description': 'Add metrics/observability hooks for monitoring evaluation performance and LLM costs.', 'suggestion': 'Emit structured logs with token counts, latency, retry attempts, and cache hit rates for production monitoring.'}",
      "metadata": {
        "strategy": "standard",
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "temperature": 0.6,
        "context_size": "full_module",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Consider surrounding module context for the review.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"Well-structured LLM evaluation code with comprehensive error handling and good separation of concerns. Strong overall quality with minor issues around type safety, documentation, and a potential security consideration with regex parsing.\",\n  \"issues\": [\n    {\n      \"type\": \"Type Safety\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_extract_response_content` method uses duck-typing to handle both Mapping and object types, but doesn't validate the structure thoroughly. If `choices[0]` is neither Mapping nor object with expected attributes, it silently fails.\",\n      \"line_number\": 201,\n      \"suggestion\": \"Add explicit type checking or use a TypedDict/dataclass for response validation. Consider: `if not isinstance(choices[0], (Mapping, dict))` with a clear error message.\"\n    },\n    {\n      \"type\": \"Input Validation\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_extract_max_score` regex pattern `(\\\\d+(?:\\\\.\\\\d+)?)\\\\s*-\\\\s*(\\\\d+(?:\\\\.\\\\d+)?)` could match unintended strings. No validation that the extracted values make logical sense (e.g., max >= min).\",\n      \"line_number\": 251,\n      \"suggestion\": \"Add validation: `if min_val >= max_val: raise ValueError(...)`. Also consider anchoring regex with `^` and `$` or being more specific about expected format.\"\n    },\n    {\n      \"type\": \"Security - Regex DoS\",\n      \"severity\": \"low\",\n      \"description\": \"The regex in `_extract_max_score` is simple enough to be safe, but the `_load_json` method strips markdown fences with simple string operations. If content is extremely large, repeated string operations could be inefficient.\",\n      \"line_number\": 220,\n      \"suggestion\": \"Use compiled regex or more efficient string manipulation for markdown fence removal. Consider: `re.sub(r'^```.*?\\\\n|\\\\n?```$', '', stripped)`\"\n    },\n    {\n      \"type\": \"Logic Error\",\n      \"severity\": \"medium\",\n      \"description\": \"In `_parse_dimension_scores`, when `entry` is not a Mapping, `reasoning` is set to `None`, but the code doesn't validate that `raw_score` exists or is valid. This could lead to confusing error messages.\",\n      \"line_number\": 244,\n      \"suggestion\": \"Add explicit validation: `if raw_score is None: raise EvaluationError(f'Missing score for {name}')` before attempting coercion.\"\n    },\n    {\n      \"type\": \"Documentation\",\n      \"severity\": \"low\",\n      \"description\": \"The `evaluate` method has many parameters but lacks a detailed docstring explaining the caching behavior, retry logic, and parameter interactions.\",\n      \"line_number\": 62,\n      \"suggestion\": \"Add comprehensive docstring with parameter descriptions, return value details, and examples of cache behavior.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"low\",\n      \"description\": \"The `_request_with_retry` method catches all exceptions broadly (`except Exception`). This could mask programming errors or unexpected issues that shouldn't be retried.\",\n      \"line_number\": 148,\n      \"suggestion\": \"Catch specific exceptions (e.g., `litellm.APIError`, `asyncio.TimeoutError`) and re-raise others immediately. Add a pragma comment explaining why broad catching is needed.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Multiple `assert` statements used for type-checking (`assert resolved_cache_key is not None`). While this works, it's unconventional and could be disabled in optimized Python.\",\n      \"line_number\": 128,\n      \"suggestion\": \"Use explicit `if resolved_cache_key is None: raise RuntimeError(...)` or rely on type narrowing from control flow. Asserts should be for debugging, not runtime validation.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"The `_format_rubric` method iterates through dimensions and builds strings. For large rubrics, this is fine, but the method could be more efficient.\",\n      \"line_number\": 185,\n      \"suggestion\": \"Minor: Use `'\\n'.join()` more idiomatically or consider f-strings for clarity. Current implementation is acceptable but could be: `[f'- {name}: {md[\\\"description\\\"]} (Scale: {md[\\\"scale\\\"]})' for name, md in dimensions]`\"\n    },\n    {\n      \"type\": \"Testing Concern\",\n      \"severity\": \"low\",\n      \"description\": \"The `_request_with_retry` method has a `pragma: no cover` comment, but retry logic should be testable with mocks. This suggests incomplete test coverage.\",\n      \"line_number\": 148,\n      \"suggestion\": \"Write tests that mock `litellm.acompletion` to raise exceptions on first N attempts, then succeed. Remove pragma comment once covered.\"\n    },\n    {\n      \"type\": \"Type Hint\",\n      \"severity\": \"low\",\n      \"description\": \"Return type of `_extract_response_content` is `str`, but the method could potentially return an empty string after markdown stripping, which might be undesirable.\",\n      \"line_number\": 210,\n      \"suggestion\": \"Add validation: `if not content.strip(): raise EvaluationError('Response content is empty after parsing')` before returning.\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"priority\": \"high\",\n      \"description\": \"Add comprehensive logging of cache operations and LLM interactions for debugging. Currently missing details about cache misses and model selection.\",\n      \"suggestion\": \"Add debug logs when: cache backend is None but caching requested, when falling back to default model/temperature, and when response parsing begins.\"\n    },\n    {\n      \"priority\": \"high\",\n      \"description\": \"Create a response schema validation layer to ensure LLM responses match expected JSON structure before parsing.\",\n      \"suggestion\": \"Use `pydantic.BaseModel` or `jsonschema` to validate the parsed response has required fields for all rubric dimensions with proper types.\"\n    },\n    {\n      \"priority\": \"medium\",\n      \"description\": \"The `DEFAULT_RUBRIC` is a class variable but could be mutated. Consider making it immutable.\",\n      \"suggestion\": \"Use `types.MappingProxyType(DEFAULT_RUBRIC)` or convert to a frozen dataclass/TypedDict for immutability.\"\n    },\n    {\n      \"priority\": \"medium\",\n      \"description\": \"Add timeout handling for the LiteLLM call to prevent indefinite hangs.\",\n      \"suggestion\": \"Wrap `litellm.acompletion()` with `asyncio.timeout()` (Python 3.11+) or `asyncio.wait_for()` with a configurable timeout parameter.\"\n    },\n    {\n      \"priority\": \"medium\",\n      \"description\": \"The cache key generation relies on `build_cache_key()` which is imported but not shown. Ensure it handles edge cases like very long prompts.\",\n      \"suggestion\": \"Document the cache key generation strategy and consider adding a hash-based fallback for extremely long keys.\"\n    },\n    {\n      \"priority\": \"low\",\n      \"description\": \"Consider making retry behavior configurable per-call, not just per-instance.\",\n      \"suggestion\": \"Add optional `max_retries` and `retry_base_delay` parameters to the `evaluate()` method to override instance defaults.\"\n    },\n    {\n      \"priority\": \"low\",\n      \"description\": \"The prompt building could be more maintainable with a template system.\",\n      \"suggestion\": \"Consider using `string.Template` or `jinja2` for prompt construction to make it easier to modify and test different prompt variations.\"\n    },\n    {\n      \"priority\": \"low\",\n      \"description\": \"Add metrics/observability hooks for monitoring evaluation performance and LLM costs.\",\n      \"suggestion\": \"Emit structured logs with token counts, latency, retry attempts, and cache hit rates for production monitoring.\"\n    }\n  ]\n}\n```",
        "test_config": {
          "test_number": 5,
          "config": {
            "temperature": 0.6,
            "model": "openrouter/anthropic/claude-haiku-4.5",
            "context_size": "full_module",
            "generation_strategy": "standard"
          }
        },
        "workflow": {
          "started_at": "2025-10-26T06:23:25.758741Z",
          "completed_at": "2025-10-26T06:23:44.608976Z",
          "duration_seconds": 18.850021708989516
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating LLM outputs against a rubric, but has notable gaps in edge case handling. The `_extract_response_content` method uses duck-typing without thorough validation, potentially silently failing on malformed responses. The `_extract_max_score` regex lacks validation that extracted values make logical sense (max >= min). The `_parse_dimension_scores` doesn't validate that `raw_score` exists before coercion. These issues mean some malformed LLM responses or edge cases won't be handled correctly, reducing reliability despite the overall structure being sound."
            },
            "error_handling": {
              "score": 65,
              "reasoning": "Error handling is present but incomplete. The code uses `EvaluationError` appropriately, but several critical validations are missing: no explicit type checking in `_extract_response_content` (silently fails), no validation that regex-extracted score ranges are logically valid, no validation that required scores exist before processing. The `_request_with_retry` method catches all exceptions broadly, which could mask programming errors. Logging is minimal—no debug logs for cache operations, model fallbacks, or response parsing. Error messages lack context in several places. The code would benefit from more defensive programming and comprehensive logging."
            },
            "code_quality": {
              "score": 75,
              "reasoning": "The code demonstrates good separation of concerns with distinct methods for parsing, formatting, and extraction. Type hints are present but incomplete in some areas (duck-typing in `_extract_response_content`). The use of `assert` statements for runtime validation is unconventional and problematic. The `DEFAULT_RUBRIC` class variable is mutable, risking accidental modification. Method naming is clear (e.g., `_extract_response_content`, `_parse_dimension_scores`). However, the code lacks comprehensive docstrings—the `evaluate` method has many parameters but minimal documentation of caching behavior and retry logic. The overall structure is sound but could benefit from stricter type safety and better documentation."
            },
            "performance": {
              "score": 78,
              "reasoning": "The caching strategy is appropriate and minimizes unnecessary LLM calls. The regex patterns are reasonably efficient, though the `_extract_max_score` pattern could be more specific. The `_load_json` method uses simple string operations for markdown fence removal, which is acceptable for typical inputs but could be inefficient for very large content (repeated string operations). No obvious major bottlenecks exist. The code doesn't include timeout handling for LiteLLM calls, which could lead to indefinite hangs in production. Token usage appears reasonable, but there's no observability for monitoring costs or performance metrics in production."
            },
            "testability": {
              "score": 68,
              "reasoning": "The code has reasonable interfaces for mocking (async methods, dependency injection via cache backend and model parameters), but testability is hindered by several factors. The `_request_with_retry` method has a `pragma: no cover` comment, suggesting incomplete test coverage for retry logic. The duck-typing in `_extract_response_content` makes it harder to test edge cases without complex mocking. The `build_cache_key()` dependency is imported but not shown, making it difficult to assess cache key generation testing. The broad exception catching in `_request_with_retry` makes it harder to test specific failure modes. Methods are reasonably pure, but the lack of explicit validation makes edge cases harder to reproduce and test systematically."
            },
            "documentation": {
              "score": 62,
              "reasoning": "Documentation is sparse and incomplete. The `evaluate` method lacks a detailed docstring explaining caching behavior, retry logic, and parameter interactions. Most helper methods (`_extract_response_content`, `_parse_dimension_scores`, `_extract_max_score`) have minimal or no docstrings. Type hints are present but incomplete (duck-typing reduces clarity). Variable names are generally clear, but complex logic like regex patterns and markdown stripping lack inline comments explaining intent. The cache key generation strategy is not documented. No examples are provided for how to use the evaluator or handle common scenarios. The code would significantly benefit from comprehensive docstrings, inline comments for complex logic, and usage examples."
            }
          },
          "temperature": 0.3,
          "cache_key": "87c3df92f3a4146da882aa26f5d6a8d94096387e064ff42989afd44f3321d324",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "timestamp": "2025-10-26T06:23:55.836426Z"
    },
    {
      "test_number": 6,
      "config": {
        "test_number": 6,
        "config_values": {
          "temperature": 0.6,
          "model": "openrouter/anthropic/claude-haiku-4.5",
          "context_size": "full_module",
          "generation_strategy": "chain_of_thought"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.72,
            "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLMs, but has significant gaps in edge case handling. The score extraction logic works for normal cases but fails silently on malformed responses (medium severity issue at line 171). The normalization logic has a critical bug where max_score defaults to 100 without validation, causing incorrect normalization for non-standard scales (line 237). Missing dimension validation (line 214) could mask incomplete evaluations. The default rubric is well-designed, but the overall robustness is compromised by these issues."
          },
          "error_handling": {
            "score": 0.58,
            "reasoning": "Error handling is inconsistent and overly broad. The critical issue is the bare `except Exception` at line 128 that catches KeyboardInterrupt and SystemExit, preventing proper shutdown and masking critical errors. The assertion at line 157 raises AssertionError instead of user-friendly EvaluationError. Input validation is minimal (only checks for empty strings, not length limits). Logging exists but lacks debug-level detail for troubleshooting cache operations and score normalization. The code lacks explicit validation of response structure before attempting to parse it, leading to potential silent failures."
          },
          "code_quality": {
            "score": 0.65,
            "reasoning": "The code has reasonable structure with clear method separation, but several design issues reduce quality. Type safety is weak due to loose isinstance() checks and getattr() fallbacks (line 171) instead of using TypedDict or Pydantic models. The tight coupling to litellm.acompletion (line 127) violates dependency injection principles and makes testing difficult. The _load_json method uses fragile string manipulation instead of robust regex (line 192). The DEFAULT_RUBRIC constant is large and should be extracted to a config module (line 25). Method naming is generally clear, but some methods like _coerce_numeric lack documentation of edge case handling."
          },
          "performance": {
            "score": 0.78,
            "reasoning": "The code demonstrates good performance practices with caching and retry logic. The async/await pattern is properly implemented. However, there are concerns: the messages parameter is converted to list immediately (line 118), which is correct but could be better documented. The regex patterns in _load_json are simple and efficient, though the markdown fence handling could be more robust. The caching strategy is sound with configurable use_cache and record_cache flags. No obvious bottlenecks exist, but the lack of metrics/observability (cache hit rates, retry counts) makes it difficult to optimize further. Token usage appears reasonable with configurable temperature and model selection."
          },
          "testability": {
            "score": 0.52,
            "reasoning": "The code has poor testability due to tight coupling to external dependencies. The direct dependency on litellm.acompletion (line 127) with no injected dependency or protocol makes unit testing difficult without complex monkeypatching. Methods like _extract_response_content and _parse_dimension_scores are not easily testable in isolation due to their reliance on specific response structures. The lack of a Protocol or abstract base class for LLM clients prevents easy mocking. While some methods like _normalize_score and _coerce_numeric could be pure functions, they're tightly integrated. The broad exception handling makes it hard to test specific error paths. No clear contracts are documented for method inputs/outputs."
          },
          "documentation": {
            "score": 0.68,
            "reasoning": "The code has basic documentation with docstrings for public methods, but significant gaps exist. Type hints are present but incomplete in some areas (loose typing with isinstance checks). Complex logic like score normalization lacks inline comments explaining the max_score extraction and defaulting behavior. The caching behavior is not well-documented (when cache is used vs. recorded, cache key generation). Edge cases like prompt injection risks (line 149) are not mentioned in docstrings. The _load_json markdown fence handling lacks explanation of its assumptions. Variable names are generally clear (e.g., workflow_output, dimension_scores), but some methods could benefit from more detailed docstrings explaining their contracts and potential failure modes."
          }
        },
        "overall_score": 0.655,
        "evaluator_model": "openrouter/anthropic/claude-haiku-4.5",
        "timestamp": "2025-10-26T06:24:28.169380Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLMs, but has significant gaps in edge case handling. The score extraction logic works for normal cases but fails silently on malformed responses (medium severity issue at line 171). The normalization logic has a critical bug where max_score defaults to 100 without validation, causing incorrect normalization for non-standard scales (line 237). Missing dimension validation (line 214) could mask incomplete evaluations. The default rubric is well-designed, but the overall robustness is compromised by these issues."
            },
            "error_handling": {
              "score": 58,
              "reasoning": "Error handling is inconsistent and overly broad. The critical issue is the bare `except Exception` at line 128 that catches KeyboardInterrupt and SystemExit, preventing proper shutdown and masking critical errors. The assertion at line 157 raises AssertionError instead of user-friendly EvaluationError. Input validation is minimal (only checks for empty strings, not length limits). Logging exists but lacks debug-level detail for troubleshooting cache operations and score normalization. The code lacks explicit validation of response structure before attempting to parse it, leading to potential silent failures."
            },
            "code_quality": {
              "score": 65,
              "reasoning": "The code has reasonable structure with clear method separation, but several design issues reduce quality. Type safety is weak due to loose isinstance() checks and getattr() fallbacks (line 171) instead of using TypedDict or Pydantic models. The tight coupling to litellm.acompletion (line 127) violates dependency injection principles and makes testing difficult. The _load_json method uses fragile string manipulation instead of robust regex (line 192). The DEFAULT_RUBRIC constant is large and should be extracted to a config module (line 25). Method naming is generally clear, but some methods like _coerce_numeric lack documentation of edge case handling."
            },
            "performance": {
              "score": 78,
              "reasoning": "The code demonstrates good performance practices with caching and retry logic. The async/await pattern is properly implemented. However, there are concerns: the messages parameter is converted to list immediately (line 118), which is correct but could be better documented. The regex patterns in _load_json are simple and efficient, though the markdown fence handling could be more robust. The caching strategy is sound with configurable use_cache and record_cache flags. No obvious bottlenecks exist, but the lack of metrics/observability (cache hit rates, retry counts) makes it difficult to optimize further. Token usage appears reasonable with configurable temperature and model selection."
            },
            "testability": {
              "score": 52,
              "reasoning": "The code has poor testability due to tight coupling to external dependencies. The direct dependency on litellm.acompletion (line 127) with no injected dependency or protocol makes unit testing difficult without complex monkeypatching. Methods like _extract_response_content and _parse_dimension_scores are not easily testable in isolation due to their reliance on specific response structures. The lack of a Protocol or abstract base class for LLM clients prevents easy mocking. While some methods like _normalize_score and _coerce_numeric could be pure functions, they're tightly integrated. The broad exception handling makes it hard to test specific error paths. No clear contracts are documented for method inputs/outputs."
            },
            "documentation": {
              "score": 68,
              "reasoning": "The code has basic documentation with docstrings for public methods, but significant gaps exist. Type hints are present but incomplete in some areas (loose typing with isinstance checks). Complex logic like score normalization lacks inline comments explaining the max_score extraction and defaulting behavior. The caching behavior is not well-documented (when cache is used vs. recorded, cache key generation). Edge cases like prompt injection risks (line 149) are not mentioned in docstrings. The _load_json markdown fence handling lacks explanation of its assumptions. Variable names are generally clear (e.g., workflow_output, dimension_scores), but some methods could benefit from more detailed docstrings explaining their contracts and potential failure modes."
            }
          },
          "temperature": 0.3,
          "cache_key": "ae8f08a03d5a365cd8c4228c0a83ea9f40481be0dcf6ccf5216852f58db985d9",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "cost": 0.0,
      "latency": 21377.665125008207,
      "utility": 0.6510921695774032,
      "workflow_output": "Well-structured LLM evaluation code with strong error handling and validation. The implementation follows async best practices and includes thoughtful features like caching and retry logic. However, there are several issues around type safety, testability, and potential edge cases that should be addressed.\nIssues:\n- (medium) The `_extract_response_content` method uses loose type checking with `isinstance()` and `getattr()` fallbacks, making it fragile to unexpected response formats. The method could fail silently or with unclear errors if LiteLLM changes its response structure. [line 171] Suggested fix: Consider creating a response dataclass or TypedDict to enforce structure. Add explicit type hints and validate response shape early: `if not hasattr(response, 'choices') or not response.choices: raise EvaluationError(...)`\n- (medium) The broad `except Exception` in `_request_with_retry` catches all exceptions including KeyboardInterrupt and SystemExit. This can mask critical errors and prevent proper shutdown. [line 128] Suggested fix: Catch specific exceptions: `except (litellm.APIError, litellm.Timeout, asyncio.TimeoutError) as exc:` or at minimum exclude `BaseException` subclasses. Document which exceptions trigger retries.\n- (high) In `_parse_dimension_scores`, if the LLM returns a score outside the expected rubric dimensions, it silently ignores extra dimensions. This could mask incomplete evaluations if the LLM misunderstands the format. [line 214] Suggested fix: Add validation: `unexpected_dims = set(payload.keys()) - set(rubric.keys()); if unexpected_dims: self._logger.warning(f'Unexpected dimensions: {unexpected_dims}')`\n- (medium) The `workflow_output` parameter only checks for empty/whitespace strings but doesn't validate length. Very large outputs could cause token limit issues or unexpected LLM behavior. [line 67] Suggested fix: Add a configurable max_length parameter and validate: `if len(workflow_output) > max_length: raise EvaluationError(f'Output exceeds {max_length} characters')`\n- (medium) Direct dependency on `litellm.acompletion` makes unit testing difficult. The class is tightly coupled to the external library with no seam for mocking. [line 127] Suggested fix: Extract LLM calling logic into an injected dependency or create a protocol: `class LLMClient(Protocol): async def complete(...) -> Any`. This enables easy mocking in tests.\n- (medium) In `_normalize_score`, the max_score is extracted from the rubric scale string, but if parsing fails, it defaults to 100. If the actual scale is '0-20', scores of 15 would be normalized incorrectly as 0.15 instead of 0.75. [line 237] Suggested fix: Log a warning when defaulting to 100: `self._logger.warning(f'Could not parse scale {scale!r}, defaulting to 100')`\n- (low) The `messages` parameter is converted to a list in `_request_with_retry` but the original `Iterable` could be a generator that gets exhausted if retries occur. [line 118] Suggested fix: The current code handles this correctly by converting to list immediately. Document this design decision in a comment to prevent future regressions.\n- (low) User-provided `extra_instructions` are directly injected into the prompt without sanitization. While LLMs are generally robust, this could enable prompt injection attacks. [line 149] Suggested fix: Add a note in docstring warning about prompt injection risks, or implement instruction validation: `if any(dangerous_pattern in extra_instructions for dangerous_pattern in [...]):`\n- (low) The `_load_json` method handles markdown code fences but the logic is fragile - it assumes exactly one newline after the opening fence. Inconsistent formatting could break parsing. [line 192] Suggested fix: Use a more robust regex: `stripped = re.sub(r'^```(?:json)?\\s*\\n?|\\n?```$', '', stripped, flags=re.MULTILINE)`\n- (low) The `_coerce_numeric` method accepts `str` and converts via `float()`, but doesn't handle edge cases like 'inf', 'nan', or scientific notation consistently. [line 259] Suggested fix: The `math.isfinite()` check already handles inf/nan correctly. Consider documenting this or explicitly rejecting string representations: `if stripped.lower() in ('inf', 'nan', '-inf'): raise ValueError(...)`\n- (low) The `DEFAULT_RUBRIC` is a large constant that could grow. Consider moving to a separate config file or constant module. [line 25] Suggested fix: Extract to a `rubrics.py` module or load from JSON config file for easier maintenance and updates.\n- (medium) The assertion `assert content is not None` at line 157 will raise `AssertionError` in production if somehow content is still None, which is not user-friendly. [line 157] Suggested fix: Replace with explicit check: `if content is None: raise EvaluationError('Failed to extract response content after cache and LLM call')`\nSuggestions:\n- {'priority': 'high', 'title': 'Add Response Schema Validation', 'description': 'Create a Pydantic model or TypedDict for the expected LLM response structure to catch malformed responses early and provide clear error messages.', 'example': 'from pydantic import BaseModel\\n\\nclass DimensionScoreResponse(BaseModel):\\n    score: float\\n    reasoning: Optional[str]\\n\\nclass EvaluationResponse(BaseModel):\\n    dimensions: Dict[str, DimensionScoreResponse]'}\n- {'priority': 'high', 'title': 'Improve Exception Specificity', 'description': 'Replace broad `except Exception` with specific LiteLLM exceptions to avoid catching unintended errors and enable better error recovery strategies.', 'example': 'from litellm import APIError, Timeout\\nexcept (APIError, Timeout, asyncio.TimeoutError) as exc:'}\n- {'priority': 'medium', 'title': 'Add Comprehensive Logging', 'description': 'Add debug logging for cache operations, score normalization, and JSON parsing to aid troubleshooting in production.', 'example': \"self._logger.debug(f'Normalized score {raw_score} (max={max_score}) to {normalized_score}')\"}\n- {'priority': 'medium', 'title': 'Create Integration Tests with Mock LLM', 'description': 'Add fixture-based tests that mock LiteLLM responses to test various edge cases (malformed JSON, out-of-range scores, missing dimensions) without calling the real API.', 'example': \"@pytest.fixture\\ndef mock_llm(monkeypatch):\\n    async def mock_completion(**kwargs):\\n        return {...}\\n    monkeypatch.setattr('litellm.acompletion', mock_completion)\"}\n- {'priority': 'medium', 'title': 'Add Configuration Validation', 'description': 'Create a dataclass or Pydantic model for evaluator configuration to centralize validation and make parameters more discoverable.', 'example': 'from dataclasses import dataclass\\n\\n@dataclass\\nclass EvaluatorConfig:\\n    model: str = DEFAULT_MODEL\\n    temperature: float = DEFAULT_TEMPERATURE\\n    max_retries: int = 3\\n    # ... validate in __post_init__'}\n- {'priority': 'low', 'title': 'Document Caching Behavior', 'description': 'Add detailed docstring explaining cache key generation, when cache is used vs. recorded, and potential stale data issues.', 'example': 'Cache behavior:\\n- use_cache=True: Return cached result if available\\n- record_cache=True: Store LLM response after fetching\\n- Both can be True: Use if available, record if not\\n- Cache key is deterministic based on prompt+model+temperature'}\n- {'priority': 'low', 'title': 'Add Metrics/Observability', 'description': 'Track and expose metrics like cache hit rate, retry counts, and average response times for monitoring evaluator performance.', 'example': \"self._cache_hits += 1\\nself._total_requests += 1\\nself._logger.info(f'Cache hit rate: {self._cache_hits/self._total_requests:.1%}')\"}",
      "metadata": {
        "strategy": "chain_of_thought",
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "temperature": 0.6,
        "context_size": "full_module",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Consider surrounding module context for the review.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"Well-structured LLM evaluation code with strong error handling and validation. The implementation follows async best practices and includes thoughtful features like caching and retry logic. However, there are several issues around type safety, testability, and potential edge cases that should be addressed.\",\n  \"issues\": [\n    {\n      \"type\": \"Type Safety\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_extract_response_content` method uses loose type checking with `isinstance()` and `getattr()` fallbacks, making it fragile to unexpected response formats. The method could fail silently or with unclear errors if LiteLLM changes its response structure.\",\n      \"line_number\": 171,\n      \"suggestion\": \"Consider creating a response dataclass or TypedDict to enforce structure. Add explicit type hints and validate response shape early: `if not hasattr(response, 'choices') or not response.choices: raise EvaluationError(...)`\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"The broad `except Exception` in `_request_with_retry` catches all exceptions including KeyboardInterrupt and SystemExit. This can mask critical errors and prevent proper shutdown.\",\n      \"line_number\": 128,\n      \"suggestion\": \"Catch specific exceptions: `except (litellm.APIError, litellm.Timeout, asyncio.TimeoutError) as exc:` or at minimum exclude `BaseException` subclasses. Document which exceptions trigger retries.\"\n    },\n    {\n      \"type\": \"Logic Error\",\n      \"severity\": \"high\",\n      \"description\": \"In `_parse_dimension_scores`, if the LLM returns a score outside the expected rubric dimensions, it silently ignores extra dimensions. This could mask incomplete evaluations if the LLM misunderstands the format.\",\n      \"line_number\": 214,\n      \"suggestion\": \"Add validation: `unexpected_dims = set(payload.keys()) - set(rubric.keys()); if unexpected_dims: self._logger.warning(f'Unexpected dimensions: {unexpected_dims}')`\"\n    },\n    {\n      \"type\": \"Input Validation\",\n      \"severity\": \"medium\",\n      \"description\": \"The `workflow_output` parameter only checks for empty/whitespace strings but doesn't validate length. Very large outputs could cause token limit issues or unexpected LLM behavior.\",\n      \"line_number\": 67,\n      \"suggestion\": \"Add a configurable max_length parameter and validate: `if len(workflow_output) > max_length: raise EvaluationError(f'Output exceeds {max_length} characters')`\"\n    },\n    {\n      \"type\": \"Testability\",\n      \"severity\": \"medium\",\n      \"description\": \"Direct dependency on `litellm.acompletion` makes unit testing difficult. The class is tightly coupled to the external library with no seam for mocking.\",\n      \"line_number\": 127,\n      \"suggestion\": \"Extract LLM calling logic into an injected dependency or create a protocol: `class LLMClient(Protocol): async def complete(...) -> Any`. This enables easy mocking in tests.\"\n    },\n    {\n      \"type\": \"Logic Error\",\n      \"severity\": \"medium\",\n      \"description\": \"In `_normalize_score`, the max_score is extracted from the rubric scale string, but if parsing fails, it defaults to 100. If the actual scale is '0-20', scores of 15 would be normalized incorrectly as 0.15 instead of 0.75.\",\n      \"line_number\": 237,\n      \"suggestion\": \"Log a warning when defaulting to 100: `self._logger.warning(f'Could not parse scale {scale!r}, defaulting to 100')`\"\n    },\n    {\n      \"type\": \"Resource Management\",\n      \"severity\": \"low\",\n      \"description\": \"The `messages` parameter is converted to a list in `_request_with_retry` but the original `Iterable` could be a generator that gets exhausted if retries occur.\",\n      \"line_number\": 118,\n      \"suggestion\": \"The current code handles this correctly by converting to list immediately. Document this design decision in a comment to prevent future regressions.\"\n    },\n    {\n      \"type\": \"Security\",\n      \"severity\": \"low\",\n      \"description\": \"User-provided `extra_instructions` are directly injected into the prompt without sanitization. While LLMs are generally robust, this could enable prompt injection attacks.\",\n      \"line_number\": 149,\n      \"suggestion\": \"Add a note in docstring warning about prompt injection risks, or implement instruction validation: `if any(dangerous_pattern in extra_instructions for dangerous_pattern in [...]):`\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"The `_load_json` method handles markdown code fences but the logic is fragile - it assumes exactly one newline after the opening fence. Inconsistent formatting could break parsing.\",\n      \"line_number\": 192,\n      \"suggestion\": \"Use a more robust regex: `stripped = re.sub(r'^```(?:json)?\\\\s*\\\\n?|\\\\n?```$', '', stripped, flags=re.MULTILINE)`\"\n    },\n    {\n      \"type\": \"Type Safety\",\n      \"severity\": \"low\",\n      \"description\": \"The `_coerce_numeric` method accepts `str` and converts via `float()`, but doesn't handle edge cases like 'inf', 'nan', or scientific notation consistently.\",\n      \"line_number\": 259,\n      \"suggestion\": \"The `math.isfinite()` check already handles inf/nan correctly. Consider documenting this or explicitly rejecting string representations: `if stripped.lower() in ('inf', 'nan', '-inf'): raise ValueError(...)`\"\n    },\n    {\n      \"type\": \"Maintainability\",\n      \"severity\": \"low\",\n      \"description\": \"The `DEFAULT_RUBRIC` is a large constant that could grow. Consider moving to a separate config file or constant module.\",\n      \"line_number\": 25,\n      \"suggestion\": \"Extract to a `rubrics.py` module or load from JSON config file for easier maintenance and updates.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"The assertion `assert content is not None` at line 157 will raise `AssertionError` in production if somehow content is still None, which is not user-friendly.\",\n      \"line_number\": 157,\n      \"suggestion\": \"Replace with explicit check: `if content is None: raise EvaluationError('Failed to extract response content after cache and LLM call')`\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"priority\": \"high\",\n      \"title\": \"Add Response Schema Validation\",\n      \"description\": \"Create a Pydantic model or TypedDict for the expected LLM response structure to catch malformed responses early and provide clear error messages.\",\n      \"example\": \"from pydantic import BaseModel\\n\\nclass DimensionScoreResponse(BaseModel):\\n    score: float\\n    reasoning: Optional[str]\\n\\nclass EvaluationResponse(BaseModel):\\n    dimensions: Dict[str, DimensionScoreResponse]\"\n    },\n    {\n      \"priority\": \"high\",\n      \"title\": \"Improve Exception Specificity\",\n      \"description\": \"Replace broad `except Exception` with specific LiteLLM exceptions to avoid catching unintended errors and enable better error recovery strategies.\",\n      \"example\": \"from litellm import APIError, Timeout\\nexcept (APIError, Timeout, asyncio.TimeoutError) as exc:\"\n    },\n    {\n      \"priority\": \"medium\",\n      \"title\": \"Add Comprehensive Logging\",\n      \"description\": \"Add debug logging for cache operations, score normalization, and JSON parsing to aid troubleshooting in production.\",\n      \"example\": \"self._logger.debug(f'Normalized score {raw_score} (max={max_score}) to {normalized_score}')\"\n    },\n    {\n      \"priority\": \"medium\",\n      \"title\": \"Create Integration Tests with Mock LLM\",\n      \"description\": \"Add fixture-based tests that mock LiteLLM responses to test various edge cases (malformed JSON, out-of-range scores, missing dimensions) without calling the real API.\",\n      \"example\": \"@pytest.fixture\\ndef mock_llm(monkeypatch):\\n    async def mock_completion(**kwargs):\\n        return {...}\\n    monkeypatch.setattr('litellm.acompletion', mock_completion)\"\n    },\n    {\n      \"priority\": \"medium\",\n      \"title\": \"Add Configuration Validation\",\n      \"description\": \"Create a dataclass or Pydantic model for evaluator configuration to centralize validation and make parameters more discoverable.\",\n      \"example\": \"from dataclasses import dataclass\\n\\n@dataclass\\nclass EvaluatorConfig:\\n    model: str = DEFAULT_MODEL\\n    temperature: float = DEFAULT_TEMPERATURE\\n    max_retries: int = 3\\n    # ... validate in __post_init__\"\n    },\n    {\n      \"priority\": \"low\",\n      \"title\": \"Document Caching Behavior\",\n      \"description\": \"Add detailed docstring explaining cache key generation, when cache is used vs. recorded, and potential stale data issues.\",\n      \"example\": \"Cache behavior:\\n- use_cache=True: Return cached result if available\\n- record_cache=True: Store LLM response after fetching\\n- Both can be True: Use if available, record if not\\n- Cache key is deterministic based on prompt+model+temperature\"\n    },\n    {\n      \"priority\": \"low\",\n      \"title\": \"Add Metrics/Observability\",\n      \"description\": \"Track and expose metrics like cache hit rate, retry counts, and average response times for monitoring evaluator performance.\",\n      \"example\": \"self._cache_hits += 1\\nself._total_requests += 1\\nself._logger.info(f'Cache hit rate: {self._cache_hits/self._total_requests:.1%}')\"\n    }\n  ]\n}\n```",
        "test_config": {
          "test_number": 6,
          "config": {
            "temperature": 0.6,
            "model": "openrouter/anthropic/claude-haiku-4.5",
            "context_size": "full_module",
            "generation_strategy": "chain_of_thought"
          }
        },
        "workflow": {
          "started_at": "2025-10-26T06:23:55.842903Z",
          "completed_at": "2025-10-26T06:24:17.220803Z",
          "duration_seconds": 21.377665125008207
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLMs, but has significant gaps in edge case handling. The score extraction logic works for normal cases but fails silently on malformed responses (medium severity issue at line 171). The normalization logic has a critical bug where max_score defaults to 100 without validation, causing incorrect normalization for non-standard scales (line 237). Missing dimension validation (line 214) could mask incomplete evaluations. The default rubric is well-designed, but the overall robustness is compromised by these issues."
            },
            "error_handling": {
              "score": 58,
              "reasoning": "Error handling is inconsistent and overly broad. The critical issue is the bare `except Exception` at line 128 that catches KeyboardInterrupt and SystemExit, preventing proper shutdown and masking critical errors. The assertion at line 157 raises AssertionError instead of user-friendly EvaluationError. Input validation is minimal (only checks for empty strings, not length limits). Logging exists but lacks debug-level detail for troubleshooting cache operations and score normalization. The code lacks explicit validation of response structure before attempting to parse it, leading to potential silent failures."
            },
            "code_quality": {
              "score": 65,
              "reasoning": "The code has reasonable structure with clear method separation, but several design issues reduce quality. Type safety is weak due to loose isinstance() checks and getattr() fallbacks (line 171) instead of using TypedDict or Pydantic models. The tight coupling to litellm.acompletion (line 127) violates dependency injection principles and makes testing difficult. The _load_json method uses fragile string manipulation instead of robust regex (line 192). The DEFAULT_RUBRIC constant is large and should be extracted to a config module (line 25). Method naming is generally clear, but some methods like _coerce_numeric lack documentation of edge case handling."
            },
            "performance": {
              "score": 78,
              "reasoning": "The code demonstrates good performance practices with caching and retry logic. The async/await pattern is properly implemented. However, there are concerns: the messages parameter is converted to list immediately (line 118), which is correct but could be better documented. The regex patterns in _load_json are simple and efficient, though the markdown fence handling could be more robust. The caching strategy is sound with configurable use_cache and record_cache flags. No obvious bottlenecks exist, but the lack of metrics/observability (cache hit rates, retry counts) makes it difficult to optimize further. Token usage appears reasonable with configurable temperature and model selection."
            },
            "testability": {
              "score": 52,
              "reasoning": "The code has poor testability due to tight coupling to external dependencies. The direct dependency on litellm.acompletion (line 127) with no injected dependency or protocol makes unit testing difficult without complex monkeypatching. Methods like _extract_response_content and _parse_dimension_scores are not easily testable in isolation due to their reliance on specific response structures. The lack of a Protocol or abstract base class for LLM clients prevents easy mocking. While some methods like _normalize_score and _coerce_numeric could be pure functions, they're tightly integrated. The broad exception handling makes it hard to test specific error paths. No clear contracts are documented for method inputs/outputs."
            },
            "documentation": {
              "score": 68,
              "reasoning": "The code has basic documentation with docstrings for public methods, but significant gaps exist. Type hints are present but incomplete in some areas (loose typing with isinstance checks). Complex logic like score normalization lacks inline comments explaining the max_score extraction and defaulting behavior. The caching behavior is not well-documented (when cache is used vs. recorded, cache key generation). Edge cases like prompt injection risks (line 149) are not mentioned in docstrings. The _load_json markdown fence handling lacks explanation of its assumptions. Variable names are generally clear (e.g., workflow_output, dimension_scores), but some methods could benefit from more detailed docstrings explaining their contracts and potential failure modes."
            }
          },
          "temperature": 0.3,
          "cache_key": "ae8f08a03d5a365cd8c4228c0a83ea9f40481be0dcf6ccf5216852f58db985d9",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "timestamp": "2025-10-26T06:24:28.169557Z"
    },
    {
      "test_number": 7,
      "config": {
        "test_number": 7,
        "config_values": {
          "temperature": 0.6,
          "model": "openrouter/anthropic/claude-sonnet-4.5",
          "context_size": "file_only",
          "generation_strategy": "standard"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.72,
            "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-powered rubrics. However, critical vulnerabilities undermine reliability: prompt injection without sanitization (line 167) could cause incorrect evaluations or security breaches; broad exception handling (line 192) may mask failures; and fragile markdown fence stripping (line 252) could fail on edge cases. The score extraction and normalization logic appears sound, but the lack of validation that all rubric dimensions exist before processing (line 265) creates potential for incomplete evaluations. The default rubric is reasonable but the system's robustness is compromised by these issues."
          },
          "error_handling": {
            "score": 0.58,
            "reasoning": "Error handling is partially implemented but has significant gaps. The code uses EvaluationError appropriately in some places, but catches broad exceptions (line 192) that hide unexpected errors rather than distinguishing transient failures (rate limits, timeouts) from permanent ones. Input validation is minimal—workflow_output and extra_instructions lack sanitization despite security implications. Error messages could be more contextual. Logging is present but lacks correlation IDs for tracking through retries and caching. Missing validation that rubric dimensions exist before processing means errors surface late rather than failing fast with clear messages."
          },
          "code_quality": {
            "score": 0.65,
            "reasoning": "The code demonstrates good practices (type hints, logging, Python conventions) but violates Single Responsibility Principle significantly. The evaluate() method at 90+ lines handles caching, cache key resolution, LLM calls, response parsing, and score extraction—these should be separate private methods. Multiple isinstance checks (line 224) for dict/object access are verbose and error-prone; Pydantic would improve this. Magic numbers (0.25 jitter, base 2 exponential) lack explanation. Inconsistent use of assert for runtime checks (line 146) instead of proper exceptions is problematic. Hard dependency on litellm without abstraction creates tight coupling. Overall structure is reasonable but needs refactoring for maintainability."
          },
          "performance": {
            "score": 0.74,
            "reasoning": "The caching strategy is well-designed and should minimize unnecessary LLM calls effectively. Retry logic with exponential backoff is appropriate. However, several inefficiencies exist: unnecessary list() copy of messages in retry loop (line 188) wastes memory; markdown fence stripping uses simple string operations that could be optimized with regex; no caching of parsed rubric prompts despite identical prompts being rebuilt repeatedly. Input validation is missing—no max_input_length parameter means excessive token usage is possible. No rate limiting could cause API quota exhaustion. No timeout parameter could lead to indefinite hangs. The system lacks metrics/telemetry to identify performance bottlenecks in production."
          },
          "testability": {
            "score": 0.52,
            "reasoning": "Testability is significantly hampered by design choices. Hard dependency on litellm (line 193) requires mocking without dependency injection or protocol abstraction—no interface to swap implementations. The evaluate() method's 90+ lines with multiple responsibilities make unit testing difficult; extracting logic into separate methods would improve testability. Response parsing logic with isinstance checks is error-prone to test. No clear contracts for private methods. Edge cases like malformed LLM responses, missing scores, and markdown fence variations are difficult to reproduce without extensive mocking. The lack of structured input validation makes it hard to test error paths. Broad exception catching obscures failure modes. Overall, the code would require significant refactoring to achieve good test coverage."
          },
          "documentation": {
            "score": 0.68,
            "reasoning": "Type hints are present and mostly accurate, supporting code clarity. However, documentation has notable gaps: complex logic like cache key resolution, response parsing, and markdown fence stripping lack inline comments explaining intent. The evaluate() method is long and complex without docstring explaining its multiple responsibilities. Magic numbers (0.25, base 2) lack explanation. Variable names are generally clear but some could be more descriptive (e.g., 'payload' at line 188). Public API docstrings exist but could include more examples and edge case documentation. The fragile markdown fence logic (line 252) has no comment explaining why simple string operations were chosen over regex. Missing documentation of security considerations around prompt injection and input validation. Overall adequate but could be significantly improved for maintainability."
          }
        },
        "overall_score": 0.6483333333333333,
        "evaluator_model": "openrouter/anthropic/claude-haiku-4.5",
        "timestamp": "2025-10-26T06:25:12.714377Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-powered rubrics. However, critical vulnerabilities undermine reliability: prompt injection without sanitization (line 167) could cause incorrect evaluations or security breaches; broad exception handling (line 192) may mask failures; and fragile markdown fence stripping (line 252) could fail on edge cases. The score extraction and normalization logic appears sound, but the lack of validation that all rubric dimensions exist before processing (line 265) creates potential for incomplete evaluations. The default rubric is reasonable but the system's robustness is compromised by these issues."
            },
            "error_handling": {
              "score": 58,
              "reasoning": "Error handling is partially implemented but has significant gaps. The code uses EvaluationError appropriately in some places, but catches broad exceptions (line 192) that hide unexpected errors rather than distinguishing transient failures (rate limits, timeouts) from permanent ones. Input validation is minimal—workflow_output and extra_instructions lack sanitization despite security implications. Error messages could be more contextual. Logging is present but lacks correlation IDs for tracking through retries and caching. Missing validation that rubric dimensions exist before processing means errors surface late rather than failing fast with clear messages."
            },
            "code_quality": {
              "score": 65,
              "reasoning": "The code demonstrates good practices (type hints, logging, Python conventions) but violates Single Responsibility Principle significantly. The evaluate() method at 90+ lines handles caching, cache key resolution, LLM calls, response parsing, and score extraction—these should be separate private methods. Multiple isinstance checks (line 224) for dict/object access are verbose and error-prone; Pydantic would improve this. Magic numbers (0.25 jitter, base 2 exponential) lack explanation. Inconsistent use of assert for runtime checks (line 146) instead of proper exceptions is problematic. Hard dependency on litellm without abstraction creates tight coupling. Overall structure is reasonable but needs refactoring for maintainability."
            },
            "performance": {
              "score": 74,
              "reasoning": "The caching strategy is well-designed and should minimize unnecessary LLM calls effectively. Retry logic with exponential backoff is appropriate. However, several inefficiencies exist: unnecessary list() copy of messages in retry loop (line 188) wastes memory; markdown fence stripping uses simple string operations that could be optimized with regex; no caching of parsed rubric prompts despite identical prompts being rebuilt repeatedly. Input validation is missing—no max_input_length parameter means excessive token usage is possible. No rate limiting could cause API quota exhaustion. No timeout parameter could lead to indefinite hangs. The system lacks metrics/telemetry to identify performance bottlenecks in production."
            },
            "testability": {
              "score": 52,
              "reasoning": "Testability is significantly hampered by design choices. Hard dependency on litellm (line 193) requires mocking without dependency injection or protocol abstraction—no interface to swap implementations. The evaluate() method's 90+ lines with multiple responsibilities make unit testing difficult; extracting logic into separate methods would improve testability. Response parsing logic with isinstance checks is error-prone to test. No clear contracts for private methods. Edge cases like malformed LLM responses, missing scores, and markdown fence variations are difficult to reproduce without extensive mocking. The lack of structured input validation makes it hard to test error paths. Broad exception catching obscures failure modes. Overall, the code would require significant refactoring to achieve good test coverage."
            },
            "documentation": {
              "score": 68,
              "reasoning": "Type hints are present and mostly accurate, supporting code clarity. However, documentation has notable gaps: complex logic like cache key resolution, response parsing, and markdown fence stripping lack inline comments explaining intent. The evaluate() method is long and complex without docstring explaining its multiple responsibilities. Magic numbers (0.25, base 2) lack explanation. Variable names are generally clear but some could be more descriptive (e.g., 'payload' at line 188). Public API docstrings exist but could include more examples and edge case documentation. The fragile markdown fence logic (line 252) has no comment explaining why simple string operations were chosen over regex. Missing documentation of security considerations around prompt injection and input validation. Overall adequate but could be significantly improved for maintainability."
            }
          },
          "temperature": 0.3,
          "cache_key": "b79953eccb8fcfc20c8013404e0f20b470e9bcc58c95455bb44e451e8e04a05a",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "cost": 0.0,
      "latency": 31694.185041997116,
      "utility": 0.6372043078166166,
      "workflow_output": "Well-structured LLM-powered rubric evaluator with good error handling, retry logic, and caching support. The code follows Python best practices with type hints, proper logging, and comprehensive validation. However, there are opportunities to improve testability, reduce complexity, and enhance security around prompt injection.\nIssues:\n- (high) Potential prompt injection vulnerability - workflow_output is directly embedded in prompts without sanitization [line 167] Suggested fix: Sanitize workflow_output to prevent prompt injection attacks. Consider escaping special characters, limiting length, or using structured input formats that clearly separate user content from instructions.\n- (medium) extra_instructions parameter allows arbitrary text injection into prompts without validation [line 171] Suggested fix: Add validation for extra_instructions to prevent malicious prompt manipulation. Consider a whitelist approach or structured format for additional instructions.\n- (medium) The evaluate() method is too long (90+ lines) and handles multiple responsibilities [line 68] Suggested fix: Extract caching logic, cache key resolution, and response processing into separate private methods to improve readability and testability.\n- (medium) Broad exception catching in retry logic may hide unexpected errors [line 192] Suggested fix: Catch specific LiteLLM exceptions (e.g., rate limit, timeout, authentication errors) and handle them appropriately. Only retry on transient errors, not permanent failures.\n- (low) list(messages) creates unnecessary copy of messages in retry loop [line 188] Suggested fix: Remove 'payload = list(messages)' and pass messages directly to acompletion() since it's not modified.\n- (low) Multiple isinstance checks for dict/object attribute access is verbose and error-prone [line 224] Suggested fix: Consider using a helper method or library (like pydantic) to normalize response objects to a consistent structure.\n- (medium) Hard dependency on litellm makes unit testing difficult without mocking [line 193] Suggested fix: Inject an LLM client interface to allow easier mocking in tests. Consider using dependency injection or a protocol/abstract base class.\n- (low) Default max_score of 100.0 may not match actual rubric scales [line 290] Suggested fix: Consider making the fallback configurable or raising an error when scale cannot be parsed to ensure accurate normalization.\n- (low) Magic numbers in retry delay calculation (0.25 jitter factor, exponential base 2) [line 211] Suggested fix: Extract jitter factor and exponential base as class constants with descriptive names for better maintainability.\n- (low) Markdown fence stripping logic is fragile and may fail on edge cases [line 252] Suggested fix: Use more robust regex-based markdown fence detection and removal. Handle cases with language specifiers (```json, ```JSON) and nested backticks.\n- (medium) No validation that all rubric dimensions are present in the response before processing [line 265] Suggested fix: Add upfront validation to check if all expected dimensions exist in the payload before parsing individual scores to provide clearer error messages.\n- (low) Inconsistent use of assert for type checking vs raising exceptions [line 146] Suggested fix: Replace asserts used for runtime checks with proper if/raise statements. Asserts can be disabled with -O flag and should only be used for debugging.\nSuggestions:\n- {'description': 'Add input validation for workflow_output length to prevent excessive token usage and costs', 'example': 'Add a configurable max_input_length parameter and validate in evaluate()'}\n- {'description': 'Implement structured logging with correlation IDs to track evaluation requests through caching and retries', 'example': 'Add evaluation_id to log messages for better debugging and observability'}\n- {'description': 'Add metrics/telemetry for cache hit rates, retry counts, and evaluation latencies', 'example': 'Emit metrics using a pluggable metrics backend for production monitoring'}\n- {'description': 'Consider adding async context manager support for resource cleanup', 'example': 'Implement __aenter__ and __aexit__ to properly manage cache connections and LLM client lifecycle'}\n- {'description': 'Add validation that rubric dimensions have required keys (description, scale)', 'example': 'Validate rubric structure in evaluate() before building prompts to fail fast with clear errors'}\n- {'description': 'Implement rate limiting to prevent API quota exhaustion', 'example': 'Add configurable rate limiter using asyncio.Semaphore or external rate limiting library'}\n- {'description': 'Add timeout parameter for LLM requests to prevent indefinite hangs', 'example': 'Pass timeout to litellm.acompletion() and make it configurable'}\n- {'description': 'Consider caching parsed rubric prompts to avoid rebuilding identical prompts', 'example': 'Add an LRU cache for _build_prompt() results based on rubric hash'}",
      "metadata": {
        "strategy": "standard",
        "model": "openrouter/anthropic/claude-sonnet-4.5",
        "temperature": 0.6,
        "context_size": "file_only",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Review limited to the provided file.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"Well-structured LLM-powered rubric evaluator with good error handling, retry logic, and caching support. The code follows Python best practices with type hints, proper logging, and comprehensive validation. However, there are opportunities to improve testability, reduce complexity, and enhance security around prompt injection.\",\n  \"issues\": [\n    {\n      \"type\": \"Security\",\n      \"severity\": \"high\",\n      \"description\": \"Potential prompt injection vulnerability - workflow_output is directly embedded in prompts without sanitization\",\n      \"line_number\": 167,\n      \"suggestion\": \"Sanitize workflow_output to prevent prompt injection attacks. Consider escaping special characters, limiting length, or using structured input formats that clearly separate user content from instructions.\"\n    },\n    {\n      \"type\": \"Security\",\n      \"severity\": \"medium\",\n      \"description\": \"extra_instructions parameter allows arbitrary text injection into prompts without validation\",\n      \"line_number\": 171,\n      \"suggestion\": \"Add validation for extra_instructions to prevent malicious prompt manipulation. Consider a whitelist approach or structured format for additional instructions.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"medium\",\n      \"description\": \"The evaluate() method is too long (90+ lines) and handles multiple responsibilities\",\n      \"line_number\": 68,\n      \"suggestion\": \"Extract caching logic, cache key resolution, and response processing into separate private methods to improve readability and testability.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"Broad exception catching in retry logic may hide unexpected errors\",\n      \"line_number\": 192,\n      \"suggestion\": \"Catch specific LiteLLM exceptions (e.g., rate limit, timeout, authentication errors) and handle them appropriately. Only retry on transient errors, not permanent failures.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"list(messages) creates unnecessary copy of messages in retry loop\",\n      \"line_number\": 188,\n      \"suggestion\": \"Remove 'payload = list(messages)' and pass messages directly to acompletion() since it's not modified.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Multiple isinstance checks for dict/object attribute access is verbose and error-prone\",\n      \"line_number\": 224,\n      \"suggestion\": \"Consider using a helper method or library (like pydantic) to normalize response objects to a consistent structure.\"\n    },\n    {\n      \"type\": \"Testing\",\n      \"severity\": \"medium\",\n      \"description\": \"Hard dependency on litellm makes unit testing difficult without mocking\",\n      \"line_number\": 193,\n      \"suggestion\": \"Inject an LLM client interface to allow easier mocking in tests. Consider using dependency injection or a protocol/abstract base class.\"\n    },\n    {\n      \"type\": \"Functionality\",\n      \"severity\": \"low\",\n      \"description\": \"Default max_score of 100.0 may not match actual rubric scales\",\n      \"line_number\": 290,\n      \"suggestion\": \"Consider making the fallback configurable or raising an error when scale cannot be parsed to ensure accurate normalization.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Magic numbers in retry delay calculation (0.25 jitter factor, exponential base 2)\",\n      \"line_number\": 211,\n      \"suggestion\": \"Extract jitter factor and exponential base as class constants with descriptive names for better maintainability.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"low\",\n      \"description\": \"Markdown fence stripping logic is fragile and may fail on edge cases\",\n      \"line_number\": 252,\n      \"suggestion\": \"Use more robust regex-based markdown fence detection and removal. Handle cases with language specifiers (```json, ```JSON) and nested backticks.\"\n    },\n    {\n      \"type\": \"Functionality\",\n      \"severity\": \"medium\",\n      \"description\": \"No validation that all rubric dimensions are present in the response before processing\",\n      \"line_number\": 265,\n      \"suggestion\": \"Add upfront validation to check if all expected dimensions exist in the payload before parsing individual scores to provide clearer error messages.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Inconsistent use of assert for type checking vs raising exceptions\",\n      \"line_number\": 146,\n      \"suggestion\": \"Replace asserts used for runtime checks with proper if/raise statements. Asserts can be disabled with -O flag and should only be used for debugging.\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"description\": \"Add input validation for workflow_output length to prevent excessive token usage and costs\",\n      \"example\": \"Add a configurable max_input_length parameter and validate in evaluate()\"\n    },\n    {\n      \"description\": \"Implement structured logging with correlation IDs to track evaluation requests through caching and retries\",\n      \"example\": \"Add evaluation_id to log messages for better debugging and observability\"\n    },\n    {\n      \"description\": \"Add metrics/telemetry for cache hit rates, retry counts, and evaluation latencies\",\n      \"example\": \"Emit metrics using a pluggable metrics backend for production monitoring\"\n    },\n    {\n      \"description\": \"Consider adding async context manager support for resource cleanup\",\n      \"example\": \"Implement __aenter__ and __aexit__ to properly manage cache connections and LLM client lifecycle\"\n    },\n    {\n      \"description\": \"Add validation that rubric dimensions have required keys (description, scale)\",\n      \"example\": \"Validate rubric structure in evaluate() before building prompts to fail fast with clear errors\"\n    },\n    {\n      \"description\": \"Implement rate limiting to prevent API quota exhaustion\",\n      \"example\": \"Add configurable rate limiter using asyncio.Semaphore or external rate limiting library\"\n    },\n    {\n      \"description\": \"Add timeout parameter for LLM requests to prevent indefinite hangs\",\n      \"example\": \"Pass timeout to litellm.acompletion() and make it configurable\"\n    },\n    {\n      \"description\": \"Consider caching parsed rubric prompts to avoid rebuilding identical prompts\",\n      \"example\": \"Add an LRU cache for _build_prompt() results based on rubric hash\"\n    }\n  ]\n}\n```",
        "test_config": {
          "test_number": 7,
          "config": {
            "temperature": 0.6,
            "model": "openrouter/anthropic/claude-sonnet-4.5",
            "context_size": "file_only",
            "generation_strategy": "standard"
          }
        },
        "workflow": {
          "started_at": "2025-10-26T06:24:28.172205Z",
          "completed_at": "2025-10-26T06:24:59.866738Z",
          "duration_seconds": 31.694185041997116
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-powered rubrics. However, critical vulnerabilities undermine reliability: prompt injection without sanitization (line 167) could cause incorrect evaluations or security breaches; broad exception handling (line 192) may mask failures; and fragile markdown fence stripping (line 252) could fail on edge cases. The score extraction and normalization logic appears sound, but the lack of validation that all rubric dimensions exist before processing (line 265) creates potential for incomplete evaluations. The default rubric is reasonable but the system's robustness is compromised by these issues."
            },
            "error_handling": {
              "score": 58,
              "reasoning": "Error handling is partially implemented but has significant gaps. The code uses EvaluationError appropriately in some places, but catches broad exceptions (line 192) that hide unexpected errors rather than distinguishing transient failures (rate limits, timeouts) from permanent ones. Input validation is minimal—workflow_output and extra_instructions lack sanitization despite security implications. Error messages could be more contextual. Logging is present but lacks correlation IDs for tracking through retries and caching. Missing validation that rubric dimensions exist before processing means errors surface late rather than failing fast with clear messages."
            },
            "code_quality": {
              "score": 65,
              "reasoning": "The code demonstrates good practices (type hints, logging, Python conventions) but violates Single Responsibility Principle significantly. The evaluate() method at 90+ lines handles caching, cache key resolution, LLM calls, response parsing, and score extraction—these should be separate private methods. Multiple isinstance checks (line 224) for dict/object access are verbose and error-prone; Pydantic would improve this. Magic numbers (0.25 jitter, base 2 exponential) lack explanation. Inconsistent use of assert for runtime checks (line 146) instead of proper exceptions is problematic. Hard dependency on litellm without abstraction creates tight coupling. Overall structure is reasonable but needs refactoring for maintainability."
            },
            "performance": {
              "score": 74,
              "reasoning": "The caching strategy is well-designed and should minimize unnecessary LLM calls effectively. Retry logic with exponential backoff is appropriate. However, several inefficiencies exist: unnecessary list() copy of messages in retry loop (line 188) wastes memory; markdown fence stripping uses simple string operations that could be optimized with regex; no caching of parsed rubric prompts despite identical prompts being rebuilt repeatedly. Input validation is missing—no max_input_length parameter means excessive token usage is possible. No rate limiting could cause API quota exhaustion. No timeout parameter could lead to indefinite hangs. The system lacks metrics/telemetry to identify performance bottlenecks in production."
            },
            "testability": {
              "score": 52,
              "reasoning": "Testability is significantly hampered by design choices. Hard dependency on litellm (line 193) requires mocking without dependency injection or protocol abstraction—no interface to swap implementations. The evaluate() method's 90+ lines with multiple responsibilities make unit testing difficult; extracting logic into separate methods would improve testability. Response parsing logic with isinstance checks is error-prone to test. No clear contracts for private methods. Edge cases like malformed LLM responses, missing scores, and markdown fence variations are difficult to reproduce without extensive mocking. The lack of structured input validation makes it hard to test error paths. Broad exception catching obscures failure modes. Overall, the code would require significant refactoring to achieve good test coverage."
            },
            "documentation": {
              "score": 68,
              "reasoning": "Type hints are present and mostly accurate, supporting code clarity. However, documentation has notable gaps: complex logic like cache key resolution, response parsing, and markdown fence stripping lack inline comments explaining intent. The evaluate() method is long and complex without docstring explaining its multiple responsibilities. Magic numbers (0.25, base 2) lack explanation. Variable names are generally clear but some could be more descriptive (e.g., 'payload' at line 188). Public API docstrings exist but could include more examples and edge case documentation. The fragile markdown fence logic (line 252) has no comment explaining why simple string operations were chosen over regex. Missing documentation of security considerations around prompt injection and input validation. Overall adequate but could be significantly improved for maintainability."
            }
          },
          "temperature": 0.3,
          "cache_key": "b79953eccb8fcfc20c8013404e0f20b470e9bcc58c95455bb44e451e8e04a05a",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "timestamp": "2025-10-26T06:25:12.714822Z"
    },
    {
      "test_number": 8,
      "config": {
        "test_number": 8,
        "config_values": {
          "temperature": 0.6,
          "model": "openrouter/anthropic/claude-sonnet-4.5",
          "context_size": "file_only",
          "generation_strategy": "chain_of_thought"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.72,
            "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-based rubrics. Core logic is sound with retry mechanisms and caching. However, there are notable gaps: (1) Rubric validation is incomplete—doesn't verify non-empty dimensions or required keys, (2) The regex in _extract_max_score may fail on non-standard scale formats like '1 to 10' or 'Low-High', (3) No timeout on LLM calls could cause indefinite hangs. The score extraction and normalization logic appear correct for standard cases, but edge case handling is incomplete."
          },
          "error_handling": {
            "score": 0.68,
            "reasoning": "Good use of custom EvaluationError exceptions and input validation for temperature/top_p ranges. Retry logic with exponential backoff is well-implemented. However, significant gaps exist: (1) No size limit on workflow_output could enable DoS or excessive API costs, (2) Missing timeout configuration on LiteLLM calls, (3) Incomplete rubric validation (empty rubrics not caught), (4) Cache key validation happens after cache operations are determined, creating confusing flow. Error messages could be more contextual (e.g., including actual invalid values). Logging is present but could be more structured."
          },
          "code_quality": {
            "score": 0.7,
            "reasoning": "Code demonstrates good Python practices: proper type hints, PEP 8 naming, dataclass usage, and logical method organization. Separation of concerns is evident with helper methods. However, quality issues reduce the score: (1) The evaluate method has 11 parameters and high cyclomatic complexity—should use config dataclasses, (2) Multiple isinstance checks for Mapping vs object attributes complicate maintenance, (3) JSON markdown fence stripping is fragile, (4) Cache key computation happens even when caching is disabled. The code lacks Pydantic validation for rubric structure, relying instead on runtime checks. No leaky abstractions detected, but abstraction could be cleaner."
          },
          "performance": {
            "score": 0.75,
            "reasoning": "Caching mechanism effectively reduces redundant API calls, and async/await enables non-blocking I/O. No obvious algorithmic bottlenecks detected. However, performance concerns exist: (1) Cache key is computed unconditionally even when caching is disabled, (2) No timeout on LiteLLM calls could cause resource exhaustion, (3) No size limits on workflow_output could lead to excessive token usage and API costs, (4) Regex patterns appear reasonable but could be optimized. The retry mechanism with exponential backoff is appropriate. Overall, the design is sound but lacks safeguards against resource abuse."
          },
          "testability": {
            "score": 0.73,
            "reasoning": "Code has good testability foundations: dependency injection for logger and cache, well-isolated helper methods, and clear method contracts. Async design is testable with proper async test frameworks. However, testability is hindered by: (1) Random jitter in retry delays makes tests non-deterministic without seeding, (2) No injectable random number generator for testing, (3) Multiple isinstance checks and complex response handling make mocking harder, (4) Rubric validation logic is scattered, making edge cases harder to test in isolation. The evaluate method's complexity and parameter count make it harder to test all code paths. Overall, the code is testable but could be improved with better isolation and determinism."
          },
          "documentation": {
            "score": 0.74,
            "reasoning": "Code includes docstrings and type hints that are generally accurate and helpful. Variable and method names are clear and descriptive. However, documentation gaps exist: (1) No explanation of expected scale format in DEFAULT_RUBRIC or _extract_max_score, (2) Missing documentation of how normalization works (0-1 range), (3) No examples of rubric structure or expected LLM response format, (4) Complex logic in _extract_response_content and JSON parsing lacks inline comments, (5) No documentation of cache key format or caching behavior. Type hints are present and mostly accurate. The code would benefit from more inline comments explaining the regex patterns and response parsing logic, plus examples in docstrings."
          }
        },
        "overall_score": 0.7200000000000001,
        "evaluator_model": "openrouter/anthropic/claude-haiku-4.5",
        "timestamp": "2025-10-26T06:26:00.922844Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-based rubrics. Core logic is sound with retry mechanisms and caching. However, there are notable gaps: (1) Rubric validation is incomplete—doesn't verify non-empty dimensions or required keys, (2) The regex in _extract_max_score may fail on non-standard scale formats like '1 to 10' or 'Low-High', (3) No timeout on LLM calls could cause indefinite hangs. The score extraction and normalization logic appear correct for standard cases, but edge case handling is incomplete."
            },
            "error_handling": {
              "score": 68,
              "reasoning": "Good use of custom EvaluationError exceptions and input validation for temperature/top_p ranges. Retry logic with exponential backoff is well-implemented. However, significant gaps exist: (1) No size limit on workflow_output could enable DoS or excessive API costs, (2) Missing timeout configuration on LiteLLM calls, (3) Incomplete rubric validation (empty rubrics not caught), (4) Cache key validation happens after cache operations are determined, creating confusing flow. Error messages could be more contextual (e.g., including actual invalid values). Logging is present but could be more structured."
            },
            "code_quality": {
              "score": 70,
              "reasoning": "Code demonstrates good Python practices: proper type hints, PEP 8 naming, dataclass usage, and logical method organization. Separation of concerns is evident with helper methods. However, quality issues reduce the score: (1) The evaluate method has 11 parameters and high cyclomatic complexity—should use config dataclasses, (2) Multiple isinstance checks for Mapping vs object attributes complicate maintenance, (3) JSON markdown fence stripping is fragile, (4) Cache key computation happens even when caching is disabled. The code lacks Pydantic validation for rubric structure, relying instead on runtime checks. No leaky abstractions detected, but abstraction could be cleaner."
            },
            "performance": {
              "score": 75,
              "reasoning": "Caching mechanism effectively reduces redundant API calls, and async/await enables non-blocking I/O. No obvious algorithmic bottlenecks detected. However, performance concerns exist: (1) Cache key is computed unconditionally even when caching is disabled, (2) No timeout on LiteLLM calls could cause resource exhaustion, (3) No size limits on workflow_output could lead to excessive token usage and API costs, (4) Regex patterns appear reasonable but could be optimized. The retry mechanism with exponential backoff is appropriate. Overall, the design is sound but lacks safeguards against resource abuse."
            },
            "testability": {
              "score": 73,
              "reasoning": "Code has good testability foundations: dependency injection for logger and cache, well-isolated helper methods, and clear method contracts. Async design is testable with proper async test frameworks. However, testability is hindered by: (1) Random jitter in retry delays makes tests non-deterministic without seeding, (2) No injectable random number generator for testing, (3) Multiple isinstance checks and complex response handling make mocking harder, (4) Rubric validation logic is scattered, making edge cases harder to test in isolation. The evaluate method's complexity and parameter count make it harder to test all code paths. Overall, the code is testable but could be improved with better isolation and determinism."
            },
            "documentation": {
              "score": 74,
              "reasoning": "Code includes docstrings and type hints that are generally accurate and helpful. Variable and method names are clear and descriptive. However, documentation gaps exist: (1) No explanation of expected scale format in DEFAULT_RUBRIC or _extract_max_score, (2) Missing documentation of how normalization works (0-1 range), (3) No examples of rubric structure or expected LLM response format, (4) Complex logic in _extract_response_content and JSON parsing lacks inline comments, (5) No documentation of cache key format or caching behavior. Type hints are present and mostly accurate. The code would benefit from more inline comments explaining the regex patterns and response parsing logic, plus examples in docstrings."
            }
          },
          "temperature": 0.3,
          "cache_key": "37ca860d3a00231b1773712c9ae7ea2f147a7f8a306f063ab32d39e541e3679d",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "cost": 0.0,
      "latency": 37406.7895830085,
      "utility": 0.70487235554801,
      "workflow_output": "# Code Review: RubricEvaluator\n\n## Step-by-Step Analysis\n\n### 1. Functionality\n- Core evaluation logic appears sound with retry mechanism, caching, and JSON parsing\n- Edge cases are handled (empty strings, invalid JSON, missing scores)\n- The normalization logic extracts max scores from rubric scales and normalizes to 0-1 range\n- Potential issue: The regex in `_extract_max_score` may not handle all scale formats\n\n### 2. Code Quality\n- Good use of type hints and dataclasses\n- Follows PEP 8 naming conventions\n- Proper separation of concerns with helper methods\n- Some methods are quite long (e.g., `evaluate`)\n\n### 3. Readability\n- Well-documented with docstrings\n- Clear variable names\n- Logical method organization\n- The `evaluate` method has high complexity with many parameters and branching logic\n\n### 4. Error Handling\n- Custom `EvaluationError` exceptions are used appropriately\n- Input validation is thorough\n- Retry logic with exponential backoff is well-implemented\n- Missing: validation that rubric dimensions are non-empty\n\n### 5. Testing\n- Code is testable with dependency injection (logger, cache)\n- Async design may complicate testing slightly\n- Helper methods are well-isolated for unit testing\n\n### 6. Performance\n- Caching mechanism reduces redundant API calls\n- Async/await for non-blocking I/O\n- No obvious bottlenecks\n- Potential issue: No timeout on LiteLLM calls\n\n### 7. Security\n- Input validation prevents empty strings\n- JSON parsing is safe (uses standard library)\n- No SQL injection or XSS concerns\n- Potential issue: No size limits on workflow_output could lead to excessive API costs\n\n## Detailed Issues\n\n```json\n{\n  \"summary\": \"Well-structured LLM evaluation code with good error handling and caching. Main concerns are around input validation completeness, potential API timeout issues, and some complexity in the main evaluate method. Code follows Python best practices and is generally production-ready with minor improvements needed.\",\n  \"issues\": [\n    {\n      \"type\": \"security\",\n      \"severity\": \"medium\",\n      \"description\": \"No size limit on workflow_output parameter could lead to excessive API costs or denial of service\",\n      \"line_number\": 68,\n      \"suggestion\": \"Add a maximum length check for workflow_output (e.g., 50KB) and raise EvaluationError if exceeded. Consider adding a parameter to configure this limit.\"\n    },\n    {\n      \"type\": \"error_handling\",\n      \"severity\": \"medium\",\n      \"description\": \"No timeout configured for LiteLLM API calls, which could hang indefinitely\",\n      \"line_number\": 159,\n      \"suggestion\": \"Add a timeout parameter to litellm.acompletion() call, e.g., timeout=60.0, and make it configurable via __init__\"\n    },\n    {\n      \"type\": \"functionality\",\n      \"severity\": \"medium\",\n      \"description\": \"Rubric validation is incomplete - doesn't check if rubric is empty or if dimensions have required keys\",\n      \"line_number\": 78,\n      \"suggestion\": \"Add validation: if not rubric_definition: raise EvaluationError('Rubric cannot be empty'). Also validate that each dimension has 'description' and 'scale' keys.\"\n    },\n    {\n      \"type\": \"code_quality\",\n      \"severity\": \"low\",\n      \"description\": \"The evaluate method has too many parameters (11) and high cyclomatic complexity\",\n      \"line_number\": 68,\n      \"suggestion\": \"Consider creating an EvaluationConfig dataclass to group related parameters (model, temperature, cache settings, etc.)\"\n    },\n    {\n      \"type\": \"functionality\",\n      \"severity\": \"low\",\n      \"description\": \"Regex pattern in _extract_max_score may fail on edge cases like '1 to 10' or 'Low-High'\",\n      \"line_number\": 283,\n      \"suggestion\": \"Add more robust parsing with multiple patterns or explicit error handling when scale format is unexpected. Log a warning when defaulting to 100.0.\"\n    },\n    {\n      \"type\": \"error_handling\",\n      \"severity\": \"low\",\n      \"description\": \"Cache key validation happens after cache operations are determined, potentially confusing flow\",\n      \"line_number\": 102,\n      \"suggestion\": \"Move cache key validation earlier, right after determining if caching is requested\"\n    },\n    {\n      \"type\": \"code_quality\",\n      \"severity\": \"low\",\n      \"description\": \"Multiple isinstance checks for Mapping vs object attributes makes code harder to maintain\",\n      \"line_number\": 234,\n      \"suggestion\": \"Consider normalizing the response object to a dict early in _extract_response_content or use a response wrapper class\"\n    },\n    {\n      \"type\": \"functionality\",\n      \"severity\": \"low\",\n      \"description\": \"JSON markdown fence stripping logic is fragile and may fail on nested code blocks\",\n      \"line_number\": 260,\n      \"suggestion\": \"Use a more robust approach: strip only if starts with ```json or ``` and ends with ```, using regex or proper state tracking\"\n    },\n    {\n      \"type\": \"testing\",\n      \"severity\": \"low\",\n      \"description\": \"Random jitter in retry delay makes testing non-deterministic\",\n      \"line_number\": 174,\n      \"suggestion\": \"Add a seed parameter or allow injecting a random number generator for deterministic testing\"\n    },\n    {\n      \"type\": \"performance\",\n      \"severity\": \"low\",\n      \"description\": \"Cache key is computed even when caching is disabled\",\n      \"line_number\": 102,\n      \"suggestion\": \"Move cache key computation inside the 'if cache_backend is not None' block to avoid unnecessary work\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"description\": \"Add input size limits\",\n      \"details\": \"Implement MAX_OUTPUT_LENGTH constant (e.g., 50000 chars) and validate workflow_output length to prevent API abuse and excessive costs\"\n    },\n    {\n      \"description\": \"Improve rubric validation\",\n      \"details\": \"Create a _validate_rubric() method that checks: non-empty dict, all dimensions have required keys ('description', 'scale'), and dimension names are valid identifiers\"\n    },\n    {\n      \"description\": \"Add structured logging\",\n      \"details\": \"Include more context in log messages (e.g., cache_key, model, attempt number) using structured logging with extra fields for better observability\"\n    },\n    {\n      \"description\": \"Create configuration dataclass\",\n      \"details\": \"Define EvaluationConfig and CacheConfig dataclasses to reduce parameter count and improve type safety\"\n    },\n    {\n      \"description\": \"Add metrics/observability\",\n      \"details\": \"Consider adding hooks for metrics collection (evaluation duration, cache hit rate, retry counts) to enable monitoring in production\"\n    },\n    {\n      \"description\": \"Improve error messages\",\n      \"details\": \"Include the actual invalid value in error messages (e.g., 'Temperature 1.5 must be between 0.0 and 1.0') for easier debugging\"\n    },\n    {\n      \"description\": \"Add type validation for rubric structure\",\n      \"details\": \"Use TypedDict or Pydantic models to enforce rubric structure at runtime, catching configuration errors earlier\"\n    },\n    {\n      \"description\": \"Document scale format expectations\",\n      \"details\": \"Add docstring to DEFAULT_RUBRIC explaining expected scale format and how it's parsed by _extract_max_score\"\n    }\n  ]\n}\n```",
      "metadata": {
        "strategy": "chain_of_thought",
        "model": "openrouter/anthropic/claude-sonnet-4.5",
        "temperature": 0.6,
        "context_size": "file_only",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Review limited to the provided file.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "# Code Review: RubricEvaluator\n\n## Step-by-Step Analysis\n\n### 1. Functionality\n- Core evaluation logic appears sound with retry mechanism, caching, and JSON parsing\n- Edge cases are handled (empty strings, invalid JSON, missing scores)\n- The normalization logic extracts max scores from rubric scales and normalizes to 0-1 range\n- Potential issue: The regex in `_extract_max_score` may not handle all scale formats\n\n### 2. Code Quality\n- Good use of type hints and dataclasses\n- Follows PEP 8 naming conventions\n- Proper separation of concerns with helper methods\n- Some methods are quite long (e.g., `evaluate`)\n\n### 3. Readability\n- Well-documented with docstrings\n- Clear variable names\n- Logical method organization\n- The `evaluate` method has high complexity with many parameters and branching logic\n\n### 4. Error Handling\n- Custom `EvaluationError` exceptions are used appropriately\n- Input validation is thorough\n- Retry logic with exponential backoff is well-implemented\n- Missing: validation that rubric dimensions are non-empty\n\n### 5. Testing\n- Code is testable with dependency injection (logger, cache)\n- Async design may complicate testing slightly\n- Helper methods are well-isolated for unit testing\n\n### 6. Performance\n- Caching mechanism reduces redundant API calls\n- Async/await for non-blocking I/O\n- No obvious bottlenecks\n- Potential issue: No timeout on LiteLLM calls\n\n### 7. Security\n- Input validation prevents empty strings\n- JSON parsing is safe (uses standard library)\n- No SQL injection or XSS concerns\n- Potential issue: No size limits on workflow_output could lead to excessive API costs\n\n## Detailed Issues\n\n```json\n{\n  \"summary\": \"Well-structured LLM evaluation code with good error handling and caching. Main concerns are around input validation completeness, potential API timeout issues, and some complexity in the main evaluate method. Code follows Python best practices and is generally production-ready with minor improvements needed.\",\n  \"issues\": [\n    {\n      \"type\": \"security\",\n      \"severity\": \"medium\",\n      \"description\": \"No size limit on workflow_output parameter could lead to excessive API costs or denial of service\",\n      \"line_number\": 68,\n      \"suggestion\": \"Add a maximum length check for workflow_output (e.g., 50KB) and raise EvaluationError if exceeded. Consider adding a parameter to configure this limit.\"\n    },\n    {\n      \"type\": \"error_handling\",\n      \"severity\": \"medium\",\n      \"description\": \"No timeout configured for LiteLLM API calls, which could hang indefinitely\",\n      \"line_number\": 159,\n      \"suggestion\": \"Add a timeout parameter to litellm.acompletion() call, e.g., timeout=60.0, and make it configurable via __init__\"\n    },\n    {\n      \"type\": \"functionality\",\n      \"severity\": \"medium\",\n      \"description\": \"Rubric validation is incomplete - doesn't check if rubric is empty or if dimensions have required keys\",\n      \"line_number\": 78,\n      \"suggestion\": \"Add validation: if not rubric_definition: raise EvaluationError('Rubric cannot be empty'). Also validate that each dimension has 'description' and 'scale' keys.\"\n    },\n    {\n      \"type\": \"code_quality\",\n      \"severity\": \"low\",\n      \"description\": \"The evaluate method has too many parameters (11) and high cyclomatic complexity\",\n      \"line_number\": 68,\n      \"suggestion\": \"Consider creating an EvaluationConfig dataclass to group related parameters (model, temperature, cache settings, etc.)\"\n    },\n    {\n      \"type\": \"functionality\",\n      \"severity\": \"low\",\n      \"description\": \"Regex pattern in _extract_max_score may fail on edge cases like '1 to 10' or 'Low-High'\",\n      \"line_number\": 283,\n      \"suggestion\": \"Add more robust parsing with multiple patterns or explicit error handling when scale format is unexpected. Log a warning when defaulting to 100.0.\"\n    },\n    {\n      \"type\": \"error_handling\",\n      \"severity\": \"low\",\n      \"description\": \"Cache key validation happens after cache operations are determined, potentially confusing flow\",\n      \"line_number\": 102,\n      \"suggestion\": \"Move cache key validation earlier, right after determining if caching is requested\"\n    },\n    {\n      \"type\": \"code_quality\",\n      \"severity\": \"low\",\n      \"description\": \"Multiple isinstance checks for Mapping vs object attributes makes code harder to maintain\",\n      \"line_number\": 234,\n      \"suggestion\": \"Consider normalizing the response object to a dict early in _extract_response_content or use a response wrapper class\"\n    },\n    {\n      \"type\": \"functionality\",\n      \"severity\": \"low\",\n      \"description\": \"JSON markdown fence stripping logic is fragile and may fail on nested code blocks\",\n      \"line_number\": 260,\n      \"suggestion\": \"Use a more robust approach: strip only if starts with ```json or ``` and ends with ```, using regex or proper state tracking\"\n    },\n    {\n      \"type\": \"testing\",\n      \"severity\": \"low\",\n      \"description\": \"Random jitter in retry delay makes testing non-deterministic\",\n      \"line_number\": 174,\n      \"suggestion\": \"Add a seed parameter or allow injecting a random number generator for deterministic testing\"\n    },\n    {\n      \"type\": \"performance\",\n      \"severity\": \"low\",\n      \"description\": \"Cache key is computed even when caching is disabled\",\n      \"line_number\": 102,\n      \"suggestion\": \"Move cache key computation inside the 'if cache_backend is not None' block to avoid unnecessary work\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"description\": \"Add input size limits\",\n      \"details\": \"Implement MAX_OUTPUT_LENGTH constant (e.g., 50000 chars) and validate workflow_output length to prevent API abuse and excessive costs\"\n    },\n    {\n      \"description\": \"Improve rubric validation\",\n      \"details\": \"Create a _validate_rubric() method that checks: non-empty dict, all dimensions have required keys ('description', 'scale'), and dimension names are valid identifiers\"\n    },\n    {\n      \"description\": \"Add structured logging\",\n      \"details\": \"Include more context in log messages (e.g., cache_key, model, attempt number) using structured logging with extra fields for better observability\"\n    },\n    {\n      \"description\": \"Create configuration dataclass\",\n      \"details\": \"Define EvaluationConfig and CacheConfig dataclasses to reduce parameter count and improve type safety\"\n    },\n    {\n      \"description\": \"Add metrics/observability\",\n      \"details\": \"Consider adding hooks for metrics collection (evaluation duration, cache hit rate, retry counts) to enable monitoring in production\"\n    },\n    {\n      \"description\": \"Improve error messages\",\n      \"details\": \"Include the actual invalid value in error messages (e.g., 'Temperature 1.5 must be between 0.0 and 1.0') for easier debugging\"\n    },\n    {\n      \"description\": \"Add type validation for rubric structure\",\n      \"details\": \"Use TypedDict or Pydantic models to enforce rubric structure at runtime, catching configuration errors earlier\"\n    },\n    {\n      \"description\": \"Document scale format expectations\",\n      \"details\": \"Add docstring to DEFAULT_RUBRIC explaining expected scale format and how it's parsed by _extract_max_score\"\n    }\n  ]\n}\n```",
        "test_config": {
          "test_number": 8,
          "config": {
            "temperature": 0.6,
            "model": "openrouter/anthropic/claude-sonnet-4.5",
            "context_size": "file_only",
            "generation_strategy": "chain_of_thought"
          }
        },
        "workflow": {
          "started_at": "2025-10-26T06:25:12.720477Z",
          "completed_at": "2025-10-26T06:25:50.127674Z",
          "duration_seconds": 37.4067895830085
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 72,
              "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-based rubrics. Core logic is sound with retry mechanisms and caching. However, there are notable gaps: (1) Rubric validation is incomplete—doesn't verify non-empty dimensions or required keys, (2) The regex in _extract_max_score may fail on non-standard scale formats like '1 to 10' or 'Low-High', (3) No timeout on LLM calls could cause indefinite hangs. The score extraction and normalization logic appear correct for standard cases, but edge case handling is incomplete."
            },
            "error_handling": {
              "score": 68,
              "reasoning": "Good use of custom EvaluationError exceptions and input validation for temperature/top_p ranges. Retry logic with exponential backoff is well-implemented. However, significant gaps exist: (1) No size limit on workflow_output could enable DoS or excessive API costs, (2) Missing timeout configuration on LiteLLM calls, (3) Incomplete rubric validation (empty rubrics not caught), (4) Cache key validation happens after cache operations are determined, creating confusing flow. Error messages could be more contextual (e.g., including actual invalid values). Logging is present but could be more structured."
            },
            "code_quality": {
              "score": 70,
              "reasoning": "Code demonstrates good Python practices: proper type hints, PEP 8 naming, dataclass usage, and logical method organization. Separation of concerns is evident with helper methods. However, quality issues reduce the score: (1) The evaluate method has 11 parameters and high cyclomatic complexity—should use config dataclasses, (2) Multiple isinstance checks for Mapping vs object attributes complicate maintenance, (3) JSON markdown fence stripping is fragile, (4) Cache key computation happens even when caching is disabled. The code lacks Pydantic validation for rubric structure, relying instead on runtime checks. No leaky abstractions detected, but abstraction could be cleaner."
            },
            "performance": {
              "score": 75,
              "reasoning": "Caching mechanism effectively reduces redundant API calls, and async/await enables non-blocking I/O. No obvious algorithmic bottlenecks detected. However, performance concerns exist: (1) Cache key is computed unconditionally even when caching is disabled, (2) No timeout on LiteLLM calls could cause resource exhaustion, (3) No size limits on workflow_output could lead to excessive token usage and API costs, (4) Regex patterns appear reasonable but could be optimized. The retry mechanism with exponential backoff is appropriate. Overall, the design is sound but lacks safeguards against resource abuse."
            },
            "testability": {
              "score": 73,
              "reasoning": "Code has good testability foundations: dependency injection for logger and cache, well-isolated helper methods, and clear method contracts. Async design is testable with proper async test frameworks. However, testability is hindered by: (1) Random jitter in retry delays makes tests non-deterministic without seeding, (2) No injectable random number generator for testing, (3) Multiple isinstance checks and complex response handling make mocking harder, (4) Rubric validation logic is scattered, making edge cases harder to test in isolation. The evaluate method's complexity and parameter count make it harder to test all code paths. Overall, the code is testable but could be improved with better isolation and determinism."
            },
            "documentation": {
              "score": 74,
              "reasoning": "Code includes docstrings and type hints that are generally accurate and helpful. Variable and method names are clear and descriptive. However, documentation gaps exist: (1) No explanation of expected scale format in DEFAULT_RUBRIC or _extract_max_score, (2) Missing documentation of how normalization works (0-1 range), (3) No examples of rubric structure or expected LLM response format, (4) Complex logic in _extract_response_content and JSON parsing lacks inline comments, (5) No documentation of cache key format or caching behavior. Type hints are present and mostly accurate. The code would benefit from more inline comments explaining the regex patterns and response parsing logic, plus examples in docstrings."
            }
          },
          "temperature": 0.3,
          "cache_key": "37ca860d3a00231b1773712c9ae7ea2f147a7f8a306f063ab32d39e541e3679d",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/anthropic/claude-haiku-4.5"
        }
      },
      "timestamp": "2025-10-26T06:26:00.923100Z"
    }
  ],
  "status": "COMPLETED",
  "started_at": "2025-10-26T06:20:37.582821Z",
  "completed_at": "2025-10-26T06:26:00.926528Z",
  "experiment_metadata": {
    "config_hash": "c2607c5fa8007521fa28e004e8209f6c7757461b3fd6d443cbcc48fdf5654590",
    "created_at": "2025-10-26T06:20:37.582702Z",
    "dependencies": {
      "pydantic": "2.12.3",
      "langgraph": "1.0.1",
      "litellm": "1.79.0"
    },
    "non_deterministic_sources": [
      "llm_sampling"
    ]
  },
  "metadata": {
    "normalization": {
      "cost": {
        "min": 0.0,
        "max": 0.0
      },
      "latency": {
        "min": 15794.765084021492,
        "max": 44367.65304100118
      }
    }
  },
  "baseline_test_number": 1,
  "baseline_config": {
    "test_number": 1,
    "config_values": {
      "temperature": 0.3,
      "model": "openrouter/anthropic/claude-haiku-4.5",
      "context_size": "file_only",
      "generation_strategy": "standard"
    },
    "workflow": "code_review"
  },
  "baseline_result": {
    "test_number": 1,
    "config": {
      "test_number": 1,
      "config_values": {
        "temperature": 0.3,
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "context_size": "file_only",
        "generation_strategy": "standard"
      },
      "workflow": "code_review"
    },
    "quality_score": {
      "dimension_scores": {
        "functionality": {
          "score": 0.72,
          "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-based scoring. The async architecture, retry logic, and caching integration are well-designed. However, several edge cases are inadequately handled: (1) assertion-based type narrowing can fail silently with Python's -O flag, leaving unsafe states; (2) the `_extract_response_content()` method's multiple fallback patterns are fragile and could miss valid responses; (3) no validation that LLM-returned dimensions match the requested rubric, allowing typos to silently fail; (4) the score extraction regex allows leading zeros and imprecise numeric patterns. The normalization logic appears correct (0-1 range), and the default rubric is reasonable, but edge cases around malformed LLM responses and missing scores are not robustly handled."
        },
        "error_handling": {
          "score": 0.58,
          "reasoning": "Error handling has significant gaps despite using a custom EvaluationError. Critical issues: (1) bare `except Exception` on line 160 catches KeyboardInterrupt and SystemExit in Python <3.11, masking critical errors; (2) assertions for type narrowing can be disabled, leaving code unsafe; (3) `last_error` initialization is fragile—if `_max_retries < 1`, the assertion on line 165 could fail despite validation; (4) generic error messages don't include exception details in the message itself, only in the chain; (5) no specific exception hierarchy for different failure modes (LLM errors vs. cache errors vs. validation errors); (6) insufficient logging for production debugging. Input validation exists but lacks length limits on `extra_instructions`. The code handles LLM failures with retry logic but doesn't gracefully degrade or provide fallback behavior."
        },
        "code_quality": {
          "score": 0.65,
          "reasoning": "Code demonstrates reasonable structure with clear separation of concerns (LLM interaction, response parsing, caching), but has maintainability issues: (1) inappropriate use of assertions for type narrowing instead of explicit checks; (2) `_extract_response_content()` has repetitive, hard-to-follow fallback logic that should be extracted into a helper; (3) `extra_instructions.strip()` called twice inefficiently; (4) type hints missing on helper method return types (`_extract_response_content()`, `_load_json()`, `_parse_dimension_scores()`); (5) hardcoded system prompt and templates reduce flexibility; (6) `# pragma: no cover` on critical path suggests untestable code. Pydantic usage for validation is appropriate. Naming is generally clear, but the code violates DRY principle in response extraction logic."
        },
        "performance": {
          "score": 0.76,
          "reasoning": "Performance is generally solid with good caching strategy and minimal unnecessary LLM calls. Strengths: (1) async/await architecture enables concurrent evaluations; (2) caching integration reduces redundant LLM calls; (3) retry logic with exponential backoff is efficient; (4) regex patterns are reasonable. Concerns: (1) no documented cache backend concurrency requirements—potential race conditions if backend isn't thread-safe; (2) regex pattern `\\d+(?:\\.\\d+)?` allows leading zeros, adding unnecessary parsing overhead; (3) no length limits on `extra_instructions` could inflate token usage; (4) multiple `strip()` calls and repeated fallback logic in response extraction add minor overhead. Token usage appears reasonable given the evaluation task, but no explicit optimization for large rubrics."
        },
        "testability": {
          "score": 0.52,
          "reasoning": "Testability is significantly hampered by design choices: (1) `litellm.acompletion` is tightly coupled, making unit testing difficult without mocking the entire module; (2) broad exception handling on line 160 makes it hard to test specific failure modes; (3) `# pragma: no cover` comment on critical retry path indicates untestable code; (4) assertions for type narrowing make tests fragile; (5) no dependency injection for LLM calls or cache backend; (6) helper methods lack clear contracts and return type hints, making mocking difficult. Positive aspects: (1) clear input/output structure with Pydantic models; (2) pure functions like `_parse_dimension_scores()` are testable; (3) error cases are somewhat reproducible. The code would benefit from protocol-based interfaces or dependency injection to enable proper unit testing without mocking entire modules."
        },
        "documentation": {
          "score": 0.61,
          "reasoning": "Documentation is incomplete and inconsistent: (1) the `evaluate()` method lacks a comprehensive docstring explaining parameters, return types, caching behavior, and when `EvaluationError` is raised; (2) helper methods (`_extract_response_content()`, `_load_json()`, `_parse_dimension_scores()`) lack return type hints and docstrings; (3) no module-level docstring with usage examples; (4) inline comments are sparse for complex logic (response extraction, retry logic); (5) cache backend concurrency requirements are undocumented; (6) no examples showing error handling patterns or caching setup. Positive aspects: (1) class and method names are reasonably clear; (2) type hints exist for most parameters; (3) some inline comments explain non-obvious logic. Overall, documentation is insufficient for users to understand caching behavior, error handling, and configuration options."
        }
      },
      "overall_score": 0.64,
      "evaluator_model": "openrouter/anthropic/claude-haiku-4.5",
      "timestamp": "2025-10-26T06:21:13.000634Z",
      "metadata": {
        "rubric": {
          "functionality": {
            "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
            "scale": "0-100 points",
            "weight": 0.25
          },
          "error_handling": {
            "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
            "scale": "0-100 points",
            "weight": 0.2
          },
          "code_quality": {
            "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
            "scale": "0-100 points",
            "weight": 0.2
          },
          "performance": {
            "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
            "scale": "0-100 points",
            "weight": 0.15
          },
          "testability": {
            "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
            "scale": "0-100 points",
            "weight": 0.1
          },
          "documentation": {
            "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
            "scale": "0-100 points",
            "weight": 0.1
          }
        },
        "raw_response": {
          "functionality": {
            "score": 72,
            "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-based scoring. The async architecture, retry logic, and caching integration are well-designed. However, several edge cases are inadequately handled: (1) assertion-based type narrowing can fail silently with Python's -O flag, leaving unsafe states; (2) the `_extract_response_content()` method's multiple fallback patterns are fragile and could miss valid responses; (3) no validation that LLM-returned dimensions match the requested rubric, allowing typos to silently fail; (4) the score extraction regex allows leading zeros and imprecise numeric patterns. The normalization logic appears correct (0-1 range), and the default rubric is reasonable, but edge cases around malformed LLM responses and missing scores are not robustly handled."
          },
          "error_handling": {
            "score": 58,
            "reasoning": "Error handling has significant gaps despite using a custom EvaluationError. Critical issues: (1) bare `except Exception` on line 160 catches KeyboardInterrupt and SystemExit in Python <3.11, masking critical errors; (2) assertions for type narrowing can be disabled, leaving code unsafe; (3) `last_error` initialization is fragile—if `_max_retries < 1`, the assertion on line 165 could fail despite validation; (4) generic error messages don't include exception details in the message itself, only in the chain; (5) no specific exception hierarchy for different failure modes (LLM errors vs. cache errors vs. validation errors); (6) insufficient logging for production debugging. Input validation exists but lacks length limits on `extra_instructions`. The code handles LLM failures with retry logic but doesn't gracefully degrade or provide fallback behavior."
          },
          "code_quality": {
            "score": 65,
            "reasoning": "Code demonstrates reasonable structure with clear separation of concerns (LLM interaction, response parsing, caching), but has maintainability issues: (1) inappropriate use of assertions for type narrowing instead of explicit checks; (2) `_extract_response_content()` has repetitive, hard-to-follow fallback logic that should be extracted into a helper; (3) `extra_instructions.strip()` called twice inefficiently; (4) type hints missing on helper method return types (`_extract_response_content()`, `_load_json()`, `_parse_dimension_scores()`); (5) hardcoded system prompt and templates reduce flexibility; (6) `# pragma: no cover` on critical path suggests untestable code. Pydantic usage for validation is appropriate. Naming is generally clear, but the code violates DRY principle in response extraction logic."
          },
          "performance": {
            "score": 76,
            "reasoning": "Performance is generally solid with good caching strategy and minimal unnecessary LLM calls. Strengths: (1) async/await architecture enables concurrent evaluations; (2) caching integration reduces redundant LLM calls; (3) retry logic with exponential backoff is efficient; (4) regex patterns are reasonable. Concerns: (1) no documented cache backend concurrency requirements—potential race conditions if backend isn't thread-safe; (2) regex pattern `\\d+(?:\\.\\d+)?` allows leading zeros, adding unnecessary parsing overhead; (3) no length limits on `extra_instructions` could inflate token usage; (4) multiple `strip()` calls and repeated fallback logic in response extraction add minor overhead. Token usage appears reasonable given the evaluation task, but no explicit optimization for large rubrics."
          },
          "testability": {
            "score": 52,
            "reasoning": "Testability is significantly hampered by design choices: (1) `litellm.acompletion` is tightly coupled, making unit testing difficult without mocking the entire module; (2) broad exception handling on line 160 makes it hard to test specific failure modes; (3) `# pragma: no cover` comment on critical retry path indicates untestable code; (4) assertions for type narrowing make tests fragile; (5) no dependency injection for LLM calls or cache backend; (6) helper methods lack clear contracts and return type hints, making mocking difficult. Positive aspects: (1) clear input/output structure with Pydantic models; (2) pure functions like `_parse_dimension_scores()` are testable; (3) error cases are somewhat reproducible. The code would benefit from protocol-based interfaces or dependency injection to enable proper unit testing without mocking entire modules."
          },
          "documentation": {
            "score": 61,
            "reasoning": "Documentation is incomplete and inconsistent: (1) the `evaluate()` method lacks a comprehensive docstring explaining parameters, return types, caching behavior, and when `EvaluationError` is raised; (2) helper methods (`_extract_response_content()`, `_load_json()`, `_parse_dimension_scores()`) lack return type hints and docstrings; (3) no module-level docstring with usage examples; (4) inline comments are sparse for complex logic (response extraction, retry logic); (5) cache backend concurrency requirements are undocumented; (6) no examples showing error handling patterns or caching setup. Positive aspects: (1) class and method names are reasonably clear; (2) type hints exist for most parameters; (3) some inline comments explain non-obvious logic. Overall, documentation is insufficient for users to understand caching behavior, error handling, and configuration options."
          }
        },
        "temperature": 0.3,
        "cache_key": "2228e54f7801011c3e457f7c954b3285a6a41045ddea728a4caf83018b90d903",
        "cache_hit": false,
        "cache_recorded": true,
        "model": "openrouter/anthropic/claude-haiku-4.5"
      }
    },
    "cost": 0.0,
    "latency": 20456.82512500207,
    "utility": 0.64,
    "workflow_output": "Well-structured async LLM evaluator with solid error handling and validation. Code is generally clean and maintainable, but has some issues around type safety, error handling edge cases, and testability. The retry logic and caching integration are well-designed. Main concerns are assertion usage for type narrowing and potential race conditions in concurrent scenarios.\nIssues:\n- (medium) Using `assert` statements for type narrowing (lines 147, 165, 181) is problematic. Assertions can be disabled with Python's `-O` flag, leaving code in an unsafe state. These should be explicit type checks or guards. Suggested fix: Replace `assert content is not None` with explicit type checking or restructure control flow to guarantee non-None values. Example: `if content is None: raise EvaluationError('Content extraction failed')` or use `typing.cast()` with a comment explaining why it's safe.\n- (medium) Bare `except Exception` on line 160 catches all exceptions including KeyboardInterrupt and SystemExit (in Python <3.11). This can mask critical errors and make debugging difficult. [line 160] Suggested fix: Catch specific exceptions from litellm (e.g., `litellm.APIError`, `litellm.RateLimitError`) or at minimum use `except Exception as exc` with explicit handling. Consider using `asyncio.CancelledError` separately if needed.\n- (medium) On line 165, `assert last_error is not None` assumes an error occurred, but the loop could theoretically exit without setting `last_error` if `_max_retries < 1` (though validated in `__init__`). The assertion is fragile. [line 165] Suggested fix: Initialize `last_error` with a default exception or restructure to guarantee it's set. Example: `last_error = last_error or EvaluationError('Unknown error')`\n- (medium) The `_cache` backend is shared across concurrent `evaluate()` calls. If the cache backend is not thread-safe or doesn't handle concurrent access, race conditions could occur during `cache_backend.get()` and `cache_backend.set()` operations. Suggested fix: Document cache backend concurrency requirements. Consider adding a lock or using a thread-safe cache implementation. Add a note in docstring: 'Cache backend must be thread-safe for concurrent evaluations.'\n- (low) The `extra_instructions` parameter is only checked for emptiness after stripping (line 127), but no length limit is enforced. Very long instructions could inflate token usage and costs. [line 127] Suggested fix: Add a maximum length check for `extra_instructions`, e.g., `if len(extra_instructions) > 5000: raise ValueError('Instructions too long')`\n- (low) Generic error message on line 168 doesn't include the actual exception details in the message itself (only in the chain). Users reading logs might miss the root cause. [line 168] Suggested fix: Include exception type/message in the error string: `f'LLM evaluation failed after {self._max_retries} attempts: {type(last_error).__name__}: {last_error}'`\n- (low) The `_extract_response_content()` method handles both Mapping and object attribute access patterns (lines 186-204), but the logic is repetitive and hard to follow. Multiple fallback patterns make it fragile. Suggested fix: Extract a helper method like `_get_nested_attr(obj, key)` that handles both Mapping and attribute access uniformly. This reduces duplication and improves maintainability.\n- (low) The regex pattern on line 222 uses `\\d+(?:\\.\\d+)?` which allows leading zeros (e.g., '007'). While not breaking, it's imprecise for numeric parsing. [line 222] Suggested fix: Consider using a more precise pattern or just rely on `float()` parsing: `match = re.search(r'([\\d.]+)\\s*-\\s*([\\d.]+)', scale)` and let float() handle validation.\n- (medium) The `_request_with_retry()` method has a `# pragma: no cover` comment (line 161), but this is a critical path that should be testable. The broad exception handling makes it hard to unit test specific failure modes. [line 161] Suggested fix: Make the exception handling more specific and testable. Create a separate internal method that wraps litellm calls, allowing you to mock/test retry logic independently. Remove pragma comment and add proper tests.\n- (low) On line 127, `extra_instructions.strip()` is called twice if the condition is true (once in the condition, once in the f-string). Minor inefficiency. [line 127] Suggested fix: Store the stripped value: `stripped_instructions = extra_instructions.strip() if extra_instructions else None; instructions = f'\\n{stripped_instructions}' if stripped_instructions else ''`\n- (low) The `evaluate()` method has many parameters but lacks detailed docstring explaining each one, especially the caching parameters and their interactions. [line 77] Suggested fix: Add comprehensive docstring with parameter descriptions, return type details, and examples. Document the caching behavior and when `EvaluationError` is raised.\nSuggestions:\n- {'category': 'Type Safety', 'description': 'Add type hints to return types of helper methods like `_extract_response_content()`, `_load_json()`, and `_parse_dimension_scores()`. This improves IDE support and catches errors early.', 'priority': 'medium'}\n- {'category': 'Error Handling', 'description': 'Create a custom exception hierarchy (e.g., `LLMResponseError`, `CacheError`, `ValidationError` extending `EvaluationError`) to allow callers to handle different failure modes specifically.', 'priority': 'medium'}\n- {'category': 'Testability', 'description': 'Inject the litellm.acompletion call as a dependency or use a protocol/interface to make the class testable without mocking the entire litellm module. This enables better unit testing of retry logic.', 'priority': 'medium'}\n- {'category': 'Logging', 'description': 'Add more structured logging with context (e.g., workflow_output hash, rubric dimensions) to help debug issues in production. Consider using `logging.LogRecord` attributes.', 'priority': 'low'}\n- {'category': 'Configuration', 'description': 'Consider making the system prompt and prompt template configurable rather than hardcoded. This allows users to customize evaluation behavior without subclassing.', 'priority': 'low'}\n- {'category': 'Validation', 'description': 'Add validation that the rubric dimensions in the response match the requested rubric (case-insensitive). Currently, typos in LLM responses silently fail.', 'priority': 'medium'}\n- {'category': 'Documentation', 'description': 'Add examples in module docstring showing basic usage, caching setup, and error handling patterns.', 'priority': 'low'}",
    "metadata": {
      "strategy": "standard",
      "model": "openrouter/anthropic/claude-haiku-4.5",
      "temperature": 0.3,
      "context_size": "file_only",
      "language": "python",
      "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
      "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Review limited to the provided file.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
      "analysis_raw": "```json\n{\n  \"summary\": \"Well-structured async LLM evaluator with solid error handling and validation. Code is generally clean and maintainable, but has some issues around type safety, error handling edge cases, and testability. The retry logic and caching integration are well-designed. Main concerns are assertion usage for type narrowing and potential race conditions in concurrent scenarios.\",\n  \"issues\": [\n    {\n      \"type\": \"Type Safety\",\n      \"severity\": \"medium\",\n      \"description\": \"Using `assert` statements for type narrowing (lines 147, 165, 181) is problematic. Assertions can be disabled with Python's `-O` flag, leaving code in an unsafe state. These should be explicit type checks or guards.\",\n      \"line_number\": [147, 165, 181],\n      \"suggestion\": \"Replace `assert content is not None` with explicit type checking or restructure control flow to guarantee non-None values. Example: `if content is None: raise EvaluationError('Content extraction failed')` or use `typing.cast()` with a comment explaining why it's safe.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"Bare `except Exception` on line 160 catches all exceptions including KeyboardInterrupt and SystemExit (in Python <3.11). This can mask critical errors and make debugging difficult.\",\n      \"line_number\": 160,\n      \"suggestion\": \"Catch specific exceptions from litellm (e.g., `litellm.APIError`, `litellm.RateLimitError`) or at minimum use `except Exception as exc` with explicit handling. Consider using `asyncio.CancelledError` separately if needed.\"\n    },\n    {\n      \"type\": \"Logic Error\",\n      \"severity\": \"medium\",\n      \"description\": \"On line 165, `assert last_error is not None` assumes an error occurred, but the loop could theoretically exit without setting `last_error` if `_max_retries < 1` (though validated in `__init__`). The assertion is fragile.\",\n      \"line_number\": 165,\n      \"suggestion\": \"Initialize `last_error` with a default exception or restructure to guarantee it's set. Example: `last_error = last_error or EvaluationError('Unknown error')`\"\n    },\n    {\n      \"type\": \"Concurrency Issue\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_cache` backend is shared across concurrent `evaluate()` calls. If the cache backend is not thread-safe or doesn't handle concurrent access, race conditions could occur during `cache_backend.get()` and `cache_backend.set()` operations.\",\n      \"line_number\": [139, 177],\n      \"suggestion\": \"Document cache backend concurrency requirements. Consider adding a lock or using a thread-safe cache implementation. Add a note in docstring: 'Cache backend must be thread-safe for concurrent evaluations.'\"\n    },\n    {\n      \"type\": \"Input Validation\",\n      \"severity\": \"low\",\n      \"description\": \"The `extra_instructions` parameter is only checked for emptiness after stripping (line 127), but no length limit is enforced. Very long instructions could inflate token usage and costs.\",\n      \"line_number\": 127,\n      \"suggestion\": \"Add a maximum length check for `extra_instructions`, e.g., `if len(extra_instructions) > 5000: raise ValueError('Instructions too long')`\"\n    },\n    {\n      \"type\": \"Error Message Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Generic error message on line 168 doesn't include the actual exception details in the message itself (only in the chain). Users reading logs might miss the root cause.\",\n      \"line_number\": 168,\n      \"suggestion\": \"Include exception type/message in the error string: `f'LLM evaluation failed after {self._max_retries} attempts: {type(last_error).__name__}: {last_error}'`\"\n    },\n    {\n      \"type\": \"Code Clarity\",\n      \"severity\": \"low\",\n      \"description\": \"The `_extract_response_content()` method handles both Mapping and object attribute access patterns (lines 186-204), but the logic is repetitive and hard to follow. Multiple fallback patterns make it fragile.\",\n      \"line_number\": [186, 204],\n      \"suggestion\": \"Extract a helper method like `_get_nested_attr(obj, key)` that handles both Mapping and attribute access uniformly. This reduces duplication and improves maintainability.\"\n    },\n    {\n      \"type\": \"Regex Pattern\",\n      \"severity\": \"low\",\n      \"description\": \"The regex pattern on line 222 uses `\\\\d+(?:\\\\.\\\\d+)?` which allows leading zeros (e.g., '007'). While not breaking, it's imprecise for numeric parsing.\",\n      \"line_number\": 222,\n      \"suggestion\": \"Consider using a more precise pattern or just rely on `float()` parsing: `match = re.search(r'([\\\\d.]+)\\\\s*-\\\\s*([\\\\d.]+)', scale)` and let float() handle validation.\"\n    },\n    {\n      \"type\": \"Testing Concern\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_request_with_retry()` method has a `# pragma: no cover` comment (line 161), but this is a critical path that should be testable. The broad exception handling makes it hard to unit test specific failure modes.\",\n      \"line_number\": 161,\n      \"suggestion\": \"Make the exception handling more specific and testable. Create a separate internal method that wraps litellm calls, allowing you to mock/test retry logic independently. Remove pragma comment and add proper tests.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"On line 127, `extra_instructions.strip()` is called twice if the condition is true (once in the condition, once in the f-string). Minor inefficiency.\",\n      \"line_number\": 127,\n      \"suggestion\": \"Store the stripped value: `stripped_instructions = extra_instructions.strip() if extra_instructions else None; instructions = f'\\\\n{stripped_instructions}' if stripped_instructions else ''`\"\n    },\n    {\n      \"type\": \"Documentation\",\n      \"severity\": \"low\",\n      \"description\": \"The `evaluate()` method has many parameters but lacks detailed docstring explaining each one, especially the caching parameters and their interactions.\",\n      \"line_number\": 77,\n      \"suggestion\": \"Add comprehensive docstring with parameter descriptions, return type details, and examples. Document the caching behavior and when `EvaluationError` is raised.\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"category\": \"Type Safety\",\n      \"description\": \"Add type hints to return types of helper methods like `_extract_response_content()`, `_load_json()`, and `_parse_dimension_scores()`. This improves IDE support and catches errors early.\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"category\": \"Error Handling\",\n      \"description\": \"Create a custom exception hierarchy (e.g., `LLMResponseError`, `CacheError`, `ValidationError` extending `EvaluationError`) to allow callers to handle different failure modes specifically.\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"category\": \"Testability\",\n      \"description\": \"Inject the litellm.acompletion call as a dependency or use a protocol/interface to make the class testable without mocking the entire litellm module. This enables better unit testing of retry logic.\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"category\": \"Logging\",\n      \"description\": \"Add more structured logging with context (e.g., workflow_output hash, rubric dimensions) to help debug issues in production. Consider using `logging.LogRecord` attributes.\",\n      \"priority\": \"low\"\n    },\n    {\n      \"category\": \"Configuration\",\n      \"description\": \"Consider making the system prompt and prompt template configurable rather than hardcoded. This allows users to customize evaluation behavior without subclassing.\",\n      \"priority\": \"low\"\n    },\n    {\n      \"category\": \"Validation\",\n      \"description\": \"Add validation that the rubric dimensions in the response match the requested rubric (case-insensitive). Currently, typos in LLM responses silently fail.\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"category\": \"Documentation\",\n      \"description\": \"Add examples in module docstring showing basic usage, caching setup, and error handling patterns.\",\n      \"priority\": \"low\"\n    }\n  ]\n}\n```",
      "test_config": {
        "test_number": 1,
        "config": {
          "temperature": 0.3,
          "model": "openrouter/anthropic/claude-haiku-4.5",
          "context_size": "file_only",
          "generation_strategy": "standard"
        }
      },
      "workflow": {
        "started_at": "2025-10-26T06:20:37.589525Z",
        "completed_at": "2025-10-26T06:20:58.046570Z",
        "duration_seconds": 20.45682512500207
      },
      "evaluation": {
        "rubric": {
          "functionality": {
            "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
            "scale": "0-100 points",
            "weight": 0.25
          },
          "error_handling": {
            "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
            "scale": "0-100 points",
            "weight": 0.2
          },
          "code_quality": {
            "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
            "scale": "0-100 points",
            "weight": 0.2
          },
          "performance": {
            "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
            "scale": "0-100 points",
            "weight": 0.15
          },
          "testability": {
            "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
            "scale": "0-100 points",
            "weight": 0.1
          },
          "documentation": {
            "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
            "scale": "0-100 points",
            "weight": 0.1
          }
        },
        "raw_response": {
          "functionality": {
            "score": 72,
            "reasoning": "The code accomplishes its core purpose of evaluating workflow outputs using LLM-based scoring. The async architecture, retry logic, and caching integration are well-designed. However, several edge cases are inadequately handled: (1) assertion-based type narrowing can fail silently with Python's -O flag, leaving unsafe states; (2) the `_extract_response_content()` method's multiple fallback patterns are fragile and could miss valid responses; (3) no validation that LLM-returned dimensions match the requested rubric, allowing typos to silently fail; (4) the score extraction regex allows leading zeros and imprecise numeric patterns. The normalization logic appears correct (0-1 range), and the default rubric is reasonable, but edge cases around malformed LLM responses and missing scores are not robustly handled."
          },
          "error_handling": {
            "score": 58,
            "reasoning": "Error handling has significant gaps despite using a custom EvaluationError. Critical issues: (1) bare `except Exception` on line 160 catches KeyboardInterrupt and SystemExit in Python <3.11, masking critical errors; (2) assertions for type narrowing can be disabled, leaving code unsafe; (3) `last_error` initialization is fragile—if `_max_retries < 1`, the assertion on line 165 could fail despite validation; (4) generic error messages don't include exception details in the message itself, only in the chain; (5) no specific exception hierarchy for different failure modes (LLM errors vs. cache errors vs. validation errors); (6) insufficient logging for production debugging. Input validation exists but lacks length limits on `extra_instructions`. The code handles LLM failures with retry logic but doesn't gracefully degrade or provide fallback behavior."
          },
          "code_quality": {
            "score": 65,
            "reasoning": "Code demonstrates reasonable structure with clear separation of concerns (LLM interaction, response parsing, caching), but has maintainability issues: (1) inappropriate use of assertions for type narrowing instead of explicit checks; (2) `_extract_response_content()` has repetitive, hard-to-follow fallback logic that should be extracted into a helper; (3) `extra_instructions.strip()` called twice inefficiently; (4) type hints missing on helper method return types (`_extract_response_content()`, `_load_json()`, `_parse_dimension_scores()`); (5) hardcoded system prompt and templates reduce flexibility; (6) `# pragma: no cover` on critical path suggests untestable code. Pydantic usage for validation is appropriate. Naming is generally clear, but the code violates DRY principle in response extraction logic."
          },
          "performance": {
            "score": 76,
            "reasoning": "Performance is generally solid with good caching strategy and minimal unnecessary LLM calls. Strengths: (1) async/await architecture enables concurrent evaluations; (2) caching integration reduces redundant LLM calls; (3) retry logic with exponential backoff is efficient; (4) regex patterns are reasonable. Concerns: (1) no documented cache backend concurrency requirements—potential race conditions if backend isn't thread-safe; (2) regex pattern `\\d+(?:\\.\\d+)?` allows leading zeros, adding unnecessary parsing overhead; (3) no length limits on `extra_instructions` could inflate token usage; (4) multiple `strip()` calls and repeated fallback logic in response extraction add minor overhead. Token usage appears reasonable given the evaluation task, but no explicit optimization for large rubrics."
          },
          "testability": {
            "score": 52,
            "reasoning": "Testability is significantly hampered by design choices: (1) `litellm.acompletion` is tightly coupled, making unit testing difficult without mocking the entire module; (2) broad exception handling on line 160 makes it hard to test specific failure modes; (3) `# pragma: no cover` comment on critical retry path indicates untestable code; (4) assertions for type narrowing make tests fragile; (5) no dependency injection for LLM calls or cache backend; (6) helper methods lack clear contracts and return type hints, making mocking difficult. Positive aspects: (1) clear input/output structure with Pydantic models; (2) pure functions like `_parse_dimension_scores()` are testable; (3) error cases are somewhat reproducible. The code would benefit from protocol-based interfaces or dependency injection to enable proper unit testing without mocking entire modules."
          },
          "documentation": {
            "score": 61,
            "reasoning": "Documentation is incomplete and inconsistent: (1) the `evaluate()` method lacks a comprehensive docstring explaining parameters, return types, caching behavior, and when `EvaluationError` is raised; (2) helper methods (`_extract_response_content()`, `_load_json()`, `_parse_dimension_scores()`) lack return type hints and docstrings; (3) no module-level docstring with usage examples; (4) inline comments are sparse for complex logic (response extraction, retry logic); (5) cache backend concurrency requirements are undocumented; (6) no examples showing error handling patterns or caching setup. Positive aspects: (1) class and method names are reasonably clear; (2) type hints exist for most parameters; (3) some inline comments explain non-obvious logic. Overall, documentation is insufficient for users to understand caching behavior, error handling, and configuration options."
          }
        },
        "temperature": 0.3,
        "cache_key": "2228e54f7801011c3e457f7c954b3285a6a41045ddea728a4caf83018b90d903",
        "cache_hit": false,
        "cache_recorded": true,
        "model": "openrouter/anthropic/claude-haiku-4.5"
      }
    },
    "timestamp": "2025-10-26T06:21:13.000882Z"
  },
  "baseline_quality": 0.64,
  "quality_improvement_pct": 16.927083333333343
}