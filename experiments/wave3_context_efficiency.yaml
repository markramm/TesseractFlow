name: "wave3_context_efficiency"
workflow: "context_efficiency"
description: "Test DeepSeek R1's 64K context window efficiency across different content types and retrieval strategies"

variables:
  - name: "context_size"
    level_1: "small"          # 4K tokens
    level_2: "large"          # 32K tokens

  - name: "content_type"
    level_1: "narrative"      # Story text
    level_2: "technical"      # Code/docs

  - name: "retrieval_strategy"
    level_1: "full_context"   # All context provided
    level_2: "selective"      # Key excerpts only

  - name: "task_complexity"
    level_1: "simple"         # Single fact lookup
    level_2: "complex"        # Multi-step synthesis

  - name: "temperature"
    level_1: 0.2              # Precise retrieval
    level_2: 0.6              # Creative synthesis

  - name: "position_bias_test"
    level_1: "beginning"      # Info at start
    level_2: "end"            # Info at end

  - name: "compression_method"
    level_1: "none"           # Full verbatim text
    level_2: "summarized"     # Pre-compressed context

utility_weights:
  quality: 1.0
  cost: 0.15
  time: 0.05

workflow_config:
  evaluator_model: "openrouter/anthropic/claude-haiku-4.5"
  model: "openrouter/deepseek/deepseek-r1"

  rubric:
    retrieval_accuracy:
      description: "Correctly identifies and retrieves relevant information from context"
      weight: 0.30
      scale: "0-100 points"

    context_utilization:
      description: "Effectively uses available context without hallucination or omission"
      weight: 0.25
      scale: "0-100 points"

    answer_completeness:
      description: "Provides complete answer synthesizing all relevant context"
      weight: 0.20
      scale: "0-100 points"

    efficiency:
      description: "Handles large context efficiently without degradation"
      weight: 0.15
      scale: "0-100 points"

    position_independence:
      description: "Retrieves information regardless of position in context"
      weight: 0.10
      scale: "0-100 points"
