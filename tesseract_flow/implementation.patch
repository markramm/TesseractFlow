diff --git a/.gitignore b/.gitignore
index 478a16d6fbb1b70a7a88b022eb474a63439ba75c..d76bf45e404cb4af04ea430ee2fb52e86abdc17d 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,78 +1,41 @@
 # Python
 __pycache__/
 *.py[cod]
 *$py.class
-*.so
-.Python
+
+# Virtual environments
+.venv/
+.env/
+
+# Distribution / packaging
 build/
-develop-eggs/
 dist/
-downloads/
-eggs/
-.eggs/
-lib/
-lib64/
-parts/
-sdist/
-var/
-wheels/
 *.egg-info/
-.installed.cfg
-*.egg
-
-# Virtual environments
-venv/
-ENV/
-env/
-.venv
+.eggs/
 
-# Testing
+# Pytest
 .pytest_cache/
-.coverage
 htmlcov/
-.tox/
-.hypothesis/
+.coverage
+.coverage.*
 
 # IDEs
 .vscode/
 .idea/
 *.swp
-*.swo
-*~
-.DS_Store
 
-# Agent folders (may contain credentials)
-.claude/
-
-# Environment variables
-.env
-.env.local
-.env.*.local
-
-# Database
-*.db
-*.sqlite
-*.sqlite3
+# MyPy
+.mypy_cache/
+.dmypy.json
 
 # Logs
 *.log
 
-# Experiments and results
-results/
-experiments/results/
-*.pareto.svg
-
-# Jupyter
-.ipynb_checkpoints/
-*.ipynb
-
-# MyPy
-.mypy_cache/
-.dmypy.json
-dmypy.json
+# Local environment files
+.env.local
 
-# Ruff
-.ruff_cache/
+# Coverage reports
+coverage.xml
 
-# Distribution
-*.whl
+# Cache
+.cache/
diff --git a/README.md b/README.md
index 169247f6f577af1bfff7eeea8f85184a1fd7c451..178b1f3e6a783b178073cce1a9105571129f1899 100644
--- a/README.md
+++ b/README.md
@@ -1,447 +1,129 @@
 # TesseractFlow
 
-**Multi-dimensional LLM workflow optimization using Taguchi Design of Experiments**
+**Multi-dimensional LLM workflow optimization powered by Taguchi Design of Experiments**
 
 [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
 [![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
 [![Status: Alpha](https://img.shields.io/badge/status-alpha-orange.svg)]()
 
 ---
 
 ## Overview
 
-TesseractFlow is a **structured alternative to LLM fine-tuning** for performance optimization. Instead of expensive, brittle fine-tuning that becomes obsolete as models evolve, TesseractFlow helps you systematically optimize workflows through prompt engineering, context design, model selection, and generation strategies.
+TesseractFlow helps teams systematically improve LLM workflows without resorting to expensive model fine-tuning. By combining Taguchi orthogonal arrays, rubric-based evaluation, and multi-objective optimization, you can iterate on prompts, context windows, models, and strategies in hours instead of weeks.
 
-### Why TesseractFlow?
+### Key capabilities
 
-**The Problem with Fine-Tuning:**
-- Expensive ($1000s per training run)
-- Brittle (breaks with model updates)
-- Slow (days to weeks)
-- Vendor lock-in (tied to specific model versions)
-
-**The TesseractFlow Approach:**
-- Optimize prompts, context, and strategies (not model weights)
-- Works with any LLM provider (OpenRouter, Anthropic, OpenAI, etc.)
-- Results in hours, not weeks
-- Continuous optimization as models evolve
-- Data-driven decisions via Taguchi DOE
-
-### Core Capabilities
-
-1. **Efficient Experimentation** - Test 4-7 variables with just 8 experiments (Taguchi L8 array)
-2. **Multi-Objective Optimization** - Balance Quality, Cost, and Latency simultaneously
-3. **Pareto Frontier Analysis** - Visualize trade-offs between competing objectives
-4. **Generation Strategy Testing** - Compare standard prompting, Chain-of-Thought, Verbalized Sampling, etc.
-5. **Pluggable Architecture** - Works with LangGraph, custom workflows, any LLM provider
+- **Efficient experimentation** â€“ Generate eight Taguchi L8 trials that cover four to seven variables.
+- **Quality evaluation** â€“ Score results using rubric-driven LiteLLM calls with caching and retries.
+- **Utility optimization** â€“ Balance quality, cost, and latency using configurable weights.
+- **Analysis & reporting** â€“ Compute main effects, compare configurations, and export YAML recommendations.
+- **Visualization** â€“ Render Pareto frontiers highlighting non-dominated trade-offs.
+- **Rich CLI** â€“ Run, resume, validate, analyze, and visualize experiments from the terminal.
 
 ---
 
-## Project Status
-
-**Current:** MVP Specification Complete âœ…
-**Branch:** `001-mvp-optimizer`
-**Next:** Implementation (60-70 hours estimated)
-
-See [MVP Specification](specs/001-mvp-optimizer/) for complete details.
-
----
+## Installation
 
-## Quick Start
+The project targets Python 3.11+. Until published on PyPI, install from source:
 
-> **Note:** TesseractFlow is currently in active development. Installation instructions below reflect the planned MVP. For current status, see [Implementation Tasks](specs/001-mvp-optimizer/tasks.md).
-
-### Installation (Planned)
-
-```bash
-pip install tesseract-flow
-```
-
-**With optional strategies:**
 ```bash
-pip install tesseract-flow[verbalized-sampling]  # Adds VS support
-pip install tesseract-flow[all]                  # All optional strategies
-```
-
-### Basic Usage
-
-**1. Define your workflow:**
-
-```python
-from tesseract_flow import BaseWorkflowService
-from pydantic import BaseModel
-
-class CodeReviewInput(BaseModel):
-    code: str
-    language: str
-
-class CodeReviewOutput(BaseModel):
-    issues: List[Dict[str, str]]
-    suggestions: List[str]
-
-class CodeReviewWorkflow(BaseWorkflowService[CodeReviewInput, CodeReviewOutput]):
-    def _build_workflow(self) -> StateGraph:
-        # Define workflow logic using LangGraph
-        graph = StateGraph()
-        graph.add_node("analyze", self._analyze_code)
-        graph.add_node("suggest", self._generate_suggestions)
-        graph.add_edge("analyze", "suggest")
-        return graph.compile()
+$ git clone https://github.com/markramm/TesseractFlow.git
+$ cd TesseractFlow
+$ python -m venv .venv
+$ source .venv/bin/activate
+(.venv) $ pip install -e .
 ```
 
-**2. Configure experiment:**
-
-```yaml
-# experiments/optimize_code_review.yaml
-name: "code_review_optimization"
-workflow: "code_review"
+This installs the `tesseract` CLI along with required dependencies (Typer, Rich, LiteLLM, LangGraph, matplotlib, NumPy, PyYAML, etc.).
 
-method:
-  type: "taguchi_l8"
-
-variables:
-  temperature: {1: 0.3, 2: 0.7}
-  model: {1: "deepseek/deepseek-coder-v2", 2: "anthropic/claude-3.5-sonnet"}
-  context_size: {1: "file_only", 2: "full_module"}
-  generation_strategy: {1: "standard", 2: "verbalized_sampling"}
-
-utility_weights:
-  quality: 0.6
-  cost: 0.3
-  time: 0.1
-```
-
-**3. Run experiment:**
-
-```bash
-tesseract experiment run experiments/optimize_code_review.yaml
-```
-
-**4. Analyze results:**
-
-```
-Main Effects Analysis:
-  context_size: 45% contribution (full_module >> file_only)
-  model: 22% contribution (claude > deepseek)
-  temperature: 18% contribution (0.7 > 0.3)
-  generation_strategy: 15% contribution (verbalized_sampling > standard)
-
-Optimal Configuration:
-  temperature: 0.7
-  model: anthropic/claude-3.5-sonnet
-  context_size: full_module
-  generation_strategy: verbalized_sampling
-
-Pareto Frontier (Quality vs Cost):
-  Config 8: Quality 0.88, Cost $0.25 â­ (Highest quality)
-  Config 4: Quality 0.82, Cost $0.12 â­ (Best balance)
-  Config 1: Quality 0.72, Cost $0.02 â­ (Lowest cost)
-```
+Set the appropriate provider API keys before running experiments, e.g. `export OPENROUTER_API_KEY=...`.
 
 ---
 
-## Key Concepts
-
-### Taguchi Design of Experiments
-
-Traditional optimization requires testing every combination of variables (full factorial):
-- 4 variables Ã— 2 levels = 2^4 = **16 experiments**
-
-TesseractFlow uses Taguchi orthogonal arrays:
-- 4 variables Ã— 2 levels = **8 experiments** (L8 array)
-- **50% reduction** in experiments
-- Still identifies which variables matter most (main effects)
-
-### Multi-Objective Optimization
-
-Most LLM optimization focuses on quality alone. TesseractFlow optimizes:
-- **Quality** - Task-specific metrics (accuracy, coherence, etc.)
-- **Cost** - API costs in USD
-- **Time** - Latency in milliseconds
-
-**Utility Function:**
-```
-utility = (w_quality Ã— quality) - (w_cost Ã— normalized_cost) - (w_time Ã— normalized_time)
-```
-
-**Pareto Frontier:**
-Visualize trade-offs between quality and cost. Choose configurations based on your budget and requirements.
-
-### Generation Strategies as Variables
-
-TesseractFlow treats prompting techniques as **experimental variables to test**, not assumptions:
-
-- **Standard Prompting** - Direct LLM call
-- **Chain-of-Thought** - "Let's think step by step..."
-- **Few-Shot** - Examples before the task
-- **Verbalized Sampling** - Sample from probability distribution (optional)
-
-Test which strategy actually improves your task:
-```yaml
-variables:
-  generation_strategy:
-    1: "standard"
-    2: "verbalized_sampling"
-```
-
-Main effects analysis tells you if the strategy contributes to quality improvement.
-
-### Workflows as Boundaries (HITL)
-
-Human-in-the-loop (HITL) occurs **between workflows**, not within them:
-
-```
-Workflow 1: Generate Draft  â†’  Database (Approval Queue)  â†’  Workflow 2: Apply Feedback
-  (10-60 seconds)              (Human reviews async)           (10-60 seconds)
-```
-
-**Benefits:**
-- No complex orchestration (Temporal, checkpointing)
-- Simple synchronous workflows
-- Human reviews at their own pace
-- Works with any workflow framework
+## Quick start
+
+1. **Preview the configuration**
+   ```bash
+   tesseract experiment run examples/code_review/experiment_config.yaml --dry-run
+   ```
+2. **Execute the Taguchi experiment**
+   ```bash
+   tesseract experiment run examples/code_review/experiment_config.yaml \
+     --output results.json --record-cache --verbose
+   ```
+3. **Analyze main effects and export recommendations**
+   ```bash
+   tesseract experiment analyze results.json --format table --export optimal.yaml
+   ```
+4. **Visualize the Pareto frontier**
+   ```bash
+   tesseract visualize pareto results.json --output pareto.png --budget 0.010
+   ```
+
+Refer to [docs/user-guide/quickstart.md](docs/user-guide/quickstart.md) for a detailed walkthrough.
 
 ---
 
-## Architecture
-
-### Technology Stack
-
-- **Python 3.11+** - Core framework
-- **LangGraph** - Workflow orchestration
-- **LiteLLM** - Universal LLM provider gateway
-- **FastAPI** - REST API for workflows and approvals
-- **PostgreSQL** - Results, experiments, approval queue
-- **Langfuse** - Observability and experiment tracking
-
-### System Components
-
-```
-â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-â”‚                    CLI / API Interface                       â”‚
-â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-                            â”‚
-â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-â”‚                  Experiment Orchestrator                     â”‚
-â”‚  - Taguchi array generation (L8, L16, L18)                  â”‚
-â”‚  - Test execution across configurations                      â”‚
-â”‚  - Main effects analysis                                     â”‚
-â”‚  - Pareto frontier computation                               â”‚
-â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-                            â”‚
-â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-â”‚                   Workflow Executor                          â”‚
-â”‚  - BaseWorkflowService[TInput, TOutput]                     â”‚
-â”‚  - Generation strategy selection                            â”‚
-â”‚  - LangGraph integration                                     â”‚
-â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-          â”‚                                   â”‚
-â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-â”‚  Generation Strategies â”‚       â”‚   Quality Evaluator      â”‚
-â”‚  - Standard            â”‚       â”‚   - Rubric-based         â”‚
-â”‚  - Chain-of-Thought    â”‚       â”‚   - Pairwise A/B         â”‚
-â”‚  - Few-Shot            â”‚       â”‚   - Ensemble judges      â”‚
-â”‚  - Verbalized Sampling â”‚       â”‚   - Custom evaluators    â”‚
-â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-          â”‚                                   â”‚
-â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-â”‚              LLM Provider Gateway (LiteLLM)                  â”‚
-â”‚  OpenRouter â€¢ Anthropic â€¢ OpenAI â€¢ DeepSeek â€¢ Azure         â”‚
-â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-```
-
----
-
-## Comparison to Alternatives
-
-### vs Fine-Tuning
-
-| Fine-Tuning | TesseractFlow |
-|-------------|---------------|
-| $1000s per run | $10s for experiments |
-| Days to weeks | Hours |
-| Brittle (model version locked) | Works with any model |
-| Vendor lock-in | Provider agnostic |
-| Opaque (hard to debug) | Transparent (see what works) |
-
-### vs DSPy
-
-| DSPy | TesseractFlow |
-|------|---------------|
-| Auto-optimizes prompts | User tests variables explicitly |
-| Must use DSPy pipeline | Works with any workflow |
-| Opaque optimization | Transparent main effects |
-| Grid search (slow) | Taguchi (efficient) |
-
-### vs LangSmith
-
-| LangSmith | TesseractFlow |
-|-----------|---------------|
-| Manual A/B testing | Systematic DOE |
-| No experiment design | Taguchi arrays |
-| Observability focus | Optimization focus |
-| Run A vs B, compare | 8 tests, 4 variables |
+## Documentation
 
-**TesseractFlow integrates with LangSmith/Langfuse for observability.**
+- [Quickstart](docs/user-guide/quickstart.md)
+- [Configuration Reference](docs/user-guide/configuration.md)
+- [Interpreting Results](docs/user-guide/interpreting-results.md)
+- [API Overview](docs/api/core-modules.md)
+- [Development Setup](docs/development/setup.md)
+- [PR Troubleshooting](docs/development/git_pr_troubleshooting.md)
 
 ---
 
-## Use Cases
-
-### 1. Code Review Optimization
-Test variables:
-- Temperature (conservative vs creative)
-- Model (DeepSeek vs Claude vs GPT-4)
-- Context size (file vs module vs full codebase)
-- Generation strategy (standard vs CoT)
-
-**Result:** Find configuration that maximizes issue detection while minimizing false positives and cost.
+## CLI commands
 
-### 2. Creative Writing (Fiction Scenes)
-Test variables:
-- Temperature
-- Model
-- Context (minimal vs full story context)
-- Generation strategy (standard vs Verbalized Sampling)
+```text
+$ tesseract --help
+Usage: tesseract [OPTIONS] COMMAND [ARGS]...
 
-**Result:** Find configuration that maximizes diversity and quality while staying within budget.
-
-### 3. Documentation Generation
-Test variables:
-- Model (technical models vs general models)
-- Context size
-- Few-shot examples (0 vs 3 vs 5)
-- Output format (markdown vs restructured text)
-
-**Result:** Find configuration that produces clear, accurate documentation efficiently.
-
-### 4. Customer Support Response Generation
-Test variables:
-- Temperature
-- Model
-- Context (ticket only vs customer history)
-- Generation strategy (standard vs few-shot)
-
-**Result:** Balance quality (customer satisfaction) with cost and response time.
+Options:
+  -l, --log-level LEVEL  Set global log level (critical, error, warning, info, debug).
+  --version              Show the TesseractFlow version and exit.
+  --help                 Show this message and exit.
+```
 
----
+Available subcommands:
 
-## Roadmap
-
-### Phase 1: MVP (Current - Weeks 1-8)
-- âœ… **Specification Complete** - Full spec/plan/tasks generated via speckit workflow
-- â³ Core workflow framework (implementation starting)
-- â³ Taguchi L8 experiments
-- â³ Rubric-based evaluation
-- â³ Pareto frontier visualization
-- â³ LiteLLM provider abstraction
-- ğŸ“‹ Langfuse integration (post-MVP)
-
-### Phase 2: Advanced Features (Weeks 9-16)
-- â³ Pairwise A/B evaluation
-- â³ Judge ensembles
-- â³ L16/L18 orthogonal arrays
-- â³ Web UI for approval queue
-- â³ TruLens evaluator integration
-
-### Phase 3: Platform (Weeks 17-24)
-- ğŸ“‹ Multi-user support
-- ğŸ“‹ Experiment history and comparison
-- ğŸ“‹ Custom strategy plugins
-- ğŸ“‹ Advanced visualizations
+- `tesseract experiment run` â€“ Execute experiments with dry-run, resume, cache, and extra instruction options.
+- `tesseract experiment analyze` â€“ Main effects, baseline comparison, and YAML export.
+- `tesseract experiment status` â€“ Inspect partial runs and resume hints.
+- `tesseract experiment validate` â€“ Lint configuration files without running tests.
+- `tesseract visualize pareto` â€“ Plot quality vs cost (or any supported axes) with optional budget filtering.
 
 ---
 
-## Development Process
-
-**TesseractFlow uses Spec-Kit for systematic feature development.**
-
-### Spec-Kit Workflow
+## Project structure
 
 ```
-1. /speckit.specify  â†’ Generate spec.md from feature description
-2. /speckit.plan     â†’ Generate plan.md, research.md, data-model.md, contracts/
-3. /speckit.tasks    â†’ Generate tasks.md (132 implementation tasks)
-4. /speckit.analyze  â†’ Validate consistency across all artifacts
-5. Implementation    â†’ Follow tasks.md, check off completed items
+tesseract_flow/
+â”œâ”€â”€ core/          # Base workflow service, configuration models, strategies, exceptions
+â”œâ”€â”€ evaluation/    # Rubric evaluator, metrics, caching
+â”œâ”€â”€ experiments/   # Taguchi utilities, executor, analysis helpers
+â”œâ”€â”€ optimization/  # Utility function and Pareto frontier implementation
+â”œâ”€â”€ workflows/     # LangGraph workflows (code_review example)
+â””â”€â”€ cli/           # Typer-powered CLI entry points
 ```
 
-### Current Status
-
-- âœ… **Specification Complete** - spec.md with 3 user stories, 15 requirements, 8 success criteria
-- âœ… **Research Complete** - 5 technical decisions documented
-- âœ… **Planning Complete** - Architecture, dependencies, data model defined
-- âœ… **Tasks Complete** - 132 tasks across 6 phases (64-74 hours estimated)
-- âœ… **Consistency Validated** - All artifacts aligned, 0 ambiguities
-- â³ **Implementation Starting** - Phase 1: Setup (T001-T007)
-
-### Development Guidelines
-
-All work aligns with specifications in `specs/001-mvp-optimizer/`:
-- User stories define what to build (US1: Run Experiment, US2: Analyze, US3: Visualize)
-- Requirements define how it works (FR-001 to FR-015, NFR-001 to NFR-005)
-- Tasks break work into 132 concrete steps with dependencies
-- Constitution principles govern all architectural decisions
-
-See [.agents](.agents) for complete development guidelines and [CLAUDE.md](CLAUDE.md) for AI assistant context.
+Unit tests live under `tests/` with integration coverage for CLI flows, evaluation, and visualization.
 
 ---
 
 ## Contributing
 
-TesseractFlow is in early development. We welcome:
+Contributions and feedback are welcome! Please open an issue to discuss major features or bug reports. When submitting patches:
 
-- **Use cases and feedback** - What workflows are you optimizing?
-- **Bug reports** - Found an issue? Open a GitHub issue.
-- **Feature requests** - What would make TesseractFlow more useful?
-- **Code contributions** - PRs welcome! See `CONTRIBUTING.md`
-
----
-
-## Documentation
-
-**MVP Specification** (Branch: `001-mvp-optimizer`):
-- [Feature Specification](specs/001-mvp-optimizer/spec.md) - User stories, requirements, success criteria
-- [Implementation Plan](specs/001-mvp-optimizer/plan.md) - Technical architecture, decisions
-- [Task Breakdown](specs/001-mvp-optimizer/tasks.md) - 132 implementation tasks
-- [Quickstart Guide](specs/001-mvp-optimizer/quickstart.md) - Complete walkthrough
-
-**Architecture**:
-- [Unified Specification](docs/architecture/unified-spec.md)
-- [Simplified HITL Pattern](docs/architecture/simplified-hitl.md)
-- [Generation Strategies](docs/architecture/generation-strategies.md)
-- [Examples](examples/)
+- Run `pytest` (with coverage enabled) to ensure tests pass.
+- Include documentation updates for new features or CLI options.
+- Follow the coding patterns established in `tesseract_flow/core` and `tesseract_flow/cli`.
 
 ---
 
 ## License
 
-MIT License - see [LICENSE](LICENSE) for details
-
----
-
-## Citation
-
-If you use TesseractFlow in your research or product, please cite:
-
-```bibtex
-@software{tesseractflow2025,
-  title = {TesseractFlow: Multi-dimensional LLM Workflow Optimization},
-  author = {Ramm, Mark},
-  year = {2025},
-  url = {https://github.com/markramm/TesseractFlow}
-}
-```
-
----
-
-## Acknowledgments
-
-- **Taguchi Methods** - Genichi Taguchi's Design of Experiments
-- **LangGraph** - Workflow orchestration framework
-- **Verbalized Sampling** - Research from CHATS Lab (arXiv:2510.01171v3)
-- **LiteLLM** - Universal LLM provider abstraction
-
----
-
-**TesseractFlow** - Navigate the quality-cost-time manifold
+TesseractFlow is released under the [MIT License](LICENSE).
diff --git a/docs/api/core-modules.md b/docs/api/core-modules.md
new file mode 100644
index 0000000000000000000000000000000000000000..148a426c385cf2eb737c413db0985fe7774b4c0a
--- /dev/null
+++ b/docs/api/core-modules.md
@@ -0,0 +1,98 @@
+# Core Module Reference
+
+This document summarizes the primary public classes and functions exposed by TesseractFlow. Import paths are relative to the `tesseract_flow` package.
+
+---
+
+## `tesseract_flow.core`
+
+### `BaseWorkflowService`
+- Generic base class for LangGraph-powered workflows.
+- Subclass and implement `_build_workflow()` to return a compiled `StateGraph`.
+- Provides `run(input_model)` which measures runtime, collects metadata, and converts LangGraph output into the declared `OutputModel`.
+
+### Configuration Models (`core.config`)
+- `ExperimentConfig` â€“ Validates YAML experiments, enforces variable/weight rules, computes hashes, and derives metadata.
+- `TestConfiguration` â€“ Represents a single Taguchi trial with normalized identifiers.
+- `ExperimentRun` â€“ Tracks execution status, normalization statistics, baseline metadata, and aggregated analysis.
+- `WorkflowConfig` â€“ Base class for workflow-specific settings (extended by code review workflow).
+- `ExperimentMetadata` â€“ Captures version info, dependency hashes, and reproducibility data.
+- `TestResult` â€“ Stores raw evaluator payloads, normalized metrics, and derived utility scores.
+
+### Strategies (`core.strategies`)
+- `GenerationStrategy` â€“ Protocol defining how workflows interact with language models.
+- `StandardStrategy` â€“ Default implementation using LiteLLM chat completion calls.
+- `GENERATION_STRATEGIES` â€“ Registry mapping strategy names to callables.
+- `register_strategy(name, factory)` / `get_strategy(name)` â€“ Runtime extension hooks for custom strategies.
+
+### Exceptions (`core.exceptions`)
+- `ConfigurationError`, `ExperimentError`, `EvaluationError`, `CacheError`, etc. â€“ Domain-specific error types surfaced throughout the CLI.
+
+---
+
+## `tesseract_flow.experiments`
+
+### Taguchi utilities (`experiments.taguchi`)
+- `L8_ARRAY` and `generate_test_configs()` â€“ Produce orthogonal test plans for 4â€“7 variables.
+
+### Experiment execution (`experiments.executor`)
+- `ExperimentExecutor` â€“ Orchestrates workflow execution, evaluator calls, persistence, and resume logic.
+- `run_single_test()` â€“ Executes a single configuration and records metrics.
+- `run()` â€“ Executes an entire Taguchi run with optional progress callbacks.
+- `load_run(path)` / `save_run(path, run)` â€“ JSON serialization helpers used by the CLI.
+
+### Analysis (`experiments.analysis`)
+- `MainEffectsAnalyzer` â€“ Computes main effects, sum-of-squares, and contribution percentages.
+- `identify_optimal_config()` â€“ Returns the best configuration and associated statistics.
+- `calculate_quality_improvement()` â€“ Computes baseline vs optimal uplift.
+- `compare_configurations()` â€“ Generates baseline/optimal comparison metadata.
+- `export_optimal_config(path, configuration)` â€“ Writes YAML for downstream automation.
+
+---
+
+## `tesseract_flow.evaluation`
+
+### Metrics (`evaluation.metrics`)
+- `DimensionScore` â€“ Normalized rubric dimension with optional weight.
+- `QualityScore` â€“ Aggregates dimension scores into a single overall metric.
+
+### Rubric evaluator (`evaluation.rubric`)
+- `RubricEvaluator` â€“ LiteLLM-backed evaluator with retry logic, caching hooks, and structured parsing.
+- `build_prompt()` â€“ Generates the system/user prompts containing rubric instructions.
+- `parse_response()` â€“ Validates model output and converts it into `QualityScore` objects.
+
+### Cache (`evaluation.cache`)
+- `CacheBackend` â€“ Protocol for cache implementations.
+- `FileCacheBackend` â€“ Default implementation storing JSON responses on disk with hashing.
+
+---
+
+## `tesseract_flow.optimization`
+
+### Utility (`optimization.utility`)
+- `UtilityFunction` â€“ Applies min-max normalization to cost/latency and computes weighted utilities.
+
+### Pareto (`optimization.pareto`)
+- `ParetoPoint` / `ParetoFrontier` â€“ Identify non-dominated points, filter by budget, and render matplotlib charts.
+
+---
+
+## `tesseract_flow.workflows`
+
+### Code Review Workflow (`workflows.code_review`)
+- `CodeReviewWorkflow` â€“ LangGraph workflow orchestrating analysis and suggestion generation via registered strategies.
+- `CodeReviewInput` / `CodeReviewOutput` / `CodeIssue` â€“ Pydantic models describing workflow inputs and structured outputs.
+- Helper functions for prompt rendering, metadata capture, and strategy instantiation.
+
+---
+
+## CLI Packages (`tesseract_flow.cli`)
+
+- `main.app` â€“ Root Typer application with global `--log-level` and `--version` options.
+- `experiment.run` â€“ Execute Taguchi experiments with dry run, resume, cache, and instruction flags.
+- `experiment.analyze` â€“ Main effects reporting and optimal configuration export.
+- `experiment.status` â€“ Inspect saved runs and highlight incomplete tests.
+- `experiment.validate` â€“ Perform configuration linting without execution.
+- `visualize.pareto` â€“ Render Pareto frontiers with customizable axes and budget filters.
+
+Use `python -m tesseract_flow.cli.main --help` or `tesseract --help` after installation to browse full option lists.
diff --git a/docs/development/git_pr_troubleshooting.md b/docs/development/git_pr_troubleshooting.md
new file mode 100644
index 0000000000000000000000000000000000000000..552cfe71b3689e2a9126f182d6883c4d191ad2d4
--- /dev/null
+++ b/docs/development/git_pr_troubleshooting.md
@@ -0,0 +1,67 @@
+# Pull Request Troubleshooting
+
+This repository is initialized without a Git remote, so commands such as `git push` or any tooling that relies on a GitHub pull request will fail until you configure one. Even after renaming the working branch to match the project guide (`001-mvp-optimizer`), the automation still cannot open a PR because there is nowhere to push the commits.
+
+## Why PR creation fails
+
+The automation checks for an `origin` remote before attempting to create a pull request. Because the project is bootstrapped locally for implementation work, no remote is defined by default:
+
+```bash
+git remote -v
+# (no output)
+```
+
+Without a remote, Git cannot determine where to push your commits, which is a prerequisite for PR creation on GitHub.
+
+You can verify the current branch separately. After the recent rename the status shows the expected branch but still fails to push:
+
+```bash
+git status -sb
+## 001-mvp-optimizer
+```
+
+If your output shows a different branch name, run `git branch -m <old> 001-mvp-optimizer` to align with the specification. This alone does not fix PR creation; a remote is still required.
+
+## Fix: add your GitHub remote
+
+1. Create an empty repository on GitHub (or use an existing one).
+2. Add it as the `origin` remote in this project:
+
+```bash
+git remote add origin git@github.com:<your-username>/<your-repo>.git
+```
+
+3. Push the current branch:
+
+```bash
+git push -u origin 001-mvp-optimizer
+```
+
+After the remote exists, the PR automation can push commits and open a pull request as expected.
+
+## Local work is unaffected
+
+You can continue committing locally even without a remote:
+
+```bash
+git status
+# On branch 001-mvp-optimizer
+# nothing to commit, working tree clean
+
+# make your changes, then
+
+git add <files>
+git commit -m "feat: describe your change"
+```
+
+These commits remain in your local Git history and can be pushed later once a remote is configured.
+
+## Missing pytest plugins
+
+If `pytest` fails with messages about missing coverage or asyncio plugins, install them explicitly:
+
+```bash
+pip install pytest-cov pytest-asyncio
+```
+
+These packages are already listed in `pyproject.toml`, but they need to be installed into your active environment (e.g., a new virtualenv) before coverage-enabled test runs will succeed.
diff --git a/docs/development/setup.md b/docs/development/setup.md
new file mode 100644
index 0000000000000000000000000000000000000000..5f03c4a3abfb36d74e0b6f4ce762cf188769e277
--- /dev/null
+++ b/docs/development/setup.md
@@ -0,0 +1,55 @@
+# Development Environment Setup
+
+This guide explains how to get a local development environment ready for working on TesseractFlow.
+
+## Prerequisites
+
+- Python 3.11 or newer
+- Poetry or pip for dependency management
+- Git
+- Make (optional but recommended)
+
+## Installation Steps
+
+1. **Clone the repository**
+   ```bash
+   git clone https://github.com/markramm/TesseractFlow.git
+   cd TesseractFlow
+   ```
+
+2. **Create a virtual environment**
+   ```bash
+   python -m venv .venv
+   source .venv/bin/activate  # On Windows use `.venv\\Scripts\\activate`
+   ```
+
+3. **Install dependencies**
+   ```bash
+   pip install --upgrade pip
+   pip install -e .[dev]
+   ```
+
+4. **Set environment variables**
+   ```bash
+   cp .env.example .env
+   # Update provider API keys as needed
+   ```
+
+5. **Run tests to verify setup**
+   ```bash
+   pytest
+   ```
+
+## Tooling
+
+- **Formatting**: `black` (configured via `pyproject.toml`)
+- **Linting**: `ruff`
+- **Type checking**: `mypy`
+- **Testing**: `pytest` with coverage reporting
+
+Use `pre-commit install` to enable automated checks before each commit.
+
+## Troubleshooting
+
+If dependencies fail to install, ensure you are using Python 3.11+ and upgrade `pip`.
+For issues with native extensions, install system build tools (e.g., `build-essential` on Ubuntu).
diff --git a/docs/user-guide/configuration.md b/docs/user-guide/configuration.md
new file mode 100644
index 0000000000000000000000000000000000000000..82f3afc047f9c6e1087b376bb08ed6daed10d0a8
--- /dev/null
+++ b/docs/user-guide/configuration.md
@@ -0,0 +1,120 @@
+# Configuration Reference
+
+TesseractFlow experiments are described with YAML files that map directly to the `ExperimentConfig` Pydantic model. This guide explains every supported section and provides tips for creating valid configurations that work with the Taguchi L8 design.
+
+> Use `tesseract experiment validate <config.yaml>` to lint a configuration and preview inferred metadata before running a full experiment.
+
+---
+
+## Top-level fields
+
+| Field | Required | Description |
+| ----- | -------- | ----------- |
+| `name` | âœ… | Unique experiment identifier. Used for output file naming and run metadata. |
+| `workflow` | âœ… | Name of the workflow to execute. The distribution ships with the `code_review` workflow. |
+| `variables` | âœ… | Mapping of experimental variables to two discrete levels. Each value may be any JSON-serializable type. |
+| `utility_weights` | âœ… | Weights used to balance quality, cost, and latency inside the utility function. |
+| `workflow_config` | âœ… | Workflow-specific configuration passed to the workflow service. Structure depends on the workflow. |
+| `seed` | Optional | Integer used to seed deterministic operations (sampling, evaluator shuffling). |
+| `metadata` | Optional | Arbitrary key/value metadata stored alongside experiment results. |
+
+All other keys are validated and preserved by the underlying model; unexpected sections raise a `ConfigurationError`.
+
+---
+
+## Variables block
+
+Each variable must specify exactly **two levels** to align with the Taguchi L8 design. Levels can be numbers, strings, or structured objects. Example:
+
+```yaml
+variables:
+  temperature:
+    level_1: 0.3
+    level_2: 0.7
+  model:
+    level_1: "openrouter/anthropic/claude-3.5-sonnet"
+    level_2: "openrouter/openai/gpt-4.1-mini"
+  context_size:
+    level_1: "file_only"
+    level_2: "full_module"
+  generation_strategy:
+    level_1:
+      name: standard
+    level_2:
+      name: chain_of_thought
+      max_steps: 3
+```
+
+### Validation rules
+
+- A minimum of 4 and a maximum of 7 variables are supported.
+- Variable identifiers must be snake_case and begin with a letter.
+- Each `level_1`/`level_2` entry is required. Additional keys trigger validation errors.
+- Structured levels are preserved as dictionaries inside `TestConfiguration` instances and emitted in result files.
+
+---
+
+## Utility weights
+
+Utility scores are computed per test using the normalized metrics and the provided weights:
+
+```text
+utility = (quality_weight Ã— quality)
+          - (cost_weight Ã— normalized_cost)
+          - (time_weight Ã— normalized_latency)
+```
+
+Guidance:
+
+- All weights must be non-negative numbers.
+- At least one weight must be non-zero.
+- Increase `cost` or `time` weights to penalize expensive or slow configurations.
+
+---
+
+## Workflow configuration
+
+The `workflow_config` block is passed straight into the workflow service. For the built-in `code_review` workflow the following keys are recognized:
+
+| Key | Description |
+| --- | ----------- |
+| `rubric` | Mapping of rubric dimension identifiers to metadata (`description`, `scale`, optional `weight`). |
+| `sample_code_path` | Path to the source file evaluated during dry runs and tests. |
+| `language` | Optional override for language detection (defaults to auto-detected via file suffix). |
+| `prompt_overrides` | Optional overrides for the default system/user prompts. |
+
+Rubric definitions must include at least one dimension. Each dimension becomes a `DimensionScore` in the evaluator output and contributes to the aggregate quality score.
+
+---
+
+## Advanced options
+
+### Cache controls
+
+Experiment runs can reuse evaluator outputs via the FileCache backend. Supply cache flags on the CLI:
+
+- `--use-cache` â€“ Replay cached responses if present.
+- `--record-cache` â€“ Persist all responses using hashes derived from prompt, model, and temperature.
+- `--cache-dir` â€“ Override the default cache location.
+
+### Resume support
+
+Specify `--resume --output <results.json>` when re-running a command to resume from partial results. The executor ensures previously completed trials are skipped and maintains consistent normalization metadata.
+
+### Extra evaluator instructions
+
+Use `--instructions "text"` to append additional rubric context when invoking `experiment run`. This is particularly helpful when adapting the code review workflow to other languages or repositories.
+
+---
+
+## Result files
+
+Experiments produce JSON documents adhering to the `ExperimentRun` model. Key sections include:
+
+- `experiment_id`, `name`, and `workflow`
+- `test_configurations` â€“ ordered list describing each Taguchi trial
+- `results` â€“ per-test quality metrics, raw evaluator responses, and cost/time measurements
+- `normalization` â€“ min/max statistics used for utility calculations
+- `analysis` â€“ baseline metadata, optimal configuration summary, and quality improvement percentages
+
+These files feed directly into `tesseract experiment analyze` and `tesseract visualize pareto`.
diff --git a/docs/user-guide/interpreting-results.md b/docs/user-guide/interpreting-results.md
new file mode 100644
index 0000000000000000000000000000000000000000..635550cd04fe24069f363f06c208ff4419ffe9b1
--- /dev/null
+++ b/docs/user-guide/interpreting-results.md
@@ -0,0 +1,98 @@
+# Interpreting Results
+
+After running an experiment you receive a JSON file (default `results.json`) plus optional CLI summaries. This guide explains how to interpret those outputs and translate them into workflow improvements.
+
+---
+
+## 1. Inspect the run summary
+
+Every run stores metadata alongside results:
+
+- **`experiment_id`** â€“ Deterministic UUID derived from the configuration hash.
+- **`started_at` / `completed_at`** â€“ ISO 8601 timestamps that let you measure total runtime.
+- **`baseline`** â€“ Captures the initial configuration (Taguchi row 1) and its quality score for comparison.
+- **`normalization`** â€“ Min/max values for cost and latency used in the utility function.
+
+Use the CLI to display the latest run at any time:
+
+```bash
+$ tesseract experiment status results.json
+```
+
+This command highlights unfinished tests, shows resume hints, and summarizes the best configuration discovered so far.
+
+---
+
+## 2. Evaluate quality metrics
+
+The evaluator returns structured quality data per trial:
+
+- `score.overall` â€“ Weighted aggregate between 0 and 1.
+- `score.dimensions` â€“ Individual rubric dimensions, each normalized between 0 and 1.
+- `raw_response` â€“ Unmodified JSON returned by the LLM before normalization.
+
+### Troubleshooting evaluator output
+
+- **Low variance** â€“ If every trial produces nearly identical scores, revisit rubric wording or enable `--instructions` to provide more context.
+- **Parsing failures** â€“ Inspect the raw response to confirm the model followed the schema. Consider switching providers or temperature levels.
+- **Baseline worse than optimal** â€“ Expected! Use the improvement percentage to quantify gains relative to the starting point.
+
+---
+
+## 3. Understand main effects
+
+Run the analysis command to compute variable contributions:
+
+```bash
+$ tesseract experiment analyze results.json --format table
+```
+
+The Rich table lists each variable, its effect size, average utility per level, and contribution percentage. Focus your iteration on variables with the largest contributions.
+
+- **High contribution (>30%)** â€“ Strong leverage. Standardize on the better-performing level and explore additional variants.
+- **Medium contribution (10â€“30%)** â€“ Influential but secondary. Tune alongside high-impact variables.
+- **Low contribution (<10%)** â€“ Minimal effect. Consider removing to reduce experiment complexity.
+
+The command also reports the quality improvement percentage compared to the baseline and exports the recommended configuration when `--export` is supplied.
+
+---
+
+## 4. Compare configurations
+
+Inside the JSON results the `analysis` block includes:
+
+- `optimal_configuration` â€“ Full variable assignment for the best-scoring trial.
+- `baseline_configuration` â€“ Original Taguchi row for reference.
+- `quality_improvement_pct` â€“ Percent uplift relative to the baseline score.
+- `top_tests` â€“ Utility-sorted leaderboard to help you inspect near-optimal alternatives.
+
+Use these data to brief stakeholders or update CI/CD pipelines with concrete configuration changes.
+
+---
+
+## 5. Visualize trade-offs
+
+The Pareto frontier reveals how quality trades off against cost and latency:
+
+```bash
+$ tesseract visualize pareto results.json --output pareto.png --budget 0.012
+```
+
+Interpretation tips:
+
+- Points on the frontier are non-dominated (no other trial is better on both axes).
+- Bubble size encodes latency â€“ smaller bubbles indicate faster responses.
+- The optional budget line highlights tests that meet cost constraints.
+
+Check the console output for a textual summary of Pareto-optimal configurations sorted by quality, along with budget-aware recommendations.
+
+---
+
+## 6. Decide next actions
+
+1. **Adopt the recommended configuration** â€“ Apply the optimal levels to your production workflow.
+2. **Rerun with refined variables** â€“ Swap out low-contribution variables for new hypotheses.
+3. **Adjust utility weights** â€“ Rebalance priorities if cost or latency constraints change.
+4. **Record evaluator cache** â€“ Keep `--record-cache` enabled to compare historical runs consistently.
+
+By iterating on these steps you can continuously improve LLM workflows without retraining models.
diff --git a/docs/user-guide/quickstart.md b/docs/user-guide/quickstart.md
new file mode 100644
index 0000000000000000000000000000000000000000..44b1a0262130b1db65baa02b9320a97eefbc9a97
--- /dev/null
+++ b/docs/user-guide/quickstart.md
@@ -0,0 +1,137 @@
+# Quickstart
+
+This quickstart walks through running the built-in code review optimization experiment end-to-end. By the end you will have:
+
+1. Loaded the sample Taguchi L8 experiment configuration.
+2. Executed all eight trials with cached rubric evaluation responses.
+3. Analyzed the resulting quality, cost, and latency trade-offs.
+4. Visualized the Pareto frontier and exported the recommended configuration.
+
+---
+
+## Prerequisites
+
+- Python 3.11 or newer.
+- An API key for at least one LiteLLM-supported provider (OpenRouter, Anthropic, OpenAI, etc.).
+- Git (for cloning this repository) and virtualenv tooling such as `uv`, `pipenv`, or `python -m venv`.
+
+> The CLI supports offline execution using the bundled cache. Real LLM access is required to record fresh evaluations.
+
+---
+
+## 1. Install TesseractFlow from source
+
+```bash
+# Clone the repository
+$ git clone https://github.com/markramm/TesseractFlow.git
+$ cd TesseractFlow
+
+# Create and activate a virtual environment (example using venv)
+$ python -m venv .venv
+$ source .venv/bin/activate
+
+# Install the package and CLI
+(.venv) $ pip install -e .
+```
+
+The editable install exposes the `tesseract` CLI entrypoint and brings in required dependencies such as Typer, Rich, LiteLLM, and matplotlib.
+
+---
+
+## 2. Configure environment variables
+
+Export any provider credentials required by LiteLLM. The examples below use OpenRouter:
+
+```bash
+(.venv) $ export OPENROUTER_API_KEY="sk-your-key"
+```
+
+You can also store the key inside a `.env` file that your shell or process manager loads before running experiments.
+
+---
+
+## 3. Inspect the sample experiment
+
+The repository ships with a ready-to-run configuration:
+
+```bash
+(.venv) $ cat examples/code_review/experiment_config.yaml
+```
+
+This file defines four binary variables (temperature, model, context window, and generation strategy), utility weights, and the rubric used to grade each review.
+
+---
+
+## 4. Perform a dry run
+
+Validate the configuration and preview the Taguchi test matrix without executing any LLM calls:
+
+```bash
+(.venv) $ tesseract experiment run examples/code_review/experiment_config.yaml --dry-run
+```
+
+You should see the inferred eight test configurations along with summary statistics and resume hints.
+
+---
+
+## 5. Execute the experiment
+
+Run all eight tests, resuming from previous partial results if necessary:
+
+```bash
+(.venv) $ tesseract experiment run \
+    examples/code_review/experiment_config.yaml \
+    --output results.json \
+    --record-cache \
+    --verbose
+```
+
+Key behaviours during execution:
+
+- Progress is displayed with Rich spinners and bars.
+- Partial results are written to `results.json` after each trial.
+- Evaluation responses are cached under `~/.cache/tesseract_flow/evaluations/` when `--record-cache` is supplied.
+- Use `--use-cache` to replay cached responses without performing new API calls.
+
+If the command exits early, re-run it with `--resume --output results.json` to continue from the saved state.
+
+---
+
+## 6. Analyze results
+
+Generate main effects, compare baseline versus optimal configurations, and optionally export the recommended YAML:
+
+```bash
+(.venv) $ tesseract experiment analyze results.json --format table --export optimal.yaml
+```
+
+Useful variations:
+
+- `--format json` or `--format markdown` for automated pipelines.
+- Omit `--export` to skip writing the optimal configuration file.
+
+The analysis output highlights each variable's contribution, shows the baseline quality, and reports the quality improvement percentage.
+
+---
+
+## 7. Visualize the Pareto frontier
+
+Render a PNG chart that illustrates quality/cost trade-offs across all trials:
+
+```bash
+(.venv) $ tesseract visualize pareto results.json --output pareto.png --budget 0.010
+```
+
+This produces both a console summary of Pareto-optimal configurations and a saved image containing labelled points, latency-driven bubble sizes, and an optional budget threshold.
+
+Use `--y-axis utility` or `--x-axis latency` to explore alternative views, and omit `--output` to display the chart interactively (if a display server is available).
+
+---
+
+## 8. Next steps
+
+- Edit `examples/code_review/experiment_config.yaml` to add additional variables or adjust utility weights.
+- Point the workflow at your own repository by updating `workflow_config.sample_code_path`.
+- Integrate the underlying `ExperimentExecutor` or `RubricEvaluator` classes into custom automation by importing them from `tesseract_flow.experiments` and `tesseract_flow.evaluation`.
+
+Happy optimizing!
diff --git a/examples/README.md b/examples/README.md
new file mode 100644
index 0000000000000000000000000000000000000000..7a84cfdb3137af1fec66572fc0ca2815960e737c
--- /dev/null
+++ b/examples/README.md
@@ -0,0 +1,33 @@
+# Examples
+
+The `code_review` example demonstrates how to optimize an LLM code review workflow with the Taguchi L8 design.
+
+## Directory structure
+
+```
+examples/
+â””â”€â”€ code_review/
+    â”œâ”€â”€ experiment_config.yaml   # Ready-to-run Taguchi configuration
+    â””â”€â”€ sample_code/
+        â”œâ”€â”€ example1.py          # Baseline snippet used during runs
+        â””â”€â”€ example2.py          # Additional file for experimentation
+```
+
+## Running the example
+
+1. Install TesseractFlow in a virtual environment (`pip install -e .`).
+2. Export an API key supported by LiteLLM (`export OPENROUTER_API_KEY=...`).
+3. Execute the experiment:
+
+   ```bash
+   tesseract experiment run examples/code_review/experiment_config.yaml --output results.json --record-cache
+   ```
+
+4. Analyze and visualize the results:
+
+   ```bash
+   tesseract experiment analyze results.json --export optimal.yaml
+   tesseract visualize pareto results.json --output pareto.png --budget 0.010
+   ```
+
+The configuration balances quality, cost, and latency across four binary variables: temperature, model, context window, and generation strategy. Use it as a template for your own repositories by replacing `sample_code_path` and rubric details.
diff --git a/examples/code_review/experiment_config.yaml b/examples/code_review/experiment_config.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..bc6f6c848801b3684b6aa91bcbfe3ccdf0ac8961
--- /dev/null
+++ b/examples/code_review/experiment_config.yaml
@@ -0,0 +1,39 @@
+# Taguchi L8 experiment configuration for the code review workflow
+name: "code_review_optimization"
+workflow: "code_review"
+
+variables:
+  - name: "temperature"
+    level_1: 0.3
+    level_2: 0.7
+  - name: "model"
+    level_1: "openai/gpt-4o-mini"
+    level_2: "anthropic/claude-3.5-sonnet"
+  - name: "context_size"
+    level_1: "file_only"
+    level_2: "full_module"
+  - name: "generation_strategy"
+    level_1: "standard"
+    level_2: "chain_of_thought"
+
+utility_weights:
+  quality: 1.0
+  cost: 0.1
+  time: 0.05
+
+workflow_config:
+  rubric:
+    clarity:
+      description: "Is the review easy to follow and understand?"
+      scale: "1-10"
+    accuracy:
+      description: "Do the identified issues reflect real problems?"
+      scale: "1-10"
+    completeness:
+      description: "Does the review cover important aspects of the code?"
+      scale: "1-10"
+  sample_code_path: "examples/code_review/sample_code/example1.py"
+  language: "python"
+  context_descriptions:
+    file_only: "Review limited to the provided snippet."
+    full_module: "Consider module-level implications during review."
diff --git a/examples/code_review/sample_code/example1.py b/examples/code_review/sample_code/example1.py
new file mode 100644
index 0000000000000000000000000000000000000000..252411de5c853af0ed5272a9e96b94dc3b334644
--- /dev/null
+++ b/examples/code_review/sample_code/example1.py
@@ -0,0 +1,9 @@
+"""Example module showcasing simple math operations."""
+
+
+def add(a: int, b: int) -> int:
+    return a + b
+
+
+def subtract(a: int, b: int) -> int:
+    return a - b
diff --git a/examples/code_review/sample_code/example2.py b/examples/code_review/sample_code/example2.py
new file mode 100644
index 0000000000000000000000000000000000000000..7d2b3ae06b2509c48c48b970a8301c8ea401b3ed
--- /dev/null
+++ b/examples/code_review/sample_code/example2.py
@@ -0,0 +1,9 @@
+"""Example module showcasing string utilities."""
+
+
+def title_case(text: str) -> str:
+    return text.title()
+
+
+def reverse(text: str) -> str:
+    return text[::-1]
diff --git a/pyproject.toml b/pyproject.toml
index 1c93a936572fca673c8127039260002639635eb3..1d55f23bc4e030de2aabb2717060f290709ad187 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -16,94 +16,89 @@ keywords = [
     "llm",
     "optimization",
     "taguchi",
     "design-of-experiments",
     "doe",
     "workflow",
     "langgraph",
     "prompt-engineering"
 ]
 classifiers = [
     "Development Status :: 3 - Alpha",
     "Intended Audience :: Developers",
     "Intended Audience :: Science/Research",
     "License :: OSI Approved :: MIT License",
     "Programming Language :: Python :: 3",
     "Programming Language :: Python :: 3.11",
     "Programming Language :: Python :: 3.12",
     "Topic :: Scientific/Engineering :: Artificial Intelligence",
     "Topic :: Software Development :: Libraries :: Python Modules",
 ]
 
 dependencies = [
     "pydantic>=2.0",
     "pyyaml>=6.0",
     "numpy>=1.24",
-    "scipy>=1.10",
     "langgraph>=0.2.0",
-    "langchain-core>=0.3.0",
     "litellm>=1.0",
-    "httpx>=0.25",
     "jinja2>=3.1",
+    "rich>=13.7",
     "matplotlib>=3.7",
-    "fastapi>=0.104",
-    "uvicorn>=0.24",
-    "sqlalchemy>=2.0",
-    "asyncpg>=0.29",
-    "alembic>=1.12",
-    "python-dotenv>=1.0",
+    "typer>=0.9",
 ]
 
 [project.optional-dependencies]
 verbalized-sampling = [
     "verbalized-sampling>=0.1.0",
 ]
 observability = [
     "langfuse>=2.0",
     "weave>=0.50",
 ]
 dev = [
     "pytest>=7.4",
     "pytest-asyncio>=0.21",
     "pytest-cov>=4.1",
     "black>=23.0",
     "ruff>=0.1",
     "mypy>=1.7",
     "pre-commit>=3.5",
+    "build>=1.0",
 ]
 all = [
     "tesseract-flow[verbalized-sampling,observability,dev]",
 ]
 
 [project.urls]
 Homepage = "https://github.com/markramm/TesseractFlow"
 Documentation = "https://github.com/markramm/TesseractFlow/tree/main/docs"
 Repository = "https://github.com/markramm/TesseractFlow"
 Issues = "https://github.com/markramm/TesseractFlow/issues"
 
 [project.scripts]
 tesseract = "tesseract_flow.cli:main"
+tesseract-flow = "tesseract_flow.cli:main"
 
 [tool.setuptools.packages.find]
 where = ["."]
 include = ["tesseract_flow*"]
 
 [tool.pytest.ini_options]
 testpaths = ["tests"]
 asyncio_mode = "auto"
 addopts = [
     "--cov=tesseract_flow",
     "--cov-report=html",
     "--cov-report=term-missing",
     "-v"
 ]
 
 [tool.black]
 line-length = 100
 target-version = ["py311"]
 include = '\.pyi?$'
 
 [tool.ruff]
 line-length = 100
 target-version = "py311"
 select = [
     "E",   # pycodestyle errors
diff --git a/tesseract_flow/__init__.py b/tesseract_flow/__init__.py
index 92628b0942a93ea6f8909929b25c7987dec056c7..24cdd502d6a69124523ec0a600f36818e1ba8190 100644
--- a/tesseract_flow/__init__.py
+++ b/tesseract_flow/__init__.py
@@ -1,25 +1,30 @@
 """
 TesseractFlow - Multi-dimensional LLM Workflow Optimization
 
 A framework for systematic LLM workflow optimization using Taguchi Design of Experiments.
 """
 
 __version__ = "0.1.0"
 
 from tesseract_flow.core.base_workflow import BaseWorkflowService
 from tesseract_flow.core.config import WorkflowConfig
 from tesseract_flow.core.strategies import (
     GENERATION_STRATEGIES,
+    GenerationStrategy,
     get_strategy,
     register_strategy,
-    GenerationStrategy,
 )
+from tesseract_flow.workflows import CodeIssue, CodeReviewInput, CodeReviewOutput, CodeReviewWorkflow
 
 __all__ = [
     "BaseWorkflowService",
     "WorkflowConfig",
     "GENERATION_STRATEGIES",
     "get_strategy",
     "register_strategy",
     "GenerationStrategy",
+    "CodeReviewWorkflow",
+    "CodeReviewInput",
+    "CodeReviewOutput",
+    "CodeIssue",
 ]
diff --git a/tesseract_flow/cli/__init__.py b/tesseract_flow/cli/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..d71447acd95195dea3d53f37e28b275ddaff342e
--- /dev/null
+++ b/tesseract_flow/cli/__init__.py
@@ -0,0 +1,5 @@
+"""Command line interface entry points for TesseractFlow."""
+
+from .main import app, main
+
+__all__ = ["app", "main"]
diff --git a/tesseract_flow/cli/experiment.py b/tesseract_flow/cli/experiment.py
new file mode 100644
index 0000000000000000000000000000000000000000..6d345bfbcf0f8af2fad7651c8f20c26b1bec98b6
--- /dev/null
+++ b/tesseract_flow/cli/experiment.py
@@ -0,0 +1,992 @@
+"""Experiment-related CLI commands for TesseractFlow."""
+
+from __future__ import annotations
+
+import asyncio
+import logging
+from datetime import datetime, timedelta
+import json
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Mapping, Optional, Sequence, Union
+
+import typer
+from pydantic import ValidationError
+from rich.console import Console
+from rich.progress import (
+    BarColumn,
+    Progress,
+    SpinnerColumn,
+    TextColumn,
+    TimeElapsedColumn,
+)
+from rich.table import Table
+
+from tesseract_flow.core.config import ExperimentConfig, ExperimentRun, TestConfiguration
+from tesseract_flow.core.exceptions import (
+    CacheError,
+    ConfigurationError,
+    EvaluationError,
+    ExperimentError,
+)
+from tesseract_flow.evaluation.cache import FileCacheBackend
+from tesseract_flow.evaluation.rubric import RubricEvaluator
+from tesseract_flow.experiments.analysis import (
+    MainEffects,
+    MainEffectsAnalyzer,
+    calculate_quality_improvement,
+    compare_configurations,
+    export_optimal_config,
+    identify_optimal_config,
+)
+from tesseract_flow.experiments.executor import ExperimentExecutor
+from tesseract_flow.experiments.taguchi import generate_test_configs
+from tesseract_flow.workflows.code_review import CodeReviewWorkflow
+
+console = Console()
+app = typer.Typer()
+logger = logging.getLogger(__name__)
+
+_CONFIG_ERROR_EXIT = 1
+_EXECUTION_ERROR_EXIT = 2
+_IO_ERROR_EXIT = 3
+
+
+@app.command("run")
+def run_experiment(
+    ctx: typer.Context,
+    config_file: Path = typer.Argument(..., exists=True, readable=True, help="Experiment YAML file."),
+    output: Optional[Path] = typer.Option(
+        None,
+        "--output",
+        "-o",
+        help="Destination JSON file for experiment results.",
+        dir_okay=False,
+    ),
+    verbose: bool = typer.Option(False, "--verbose", "-v", help="Enable verbose logging."),
+    dry_run: bool = typer.Option(False, "--dry-run", help="Validate configuration without executing."),
+    resume: bool = typer.Option(False, "--resume", help="Resume an interrupted experiment from output file."),
+    use_cache: bool = typer.Option(False, "--use-cache", help="Replay cached evaluator responses."),
+    record_cache: bool = typer.Option(False, "--record-cache", help="Store evaluator responses to cache."),
+    cache_dir: Optional[Path] = typer.Option(
+        None,
+        "--cache-dir",
+        help="Directory for evaluator cache files (defaults to ~/.cache/tesseract_flow/evaluations).",
+        file_okay=False,
+    ),
+    extra_instructions: Optional[str] = typer.Option(
+        None,
+        "--instructions",
+        help="Additional rubric guidance appended to evaluator prompt.",
+    ),
+) -> None:
+    """Execute the Taguchi experiment defined in *config_file*."""
+
+    configure_logging(verbose, log_level=_ctx_log_level(ctx))
+
+    try:
+        config = ExperimentConfig.from_yaml(config_file)
+    except (OSError, FileNotFoundError) as exc:
+        console.print(f"[bold red]âœ— Error:[/] Failed to read configuration: {exc}")
+        raise typer.Exit(code=_IO_ERROR_EXIT) from exc
+    except ConfigurationError as exc:
+        _render_configuration_error(exc)
+        raise typer.Exit(code=_CONFIG_ERROR_EXIT) from exc
+    except (ValidationError, ValueError) as exc:
+        _render_configuration_error(ConfigurationError(str(exc)))
+        raise typer.Exit(code=_CONFIG_ERROR_EXIT) from exc
+
+    console.print(f"[green]âœ“ Loaded experiment config:[/] {config.name}")
+
+    if dry_run:
+        _handle_dry_run(config)
+        raise typer.Exit(code=0)
+
+    if resume and output is None:
+        console.print("[bold red]âœ— Error:[/] --resume requires --output pointing to saved results.")
+        raise typer.Exit(code=_IO_ERROR_EXIT)
+
+    resolved_output = output or _default_output_path(config.name)
+    resolved_output = resolved_output.resolve()
+    console.print(f"[cyan]â€¢ Results will be written to:[/] {resolved_output}")
+
+    resume_run: Optional[ExperimentRun] = None
+    if resume:
+        try:
+            resume_run = load_run(resolved_output)
+        except (OSError, ValueError, ValidationError) as exc:
+            console.print(f"[bold red]âœ— Error:[/] Unable to load saved results: {exc}")
+            raise typer.Exit(code=_IO_ERROR_EXIT) from exc
+        console.print(
+            f"[yellow]â†» Resuming run:[/] {resume_run.experiment_id} (completed {len(resume_run.results)}/"
+            f"{len(resume_run.test_configurations)} tests)"
+        )
+
+    try:
+        evaluator = build_evaluator(
+            use_cache=use_cache,
+            record_cache=record_cache,
+            cache_dir=cache_dir,
+        )
+    except (CacheError, ValueError) as exc:
+        console.print(f"[bold red]âœ— Error:[/] Failed to initialize evaluator: {exc}")
+        raise typer.Exit(code=_CONFIG_ERROR_EXIT) from exc
+
+    executor = build_executor(config, evaluator)
+
+    try:
+        run = asyncio.run(
+            _execute_experiment(
+                executor,
+                config,
+                output_path=resolved_output,
+                resume_from=resume_run,
+                extra_instructions=extra_instructions,
+            )
+        )
+    except (EvaluationError, ExperimentError) as exc:
+        _render_execution_error(exc, resolved_output)
+        raise typer.Exit(code=_EXECUTION_ERROR_EXIT) from exc
+    except CacheError as exc:
+        _render_cache_error(exc)
+        raise typer.Exit(code=_IO_ERROR_EXIT) from exc
+    except OSError as exc:
+        console.print(f"[bold red]âœ— Error:[/] Failed to persist results: {exc}")
+        _render_recovery_tips([
+            "Verify the output directory is writable.",
+            "Provide an alternate path with --output.",
+        ])
+        raise typer.Exit(code=_IO_ERROR_EXIT) from exc
+
+    console.print("[green]âœ“ All tests completed successfully")
+    console.print(f"[green]âœ“ Results saved to:[/] {resolved_output}")
+    _render_summary(run, resolved_output)
+
+
+@app.command("analyze")
+def analyze_results(
+    ctx: typer.Context,
+    results_file: Path = typer.Argument(
+        ..., exists=True, readable=True, help="Experiment results JSON file."
+    ),
+    output_format: str = typer.Option(
+        "table",
+        "--format",
+        "-f",
+        help="Output format: table, json, or markdown.",
+    ),
+    export: Optional[Path] = typer.Option(
+        None,
+        "--export",
+        "-e",
+        help="Destination YAML file for recommended configuration.",
+        dir_okay=False,
+    ),
+) -> None:
+    """Analyze experiment results and display main effects."""
+
+    configure_logging(False, log_level=_ctx_log_level(ctx))
+
+    fmt = output_format.strip().lower()
+    if fmt not in {"table", "json", "markdown"}:
+        console.print("[bold red]âœ— Error:[/] --format must be table, json, or markdown.")
+        raise typer.Exit(code=_CONFIG_ERROR_EXIT)
+
+    try:
+        run = load_run(results_file)
+    except (OSError, ValueError, ValidationError) as exc:
+        console.print(f"[bold red]âœ— Error:[/] Failed to load results: {exc}")
+        _render_recovery_tips([
+            "Ensure the file is a valid experiment JSON output.",
+            "Re-run the experiment with --output to regenerate results.",
+        ])
+        raise typer.Exit(code=_IO_ERROR_EXIT) from exc
+
+    if len(run.results) != 8:
+        console.print("[bold red]âœ— Error:[/] Completed results with 8 tests are required for analysis.")
+        raise typer.Exit(code=_CONFIG_ERROR_EXIT)
+
+    try:
+        main_effects = MainEffectsAnalyzer.compute(
+            run.results, run.config.variables, experiment_id=run.experiment_id
+        )
+    except ValueError as exc:
+        console.print(f"[bold red]âœ— Error:[/] {exc}")
+        _render_recovery_tips([
+            "Confirm the results file was produced by the current version of TesseractFlow.",
+            "Re-run analysis after regenerating results.",
+        ])
+        raise typer.Exit(code=_CONFIG_ERROR_EXIT) from exc
+
+    recommended_config = identify_optimal_config(main_effects, run.config.variables)
+    baseline_config = run.baseline_config or run.test_configurations[0]
+    baseline_result = run.baseline_result or next(
+        (item for item in run.results if item.test_number == baseline_config.test_number),
+        None,
+    )
+    baseline_quality = run.baseline_quality
+    if baseline_quality is None and baseline_result is not None:
+        baseline_quality = baseline_result.quality_score.overall_score
+
+    optimal_result = max(run.results, key=lambda item: item.utility)
+    improvement = run.quality_improvement_pct
+    if improvement is None:
+        improvement = calculate_quality_improvement(
+            baseline_quality, optimal_result.quality_score.overall_score
+        )
+
+    comparison = compare_configurations(
+        baseline_config.config_values, optimal_result.config.config_values
+    )
+
+    payload = _build_analysis_payload(
+        run,
+        main_effects,
+        baseline_config.config_values,
+        baseline_result,
+        baseline_quality,
+        optimal_result,
+        recommended_config,
+        improvement,
+        comparison,
+    )
+
+    if fmt == "json":
+        console.print_json(json.dumps(payload, default=str))
+    elif fmt == "markdown":
+        console.print(_render_markdown(payload))
+    else:
+        _render_main_effects_table(main_effects)
+        _render_comparison_table(comparison)
+        _render_metrics_summary(baseline_result, baseline_quality, optimal_result, improvement)
+        if recommended_config != optimal_result.config.config_values:
+            console.print(
+                "[cyan]â€¢ Recommended configuration (main effects):[/] "
+                f"{recommended_config}"
+            )
+
+    if export is not None:
+        try:
+            destination = export_optimal_config(
+                recommended_config,
+                export,
+                experiment_name=run.config.name,
+                workflow=run.config.workflow,
+            )
+        except OSError as exc:
+            console.print(f"[bold red]âœ— Error:[/] Failed to export configuration: {exc}")
+            _render_recovery_tips([
+                "Check the destination directory permissions.",
+                "Choose a different file path with --export.",
+            ])
+            raise typer.Exit(code=_IO_ERROR_EXIT) from exc
+        console.print(f"[green]âœ“ Exported optimal configuration to:[/] {destination}")
+
+
+@app.command("status")
+def status_experiment(
+    ctx: typer.Context,
+    results_file: Path = typer.Argument(
+        ..., exists=True, readable=True, help="Experiment results JSON file."
+    ),
+    details: bool = typer.Option(
+        False,
+        "--details",
+        help="Display per-test metrics for recorded results.",
+    ),
+) -> None:
+    """Display the current status of an experiment run."""
+
+    configure_logging(False, log_level=_ctx_log_level(ctx))
+
+    try:
+        run = load_run(results_file)
+    except (OSError, ValueError, ValidationError) as exc:
+        console.print(f"[bold red]âœ— Error:[/] Failed to load results: {exc}")
+        _render_recovery_tips([
+            "Ensure the file is a valid experiment JSON output.",
+            "Re-run the experiment with --output to regenerate results.",
+        ])
+        raise typer.Exit(code=_IO_ERROR_EXIT) from exc
+
+    console.print(
+        f"[cyan]ğŸ“‚ Experiment:[/] {run.config.name} [dim]({run.experiment_id})[/]"
+    )
+    console.print(_format_status_line(run.status))
+    console.print(_build_status_table(run))
+
+    pending = _pending_tests(run)
+    if pending:
+        console.print(
+            "[yellow]â³ Pending tests:[/] " + ", ".join(f"#{number}" for number in pending)
+        )
+    elif run.status != "COMPLETED":
+        console.print("[yellow]â³ Pending tests:[/] none (waiting for completion)")
+
+    if run.error:
+        console.print(f"[bold red]âš ï¸ Last error:[/] {run.error}")
+
+    if run.results:
+        latest = run.results[-1]
+        console.print(
+            "[green]ğŸ§ª Last recorded result:[/] "
+            f"#{latest.test_number} quality={latest.quality_score.overall_score:.3f} "
+            f"cost={latest.cost:.4f} latency={latest.latency:.1f}ms utility={latest.utility:.3f}"
+        )
+        if not details and len(run.results) < len(run.test_configurations):
+            console.print(
+                "[cyan]â€¢ Use --details to list all recorded metrics so far."
+            )
+
+    if details and run.results:
+        console.print(_build_results_table(run.results))
+
+    if run.status != "COMPLETED":
+        console.print(
+            "[cyan]Next steps:[/] Resume with "
+            f"tesseract experiment run <config.yaml> --resume --output {results_file}"
+        )
+    else:
+        console.print(
+            "[green]âœ“ Experiment completed. Analyze with:[/] "
+            f"tesseract experiment analyze {results_file}"
+        )
+
+
+@app.command("validate")
+def validate_config(
+    ctx: typer.Context,
+    config_file: Path = typer.Argument(
+        ..., exists=True, readable=True, help="Experiment YAML file to validate."
+    ),
+    show_tests: bool = typer.Option(
+        False,
+        "--show-tests/--hide-tests",
+        help="Preview generated Taguchi test configurations.",
+    ),
+) -> None:
+    """Validate an experiment configuration without running it."""
+
+    configure_logging(False, log_level=_ctx_log_level(ctx))
+
+    try:
+        config = ExperimentConfig.from_yaml(config_file)
+    except (OSError, FileNotFoundError) as exc:
+        console.print(f"[bold red]âœ— Error:[/] Failed to read configuration: {exc}")
+        raise typer.Exit(code=_IO_ERROR_EXIT) from exc
+    except ConfigurationError as exc:
+        _render_configuration_error(exc)
+        raise typer.Exit(code=_CONFIG_ERROR_EXIT) from exc
+    except (ValidationError, ValueError) as exc:
+        _render_configuration_error(ConfigurationError(str(exc)))
+        raise typer.Exit(code=_CONFIG_ERROR_EXIT) from exc
+
+    console.print(f"[green]âœ… Configuration is valid:[/] {config.name}")
+    console.print(f"[cyan]â€¢ Workflow:[/] {config.workflow}")
+
+    variables = ", ".join(variable.name for variable in config.variables)
+    console.print(f"[cyan]â€¢ Variables:[/] {variables}")
+
+    sample_code = getattr(config.workflow_config, "sample_code_path", None)
+    if sample_code:
+        console.print(f"[cyan]â€¢ Sample code:[/] {sample_code}")
+
+    rubric = getattr(config.workflow_config, "rubric", {}) or {}
+    console.print(f"[cyan]â€¢ Rubric dimensions:[/] {len(rubric)}")
+
+    if show_tests:
+        _handle_dry_run(config)
+    else:
+        console.print(
+            "[cyan]â€¢ Use --show-tests to preview the generated Taguchi configurations."
+        )
+
+
+def configure_logging(
+    verbose: bool = False,
+    *,
+    log_level: Optional[Union[str, int]] = None,
+) -> None:
+    """Configure root logging for CLI execution."""
+
+    resolved_level: int
+    if isinstance(log_level, int):
+        resolved_level = log_level
+    elif isinstance(log_level, str):
+        normalized = log_level.upper()
+        resolved_level = getattr(logging, normalized, logging.INFO)
+    else:
+        resolved_level = logging.INFO
+
+    if verbose:
+        resolved_level = logging.DEBUG
+
+    logging.basicConfig(
+        level=resolved_level,
+        format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
+        datefmt="%H:%M:%S",
+        force=True,
+    )
+    logging.getLogger("litellm").setLevel(logging.WARNING)
+    if verbose:
+        console.print("[blue]Verbose logging enabled[/]")
+
+
+def _ctx_log_level(ctx: Optional[typer.Context]) -> Optional[Union[str, int]]:
+    if ctx is None:
+        return None
+    obj = ctx.find_object(dict)
+    if not obj:
+        return None
+    return obj.get("log_level")
+
+
+def _default_output_path(config_name: str) -> Path:
+    safe_name = "".join(ch if ch.isalnum() or ch in {"-", "_"} else "_" for ch in config_name.strip())
+    safe_name = safe_name or "experiment"
+    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
+    return Path(f"results_{safe_name}_{timestamp}.json")
+
+
+def build_evaluator(
+    *, use_cache: bool, record_cache: bool, cache_dir: Optional[Path]
+) -> RubricEvaluator:
+    """Return a RubricEvaluator configured with optional caching."""
+
+    cache_backend = None
+    if use_cache or record_cache:
+        cache_path = (cache_dir or _default_cache_dir()).expanduser()
+        cache_backend = FileCacheBackend(cache_path)
+        console.print(f"[cyan]â€¢ Using evaluator cache at:[/] {cache_backend.cache_dir}")
+    return RubricEvaluator(cache=cache_backend, use_cache=use_cache, record_cache=record_cache)
+
+
+def _default_cache_dir() -> Path:
+    home = Path.home()
+    return home / ".cache" / "tesseract_flow" / "evaluations"
+
+
+def build_executor(config: ExperimentConfig, evaluator: RubricEvaluator) -> ExperimentExecutor:
+    """Construct an ExperimentExecutor for the provided configuration."""
+
+    def resolver(workflow_name: str, experiment_config: ExperimentConfig) -> CodeReviewWorkflow:
+        normalized = workflow_name.strip().lower()
+        if normalized == "code_review":
+            workflow_config = experiment_config.workflow_config
+            return CodeReviewWorkflow(config=workflow_config)
+        msg = f"Unsupported workflow '{workflow_name}'."
+        raise ExperimentError(msg)
+
+    return ExperimentExecutor(resolver, evaluator)
+
+
+def load_run(path: Path) -> ExperimentRun:
+    """Load a persisted ExperimentRun from disk."""
+
+    return ExperimentExecutor.load_run(path)
+
+
+async def _execute_experiment(
+    executor: ExperimentExecutor,
+    config: ExperimentConfig,
+    *,
+    output_path: Path,
+    resume_from: Optional[ExperimentRun],
+    extra_instructions: Optional[str],
+) -> ExperimentRun:
+    """Execute an experiment with progress reporting."""
+
+    logger.info("Executing experiment '%s' via CLI", config.name)
+    total_tests = _determine_total_tests(config, resume_from)
+    progress = _progress_renderer()
+    task_id = progress.add_task("Running experiment", total=total_tests)
+
+    if resume_from is not None:
+        progress.update(task_id, completed=len(resume_from.results))
+
+    def _on_progress(completed: int, total: int) -> None:
+        progress.update(task_id, completed=completed, total=total)
+
+    with progress:
+        run = await executor.run(
+            config,
+            resume_from=resume_from,
+            progress_callback=_on_progress,
+            persist_path=output_path,
+            extra_instructions=extra_instructions,
+        )
+
+    logger.info(
+        "Experiment '%s' completed with status %s after %s results",
+        config.name,
+        run.status,
+        len(run.results),
+    )
+    return run
+
+
+def _determine_total_tests(config: ExperimentConfig, resume_from: Optional[ExperimentRun]) -> int:
+    if resume_from is not None:
+        return len(resume_from.test_configurations)
+    test_configs = generate_test_configs(config)
+    return len(test_configs)
+
+
+def _progress_renderer() -> Progress:
+    return Progress(
+        SpinnerColumn(),
+        TextColumn("{task.description}"),
+        BarColumn(),
+        TextColumn("{task.completed}/{task.total}"),
+        TimeElapsedColumn(),
+        console=console,
+    )
+
+
+def _handle_dry_run(config: ExperimentConfig) -> None:
+    console.print("[cyan]â€¢ Generating Taguchi L8 test configurations...[/]")
+    test_configs = generate_test_configs(config)
+    console.print(f"[green]âœ“ Generated {len(test_configs)} test configurations")
+    _render_test_configuration_table(config, test_configs)
+
+
+def _render_test_configuration_table(
+    config: ExperimentConfig, test_configs: Iterable[TestConfiguration]
+) -> None:
+    variable_names = [variable.name for variable in config.variables]
+    table = Table(title="Test Configurations")
+    table.add_column("Test #", justify="right")
+    for name in variable_names:
+        table.add_column(name)
+
+    for test_config in test_configs:
+        row = [str(test_config.test_number)]
+        for name in variable_names:
+            value = test_config.config_values.get(name)
+            row.append(str(value))
+        table.add_row(*row)
+
+    console.print(table)
+
+
+def _render_summary(run: ExperimentRun, output_path: Path) -> None:
+    results = run.results
+    if not results:
+        console.print("[yellow]No results recorded for this run.[/]")
+        return
+
+    utilities = [result.utility for result in results]
+    qualities = [result.quality_score.overall_score for result in results]
+    costs = [result.cost for result in results]
+    latencies = [result.latency for result in results]
+
+    best_result = max(results, key=lambda result: result.utility)
+
+    table = Table(title="Experiment Summary")
+    table.add_column("Metric")
+    table.add_column("Value", justify="right")
+
+    table.add_row("Tests completed", f"{len(results)}/{len(run.test_configurations)}")
+    table.add_row("Utility range", f"{min(utilities):.2f} â€“ {max(utilities):.2f}")
+    table.add_row("Quality range", f"{min(qualities):.2f} â€“ {max(qualities):.2f}")
+    table.add_row("Cost range", f"{min(costs):.4f} â€“ {max(costs):.4f}")
+    table.add_row("Latency range (ms)", f"{min(latencies):.1f} â€“ {max(latencies):.1f}")
+    table.add_row(
+        "Best test",
+        f"#{best_result.test_number} utility={best_result.utility:.2f}",
+    )
+
+    console.print(table)
+    console.print(
+        f"[cyan]Next steps:[/] tesseract analyze {output_path}"
+    )
+
+
+def _format_status_line(status: str) -> str:
+    mapping = {
+        "PENDING": ("ğŸ•’", "yellow"),
+        "RUNNING": ("ğŸš€", "cyan"),
+        "COMPLETED": ("âœ…", "green"),
+        "FAILED": ("âš ï¸", "red"),
+    }
+    icon, style = mapping.get(status.upper(), ("â”", "magenta"))
+    friendly = status.replace("_", " ").title()
+    return f"[{style}]{icon}[/] [bold {style}]{friendly}[/]"
+
+
+def _build_status_table(run: ExperimentRun) -> Table:
+    table = Table(title="Progress Overview")
+    table.add_column("Metric")
+    table.add_column("Value", justify="right")
+
+    total = len(run.test_configurations)
+    completed = len(run.results)
+    table.add_row("Completed tests", f"{completed}/{total}")
+    table.add_row("Started", _format_timestamp(run.started_at))
+    table.add_row("Completed", _format_timestamp(run.completed_at))
+
+    if run.results:
+        table.add_row("Last update", _format_timestamp(run.results[-1].timestamp))
+
+    if run.started_at:
+        if run.completed_at:
+            table.add_row(
+                "Duration",
+                _format_duration(run.completed_at - run.started_at),
+            )
+        elif run.status == "RUNNING":
+            elapsed = datetime.now(tz=run.started_at.tzinfo) - run.started_at
+            table.add_row("Elapsed", _format_duration(elapsed))
+
+    if run.baseline_quality is not None:
+        table.add_row("Baseline quality", f"{run.baseline_quality:.3f}")
+    if run.quality_improvement_pct is not None:
+        table.add_row("Quality improvement", f"{run.quality_improvement_pct:.2f}%")
+
+    if run.results:
+        utilities = [result.utility for result in run.results]
+        qualities = [result.quality_score.overall_score for result in run.results]
+        table.add_row(
+            "Utility range",
+            f"{min(utilities):.3f} â€“ {max(utilities):.3f}",
+        )
+        table.add_row(
+            "Quality range",
+            f"{min(qualities):.3f} â€“ {max(qualities):.3f}",
+        )
+
+    return table
+
+
+def _build_results_table(results: Sequence[TestResult]) -> Table:
+    table = Table(title="Recorded Results")
+    table.add_column("Test #", justify="right")
+    table.add_column("Quality", justify="right")
+    table.add_column("Cost (USD)", justify="right")
+    table.add_column("Latency (ms)", justify="right")
+    table.add_column("Utility", justify="right")
+    table.add_column("Timestamp")
+
+    for result in sorted(results, key=lambda item: item.test_number):
+        table.add_row(
+            str(result.test_number),
+            f"{result.quality_score.overall_score:.3f}",
+            f"{result.cost:.4f}",
+            f"{result.latency:.1f}",
+            f"{result.utility:.3f}",
+            _format_timestamp(result.timestamp),
+        )
+
+    return table
+
+
+def _pending_tests(run: ExperimentRun) -> List[int]:
+    completed_numbers = {result.test_number for result in run.results}
+    return [
+        config.test_number
+        for config in sorted(run.test_configurations, key=lambda item: item.test_number)
+        if config.test_number not in completed_numbers
+    ]
+
+
+def _format_timestamp(value: Optional[datetime]) -> str:
+    if value is None:
+        return "â€”"
+    if value.tzinfo is not None:
+        localized = value.astimezone()
+        return localized.strftime("%Y-%m-%d %H:%M:%S %Z")
+    return value.strftime("%Y-%m-%d %H:%M:%S")
+
+
+def _format_duration(delta: timedelta) -> str:
+    total_seconds = int(delta.total_seconds())
+    if total_seconds < 0:
+        total_seconds = 0
+    hours, remainder = divmod(total_seconds, 3600)
+    minutes, seconds = divmod(remainder, 60)
+    parts = []
+    if hours:
+        parts.append(f"{hours}h")
+    if minutes:
+        parts.append(f"{minutes}m")
+    parts.append(f"{seconds}s")
+    return " ".join(parts)
+
+
+def _build_analysis_payload(
+    run: ExperimentRun,
+    main_effects: MainEffects,
+    baseline_config: Mapping[str, object],
+    baseline_result: Optional[TestResult],
+    baseline_quality: Optional[float],
+    optimal_result: TestResult,
+    recommended_config: Mapping[str, object],
+    improvement: Optional[float],
+    comparison: Mapping[str, Mapping[str, object]],
+) -> Dict[str, Any]:
+    return {
+        "experiment_id": run.experiment_id,
+        "experiment_name": run.config.name,
+        "main_effects": [
+            {
+                "variable": name,
+                "avg_level_1": effect.avg_level_1,
+                "avg_level_2": effect.avg_level_2,
+                "effect_size": effect.effect_size,
+                "sum_of_squares": effect.sum_of_squares,
+                "contribution_pct": effect.contribution_pct,
+            }
+            for name, effect in main_effects.effects.items()
+        ],
+        "baseline": {
+            "test_number": baseline_result.test_number if baseline_result else None,
+            "config": dict(baseline_config),
+            "quality": baseline_quality,
+            "cost": baseline_result.cost if baseline_result else None,
+            "latency": baseline_result.latency if baseline_result else None,
+            "utility": baseline_result.utility if baseline_result else None,
+        },
+        "optimal_observed": {
+            "test_number": optimal_result.test_number,
+            "config": dict(optimal_result.config.config_values),
+            "quality": optimal_result.quality_score.overall_score,
+            "cost": optimal_result.cost,
+            "latency": optimal_result.latency,
+            "utility": optimal_result.utility,
+        },
+        "recommended_configuration": dict(recommended_config),
+        "quality_improvement_pct": improvement,
+        "comparison": {key: dict(value) for key, value in comparison.items()},
+    }
+
+
+def _render_main_effects_table(main_effects: MainEffects) -> None:
+    table = Table(title="Main Effects")
+    table.add_column("Variable")
+    table.add_column("Avg Level 1", justify="right")
+    table.add_column("Avg Level 2", justify="right")
+    table.add_column("Effect", justify="right")
+    table.add_column("Contribution %", justify="right")
+
+    for name, effect in main_effects.effects.items():
+        table.add_row(
+            name,
+            f"{effect.avg_level_1:.3f}",
+            f"{effect.avg_level_2:.3f}",
+            f"{effect.effect_size:.3f}",
+            f"{effect.contribution_pct:.1f}",
+        )
+
+    console.print(table)
+
+
+def _render_comparison_table(comparison: Mapping[str, Mapping[str, object]]) -> None:
+    table = Table(title="Configuration Comparison")
+    table.add_column("Variable")
+    table.add_column("Baseline")
+    table.add_column("Optimal")
+    table.add_column("Changed", justify="center")
+
+    for name, values in comparison.items():
+        baseline_value = values.get("baseline")
+        candidate_value = values.get("candidate")
+        changed = values.get("changed")
+        table.add_row(
+            name,
+            str(baseline_value),
+            str(candidate_value),
+            "âœ…" if changed else "â€”",
+        )
+
+    console.print(table)
+
+
+def _render_metrics_summary(
+    baseline_result: Optional[TestResult],
+    baseline_quality: Optional[float],
+    optimal_result: TestResult,
+    improvement: Optional[float],
+) -> None:
+    table = Table(title="Baseline vs Optimal Metrics")
+    table.add_column("Metric")
+    table.add_column("Baseline", justify="right")
+    table.add_column("Optimal", justify="right")
+
+    table.add_row(
+        "Quality",
+        _format_metric(baseline_quality),
+        f"{optimal_result.quality_score.overall_score:.3f}",
+    )
+    table.add_row(
+        "Cost (USD)",
+        _format_metric(baseline_result.cost if baseline_result else None, precision=4),
+        f"{optimal_result.cost:.4f}",
+    )
+    table.add_row(
+        "Latency (ms)",
+        _format_metric(baseline_result.latency if baseline_result else None, precision=1),
+        f"{optimal_result.latency:.1f}",
+    )
+    table.add_row(
+        "Utility",
+        _format_metric(baseline_result.utility if baseline_result else None),
+        f"{optimal_result.utility:.3f}",
+    )
+
+    console.print(table)
+
+    if improvement is None:
+        console.print("[yellow]Quality improvement could not be calculated.[/]")
+        return
+
+    if improvement < 0:
+        color = "red"
+    elif 10.0 <= improvement <= 30.0:
+        color = "green"
+    else:
+        color = "yellow"
+    console.print(f"[{color}]Quality improvement: {improvement:.2f}%[/]")
+
+
+def _render_markdown(payload: Mapping[str, Any]) -> str:
+    lines = [
+        f"# Experiment Analysis: {payload['experiment_id']}",
+        "",
+        "## Main Effects",
+        "| Variable | Avg Level 1 | Avg Level 2 | Effect | Contribution % |",
+        "| --- | ---: | ---: | ---: | ---: |",
+    ]
+    for effect in payload["main_effects"]:
+        lines.append(
+            "| {variable} | {avg_level_1:.3f} | {avg_level_2:.3f} | {effect_size:.3f} | {contribution_pct:.1f} |".format(
+                **effect
+            )
+        )
+
+    lines.extend(
+        [
+            "",
+            "## Baseline vs Optimal",
+            "| Metric | Baseline | Optimal |",
+            "| --- | ---: | ---: |",
+        ]
+    )
+
+    baseline = payload["baseline"]
+    optimal = payload["optimal_observed"]
+    lines.append(
+        f"| Quality | {_format_markdown_value(baseline['quality'])} | {optimal['quality']:.3f} |"
+    )
+    lines.append(
+        f"| Cost (USD) | {_format_markdown_value(baseline['cost'], precision=4)} | {optimal['cost']:.4f} |"
+    )
+    lines.append(
+        f"| Latency (ms) | {_format_markdown_value(baseline['latency'], precision=1)} | {optimal['latency']:.1f} |"
+    )
+    lines.append(
+        f"| Utility | {_format_markdown_value(baseline['utility'])} | {optimal['utility']:.3f} |"
+    )
+
+    lines.extend(
+        [
+            "",
+            "### Configuration Changes",
+            "| Variable | Baseline | Optimal | Changed |",
+            "| --- | --- | --- | --- |",
+        ]
+    )
+
+    for name, values in payload["comparison"].items():
+        changed = "Yes" if values.get("changed") else "No"
+        lines.append(
+            f"| {name} | {values.get('baseline')} | {values.get('candidate')} | {changed} |"
+        )
+
+    improvement = payload.get("quality_improvement_pct")
+    lines.append("")
+    if improvement is not None:
+        lines.append(f"Quality improvement: {improvement:.2f}%")
+    else:
+        lines.append("Quality improvement: N/A")
+
+    lines.extend(
+        [
+            "",
+            "### Recommended Configuration",
+            "```json",
+            json.dumps(payload["recommended_configuration"], indent=2),
+            "```",
+        ]
+    )
+
+    return "\n".join(lines)
+
+
+def _format_metric(value: Optional[float], *, precision: int = 3) -> str:
+    if value is None:
+        return "N/A"
+    return f"{value:.{precision}f}"
+
+
+def _format_markdown_value(value: Optional[float], *, precision: int = 3) -> str:
+    if value is None:
+        return "N/A"
+    return f"{value:.{precision}f}"
+
+
+def _render_configuration_error(exc: ConfigurationError) -> None:
+    console.print("[bold red]âœ— Error:[/] Invalid experiment configuration.")
+    if getattr(exc, "details", None):
+        for detail in exc.details:
+            console.print(f"  [red]-[/] {detail}")
+    else:
+        console.print(f"  [red]-[/] {exc}")
+    _render_recovery_tips(
+        [
+            "Run with --dry-run to validate fixes without executing the workflow.",
+            "Cross-check variable names and workflow configuration against the specification.",
+        ]
+    )
+
+
+def _render_execution_error(exc: Exception, output_path: Path) -> None:
+    console.print(f"[bold red]âœ— Experiment execution failed:[/] {exc}")
+    tips = [
+        "Inspect the saved results file for partial progress.",
+        "Resume the run with --resume once issues are resolved.",
+    ]
+    if output_path.exists():
+        console.print(f"[yellow]! Partial results available at:[/] {output_path}")
+    else:
+        tips.append("Ensure the output directory exists and is writable.")
+    _render_recovery_tips(tips)
+
+
+def _render_cache_error(exc: CacheError) -> None:
+    console.print(f"[bold red]âœ— Cache error:[/] {exc}")
+    _render_recovery_tips(
+        [
+            "Verify the cache directory path and permissions.",
+            "Retry without --use-cache to bypass cached responses.",
+        ]
+    )
+
+
+def _render_recovery_tips(tips: Sequence[str]) -> None:
+    if not tips:
+        return
+    console.print("[yellow]Suggested next steps:[/]")
+    for tip in tips:
+        console.print(f"  [yellow]-[/] {tip}")
+
+
+__all__ = [
+    "app",
+    "analyze_results",
+    "build_evaluator",
+    "build_executor",
+    "configure_logging",
+    "load_run",
+    "run_experiment",
+    "status_experiment",
+    "validate_config",
+]
diff --git a/tesseract_flow/cli/main.py b/tesseract_flow/cli/main.py
new file mode 100644
index 0000000000000000000000000000000000000000..99cc6a138b5b5e2a962fa69f991caa9532fb97f4
--- /dev/null
+++ b/tesseract_flow/cli/main.py
@@ -0,0 +1,67 @@
+"""Typer application wiring for the TesseractFlow CLI."""
+from __future__ import annotations
+
+import logging
+from typing import Dict
+
+import typer
+
+from tesseract_flow import __version__
+
+from . import experiment, visualize
+
+app = typer.Typer(help="TesseractFlow command line interface")
+app.add_typer(experiment.app, name="experiment", help="Run and manage experiments")
+app.add_typer(visualize.app, name="visualize", help="Generate experiment visualizations")
+
+
+def _version_callback(value: bool) -> bool:
+    if value:
+        from rich.console import Console
+
+        console = Console()
+        console.print(f"TesseractFlow {__version__}")
+        raise typer.Exit()
+    return value
+
+
+@app.callback()
+def main_callback(
+    ctx: typer.Context,
+    log_level: str = typer.Option(
+        "info",
+        "--log-level",
+        "-l",
+        metavar="LEVEL",
+        help="Set global log level (critical, error, warning, info, debug).",
+    ),
+    version: bool = typer.Option(
+        False,
+        "--version",
+        callback=_version_callback,
+        is_eager=True,
+        help="Show the TesseractFlow version and exit.",
+    ),
+) -> None:
+    """Configure global CLI options before executing subcommands."""
+
+    del version  # handled by callback
+
+    ctx.ensure_object(dict)
+    obj: Dict[str, object] = ctx.obj
+    obj["log_level"] = log_level
+
+    level = log_level.upper()
+    if level not in {"CRITICAL", "ERROR", "WARNING", "INFO", "DEBUG"}:
+        raise typer.BadParameter("Log level must be one of critical, error, warning, info, debug.")
+
+    logging.basicConfig(level=getattr(logging, level), force=False)
+
+
+def main() -> None:
+    """Entrypoint executed by the console script."""
+
+    app()
+
+
+__all__ = ["app", "main"]
diff --git a/tesseract_flow/cli/visualize.py b/tesseract_flow/cli/visualize.py
new file mode 100644
index 0000000000000000000000000000000000000000..603551fe4db2111c2f9f0e089e63341de29056e9
--- /dev/null
+++ b/tesseract_flow/cli/visualize.py
@@ -0,0 +1,145 @@
+"""Visualization-related CLI commands."""
+from __future__ import annotations
+
+from pathlib import Path
+from typing import Optional
+
+import typer
+from rich.console import Console
+
+from tesseract_flow.optimization.pareto import ParetoFrontier, ParetoPoint
+
+from .experiment import console as shared_console, load_run
+
+app = typer.Typer()
+
+console: Console = shared_console
+
+_CONFIG_ERROR_EXIT = 1
+_VISUALIZATION_ERROR_EXIT = 2
+
+
+@app.command("pareto")
+def visualize_pareto(
+    results_file: Path = typer.Argument(
+        ..., exists=True, readable=True, help="Experiment results JSON file."
+    ),
+    output: Optional[Path] = typer.Option(
+        None,
+        "--output",
+        "-o",
+        help="Destination image file (PNG, SVG, or PDF).",
+    ),
+    x_axis: str = typer.Option("cost", "--x-axis", help="Metric for X-axis (cost or latency)."),
+    y_axis: str = typer.Option("quality", "--y-axis", help="Metric for Y-axis (quality or utility)."),
+    interactive: bool = typer.Option(False, "--interactive", help="Show interactive plot window."),
+    budget: Optional[float] = typer.Option(
+        None,
+        "--budget",
+        help="Highlight configurations within the provided budget threshold.",
+    ),
+) -> None:
+    """Generate a Pareto frontier visualization from experiment results."""
+
+    try:
+        run = load_run(results_file)
+    except Exception as exc:  # noqa: BLE001 - user-facing CLI error handling
+        console.print(f"[bold red]âœ— Error:[/] Failed to load results: {exc}")
+        raise typer.Exit(code=_CONFIG_ERROR_EXIT) from exc
+
+    if not run.results:
+        console.print("[bold red]âœ— Error:[/] The results file contains no test executions.")
+        raise typer.Exit(code=_CONFIG_ERROR_EXIT)
+
+    axis_x = x_axis.strip().lower()
+    axis_y = y_axis.strip().lower()
+
+    try:
+        frontier = ParetoFrontier.compute(
+            run.results,
+            experiment_id=run.experiment_id,
+            x_axis=axis_x,
+            y_axis=axis_y,
+        )
+    except ValueError as exc:
+        console.print(f"[bold red]âœ— Error:[/] {exc}")
+        raise typer.Exit(code=_CONFIG_ERROR_EXIT) from exc
+
+    console.print(f"[green]âœ“ Loaded {len(run.results)} test results")
+    console.print(
+        f"[green]âœ“ Computed Pareto frontier:[/] {len(frontier.optimal_points)} optimal points"
+    )
+
+    destination = output or _default_output_path(results_file, axis_x, axis_y)
+
+    try:
+        image_path = frontier.visualize(
+            destination,
+            show=interactive,
+            budget_threshold=budget,
+            title=f"Pareto Frontier for {run.config.name}",
+        )
+    except Exception as exc:  # noqa: BLE001 - ensure user-friendly CLI errors
+        console.print(f"[bold red]âœ— Error:[/] Failed to generate visualization: {exc}")
+        raise typer.Exit(code=_VISUALIZATION_ERROR_EXIT) from exc
+
+    console.print(f"[green]âœ“ Generated visualization:[/] {image_path}")
+
+    _render_optimal_points(frontier)
+
+    if budget is not None:
+        _render_budget_recommendation(frontier, budget)
+
+
+def _default_output_path(results_path: Path, axis_x: str, axis_y: str) -> Path:
+    safe_stem = results_path.stem
+    return results_path.with_name(f"{safe_stem}_{axis_y}_vs_{axis_x}_pareto.png")
+
+
+def _render_optimal_points(frontier: ParetoFrontier) -> None:
+    console.print("\n[bold]Pareto-optimal configurations:[/]")
+    for point in sorted(frontier.optimal_points, key=lambda item: item.test_number):
+        console.print("  " + _format_point(point, frontier.x_axis, frontier.y_axis))
+
+
+def _render_budget_recommendation(frontier: ParetoFrontier, budget: float) -> None:
+    if budget < 0:
+        console.print("[yellow]âš  Budget threshold must be non-negative; ignoring value.[/]")
+        return
+
+    candidate = frontier.best_within_budget(budget)
+    if candidate is None:
+        console.print(
+            f"[yellow]âš  No Pareto-optimal configurations within budget â‰¤ {budget:.3f}.[/]"
+        )
+        return
+
+    console.print(
+        "\n[cyan]Recommendation:[/] "
+        + _format_point(candidate, frontier.x_axis, frontier.y_axis)
+    )
+
+
+def _format_point(point: ParetoPoint, x_axis: str, y_axis: str) -> str:
+    axis_x_value = _metric_value(point, x_axis)
+    axis_y_value = _metric_value(point, y_axis)
+    latency_sec = point.latency / 1000.0
+    return (
+        f"Test #{point.test_number}: {y_axis.title()}={axis_y_value:.3f}, "
+        f"{x_axis.title()}={axis_x_value:.4f}, latency={latency_sec:.2f}s"
+    )
+
+
+def _metric_value(point: ParetoPoint, axis: str) -> float:
+    if axis == "cost":
+        return point.cost
+    if axis == "latency":
+        return point.latency
+    if axis == "quality":
+        return point.quality
+    if axis == "utility":
+        return point.utility
+    raise ValueError(f"Unsupported axis '{axis}'.")
+
+
+__all__ = ["app"]
diff --git a/tesseract_flow/core/__init__.py b/tesseract_flow/core/__init__.py
index 4afc5273c8b1234a96fbb80e45a67de5475e4f02..a356c78de5252655e254598b7575e0642449d989 100644
--- a/tesseract_flow/core/__init__.py
+++ b/tesseract_flow/core/__init__.py
@@ -1,17 +1,31 @@
 """Core workflow components."""
 
 from tesseract_flow.core.base_workflow import BaseWorkflowService
-from tesseract_flow.core.config import WorkflowConfig
+from tesseract_flow.core.config import (
+    ExperimentConfig,
+    ExperimentMetadata,
+    ExperimentRun,
+    TestConfiguration,
+    TestResult,
+    UtilityWeights,
+    WorkflowConfig,
+)
 from tesseract_flow.core.strategies import (
     GENERATION_STRATEGIES,
     get_strategy,
     register_strategy,
 )
 
 __all__ = [
     "BaseWorkflowService",
+    "ExperimentConfig",
+    "ExperimentMetadata",
+    "ExperimentRun",
+    "TestConfiguration",
+    "TestResult",
+    "UtilityWeights",
     "WorkflowConfig",
     "GENERATION_STRATEGIES",
     "get_strategy",
     "register_strategy",
 ]
diff --git a/tesseract_flow/core/base_workflow.py b/tesseract_flow/core/base_workflow.py
new file mode 100644
index 0000000000000000000000000000000000000000..4c2f3218b0f535169daf1b5aab67b28b96b9637a
--- /dev/null
+++ b/tesseract_flow/core/base_workflow.py
@@ -0,0 +1,88 @@
+"""Base workflow abstraction built on LangGraph."""
+from __future__ import annotations
+
+import logging
+from abc import ABC, abstractmethod
+from datetime import datetime, timezone
+from time import perf_counter
+from typing import Any, Generic, Optional, TypeVar
+
+from langgraph.graph import StateGraph
+from pydantic import BaseModel
+
+from .config import WorkflowConfig
+from .exceptions import WorkflowExecutionError
+
+TInput = TypeVar("TInput", bound=BaseModel)
+TOutput = TypeVar("TOutput", bound=BaseModel)
+
+logger = logging.getLogger(__name__)
+
+
+class BaseWorkflowService(ABC, Generic[TInput, TOutput]):
+    """Base class for workflow services orchestrated via LangGraph."""
+
+    def __init__(self, config: Optional[WorkflowConfig] = None) -> None:
+        self.config = config or WorkflowConfig()
+        self._compiled_graph: Optional[Any] = None
+        self._last_run_metadata: dict[str, Any] | None = None
+
+    @abstractmethod
+    def _build_workflow(self) -> StateGraph:
+        """Construct the LangGraph state graph for the workflow."""
+
+    @abstractmethod
+    def _validate_output(self, result: Any) -> TOutput:
+        """Validate and coerce the workflow result into the output schema."""
+
+    def _compile_workflow(self) -> Any:
+        if self._compiled_graph is None:
+            graph = self._build_workflow()
+            logger.debug("Compiling workflow %s", self.__class__.__name__)
+            self._compiled_graph = graph.compile()
+        return self._compiled_graph
+
+    def run(self, input_data: TInput) -> TOutput:
+        """Execute the workflow synchronously and return validated output."""
+
+        compiled = self._compile_workflow()
+        started_at = datetime.now(tz=timezone.utc)
+        start_perf = perf_counter()
+        payload = input_data.model_dump(mode="python")
+
+        logger.debug(
+            "Executing workflow %s with payload keys: %s",
+            self.__class__.__name__,
+            ", ".join(sorted(payload.keys())),
+        )
+        try:
+            result = compiled.invoke(payload)
+        except Exception as exc:  # pragma: no cover - defensive guard
+            raise WorkflowExecutionError("Workflow execution failed") from exc
+
+        duration = perf_counter() - start_perf
+        completed_at = datetime.now(tz=timezone.utc)
+
+        output = self._validate_output(result)
+        self._last_run_metadata = {
+            "started_at": started_at,
+            "completed_at": completed_at,
+            "duration_seconds": duration,
+        }
+        logger.debug(
+            "Workflow %s completed in %.2f seconds", self.__class__.__name__, duration
+        )
+        return output
+
+    @property
+    def last_run_metadata(self) -> dict[str, Any] | None:
+        """Metadata captured from the most recent execution."""
+
+        return self._last_run_metadata
+
+    def reset(self) -> None:
+        """Reset cached compiled graph to rebuild workflow on next run."""
+
+        logger.debug("Resetting compiled workflow for %s", self.__class__.__name__)
+        self._compiled_graph = None
+        self._last_run_metadata = None
diff --git a/tesseract_flow/core/config.py b/tesseract_flow/core/config.py
new file mode 100644
index 0000000000000000000000000000000000000000..9d3ed2a15945d3ec7d06c917117b40fd0ebe13f0
--- /dev/null
+++ b/tesseract_flow/core/config.py
@@ -0,0 +1,622 @@
+"""Core configuration models for experiments and workflows."""
+from __future__ import annotations
+
+import hashlib
+import json
+from datetime import datetime, timezone
+from importlib import metadata as importlib_metadata
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Mapping, Optional, Sequence
+
+import yaml
+from pydantic import (
+    BaseModel,
+    ConfigDict,
+    Field,
+    ValidationError,
+    ValidationInfo,
+    field_validator,
+    model_validator,
+)
+
+from tesseract_flow.core.exceptions import ConfigurationError
+
+from tesseract_flow.evaluation.metrics import QualityScore
+from tesseract_flow.optimization.utility import UtilityFunction
+
+from .types import ExperimentStatus, RubricDimension, UtilityWeights as UtilityWeightsBase
+
+
+class Variable(BaseModel):
+    """Represents a single experimental variable with two discrete levels."""
+
+    name: str = Field(..., min_length=1)
+    level_1: Any
+    level_2: Any
+
+    model_config = ConfigDict(frozen=True)
+
+    @field_validator("name")
+    @classmethod
+    def _validate_name(cls, value: str) -> str:
+        if not value:
+            msg = "Variable name must be non-empty."
+            raise ValueError(msg)
+        if value.strip() != value:
+            msg = "Variable name must not contain leading or trailing whitespace."
+            raise ValueError(msg)
+        if value.startswith("_"):
+            msg = "Variable name must not start with an underscore."
+            raise ValueError(msg)
+        if not value.replace("_", "").isalnum():
+            msg = "Variable name must contain only alphanumeric characters and underscores."
+            raise ValueError(msg)
+        return value
+
+    @model_validator(mode="after")
+    def _validate_levels(self) -> "Variable":
+        if self.level_1 == self.level_2:
+            msg = "Variable levels must be distinct."
+            raise ValueError(msg)
+        if type(self.level_1) is not type(self.level_2):
+            msg = "Variable levels must share the same type."
+            raise ValueError(msg)
+        return self
+
+
+class UtilityWeights(UtilityWeightsBase):
+    """Utility weight configuration with additional validation."""
+
+    @model_validator(mode="after")
+    def _ensure_positive_total(self) -> "UtilityWeights":
+        if self.quality == 0.0 and self.cost == 0.0 and self.time == 0.0:
+            msg = "At least one utility weight must be greater than zero."
+            raise ValueError(msg)
+        return self
+
+
+class WorkflowConfig(BaseModel):
+    """Workflow-specific configuration details."""
+
+    rubric: Dict[str, RubricDimension] = Field(default_factory=dict)
+    sample_code_path: Optional[str] = None
+
+    model_config = ConfigDict(extra="allow")
+
+    @field_validator("sample_code_path")
+    @classmethod
+    def _validate_sample_code_path(cls, value: Optional[str]) -> Optional[str]:
+        if value is None:
+            return value
+        stripped = value.strip()
+        if not stripped:
+            msg = "sample_code_path must not be empty if provided."
+            raise ValueError(msg)
+        return stripped
+
+
+class ExperimentConfig(BaseModel):
+    """Configuration for executing a Taguchi L8 experiment."""
+
+    name: str
+    workflow: str
+    variables: List[Variable]
+    utility_weights: UtilityWeights = Field(default_factory=UtilityWeights)
+    workflow_config: WorkflowConfig | Dict[str, Any] = Field(default_factory=WorkflowConfig)
+    seed: Optional[int] = Field(default=None, ge=0)
+
+    model_config = ConfigDict(validate_assignment=True)
+
+    @field_validator("name", "workflow")
+    @classmethod
+    def _validate_identifier(cls, value: str) -> str:
+        if not value:
+            msg = "Experiment identifiers must be non-empty."
+            raise ValueError(msg)
+        if value.strip() != value:
+            msg = "Experiment identifiers cannot contain leading or trailing whitespace."
+            raise ValueError(msg)
+        return value
+
+    @model_validator(mode="after")
+    def _validate_variables(self) -> "ExperimentConfig":
+        count = len(self.variables)
+        if count < 4 or count > 7:
+            msg = "ExperimentConfig requires between 4 and 7 variables."
+            raise ValueError(msg)
+
+        names = [variable.name for variable in self.variables]
+        if len(set(names)) != count:
+            msg = "Variable names must be unique within an experiment."
+            raise ValueError(msg)
+        return self
+
+    @model_validator(mode="before")
+    @classmethod
+    def _coerce_workflow_config(cls, data: Dict[str, Any]) -> Dict[str, Any]:
+        workflow_config = data.get("workflow_config")
+        if workflow_config is None:
+            data["workflow_config"] = WorkflowConfig()
+        elif not isinstance(workflow_config, WorkflowConfig):
+            data["workflow_config"] = WorkflowConfig.model_validate(workflow_config)
+        return data
+
+    @classmethod
+    def from_yaml(cls, path: Path | str) -> "ExperimentConfig":
+        """Load an experiment configuration from a YAML file."""
+
+        file_path = Path(path)
+        try:
+            text = file_path.read_text(encoding="utf-8")
+        except OSError:
+            # Surface file issues to caller for contextual handling.
+            raise
+
+        try:
+            data = yaml.safe_load(text)
+        except yaml.YAMLError as exc:
+            details = [_format_yaml_error(exc)] if str(exc).strip() else []
+            message = f"Failed to parse experiment YAML at {file_path}."
+            raise ConfigurationError(message, details=details) from exc
+
+        if not isinstance(data, dict):
+            message = (
+                f"Experiment configuration in {file_path} must be a mapping of keys to values."
+            )
+            raise ConfigurationError(message)
+
+        try:
+            return cls.model_validate(data)
+        except ValidationError as exc:
+            message = f"Experiment configuration in {file_path} is invalid."
+            raise ConfigurationError(message, details=_format_validation_errors(exc)) from exc
+
+    def to_yaml(self, path: Path | str) -> Path:
+        """Persist the configuration to a YAML file."""
+
+        file_path = Path(path)
+        with file_path.open("w", encoding="utf-8") as handle:
+            yaml.safe_dump(self.model_dump(mode="json"), handle, sort_keys=False)
+        return file_path
+
+
+class ExperimentMetadata(BaseModel):
+    """Metadata describing experiment reproducibility context."""
+
+    config_hash: str
+    created_at: datetime = Field(default_factory=lambda: datetime.now(tz=timezone.utc))
+    dependencies: Dict[str, str] = Field(default_factory=dict)
+    non_deterministic_sources: List[str] = Field(default_factory=list)
+    seed: Optional[int] = None
+
+    model_config = ConfigDict(frozen=True)
+
+    @field_validator("config_hash")
+    @classmethod
+    def _validate_config_hash(cls, value: str) -> str:
+        stripped = value.strip()
+        if not stripped:
+            msg = "config_hash must be a non-empty string."
+            raise ValueError(msg)
+        return stripped
+
+    @model_validator(mode="before")
+    @classmethod
+    def _coerce_dependencies(cls, data: Dict[str, Any]) -> Dict[str, Any]:
+        dependencies = data.get("dependencies") or {}
+        if not isinstance(dependencies, Mapping):
+            msg = "dependencies must be provided as a mapping of package names to versions."
+            raise ValueError(msg)
+        data["dependencies"] = {str(name): str(version) for name, version in dependencies.items()}
+
+        sources = data.get("non_deterministic_sources") or []
+        if not isinstance(sources, Sequence) or isinstance(sources, (str, bytes)):
+            msg = "non_deterministic_sources must be a sequence of strings."
+            raise ValueError(msg)
+        normalized_sources = sorted({str(source).strip() for source in sources if str(source).strip()})
+        data["non_deterministic_sources"] = normalized_sources
+        return data
+
+    @classmethod
+    def from_config(
+        cls,
+        config: "ExperimentConfig",
+        *,
+        dependencies: Optional[Mapping[str, str]] = None,
+        non_deterministic_sources: Optional[Sequence[str]] = None,
+    ) -> "ExperimentMetadata":
+        """Create metadata derived from an :class:`ExperimentConfig`."""
+
+        payload = config.model_dump(mode="json")
+        serialized = json.dumps(payload, sort_keys=True, separators=(",", ":"))
+        config_hash = hashlib.sha256(serialized.encode("utf-8")).hexdigest()
+        detected_dependencies = {
+            str(name): str(version)
+            for name, version in (dependencies or {}).items()
+            if str(name).strip() and str(version).strip()
+        }
+
+        if not detected_dependencies:
+            detected_dependencies = cls._default_dependency_versions()
+
+        sources = list(non_deterministic_sources or ["llm_sampling"])
+        return cls(
+            config_hash=config_hash,
+            dependencies=detected_dependencies,
+            non_deterministic_sources=sources,
+            seed=config.seed,
+        )
+
+    @staticmethod
+    def _default_dependency_versions() -> Dict[str, str]:
+        packages = ("pydantic", "langgraph", "litellm")
+        versions: Dict[str, str] = {}
+        for package in packages:
+            try:
+                versions[package] = importlib_metadata.version(package)
+            except importlib_metadata.PackageNotFoundError:  # pragma: no cover - optional deps
+                continue
+        return versions
+
+
+class TestConfiguration(BaseModel):
+    """Represents a single test generated from the Taguchi L8 array."""
+
+    test_number: int = Field(..., ge=1, le=8)
+    config_values: Dict[str, Any]
+    workflow: str
+
+    model_config = ConfigDict(frozen=True)
+
+    @field_validator("workflow")
+    @classmethod
+    def _validate_workflow(cls, value: str) -> str:
+        if not value:
+            msg = "Workflow identifier must be non-empty."
+            raise ValueError(msg)
+        if value.strip() != value:
+            msg = "Workflow identifier cannot contain leading or trailing whitespace."
+            raise ValueError(msg)
+        return value
+
+    @model_validator(mode="after")
+    def _validate_values(self, info: ValidationInfo) -> "TestConfiguration":
+        variable_levels = (info.context or {}).get("variable_levels")
+        if not variable_levels:
+            return self
+
+        expected_names = set(variable_levels)
+        provided_names = set(self.config_values)
+        if provided_names != expected_names:
+            missing = expected_names - provided_names
+            extra = provided_names - expected_names
+            details = []
+            if missing:
+                details.append(f"missing {sorted(missing)}")
+            if extra:
+                details.append(f"unexpected {sorted(extra)}")
+            detail_msg = ", ".join(details)
+            msg = f"Config values must match experiment variables ({detail_msg})."
+            raise ValueError(msg)
+
+        for name, value in self.config_values.items():
+            allowed_values = variable_levels[name]
+            if value not in allowed_values:
+                msg = (
+                    f"Config value for variable '{name}' must be one of the defined levels."
+                )
+                raise ValueError(msg)
+
+        return self
+
+
+class TestResult(BaseModel):
+    """Result from executing a single test configuration."""
+
+    test_number: int = Field(..., ge=1, le=8)
+    config: TestConfiguration
+    quality_score: QualityScore
+    cost: float = Field(..., ge=0.0)
+    latency: float = Field(..., ge=0.0)
+    utility: float = 0.0
+    workflow_output: str = Field(default="")
+    metadata: Dict[str, Any] = Field(default_factory=dict)
+    timestamp: datetime = Field(default_factory=lambda: datetime.now(tz=timezone.utc))
+
+    model_config = ConfigDict(validate_assignment=True)
+
+    @field_validator("workflow_output")
+    @classmethod
+    def _normalize_output(cls, value: str) -> str:
+        return value.strip()
+
+    def with_utility(self, utility: float) -> "TestResult":
+        """Return a copy of the result with an updated utility score."""
+
+        return self.model_copy(update={"utility": utility})
+
+    def with_metadata(self, **metadata: Any) -> "TestResult":
+        """Return a copy of the result with merged metadata."""
+
+        merged = {**self.metadata, **metadata}
+        return self.model_copy(update={"metadata": merged})
+
+
+class ExperimentRun(BaseModel):
+    """Tracks execution progress and results for a Taguchi L8 experiment."""
+
+    experiment_id: str
+    config: ExperimentConfig
+    test_configurations: List[TestConfiguration]
+    results: List[TestResult] = Field(default_factory=list)
+    status: ExperimentStatus = "PENDING"
+    started_at: Optional[datetime] = None
+    completed_at: Optional[datetime] = None
+    error: Optional[str] = None
+    experiment_metadata: Optional[ExperimentMetadata] = None
+    metadata: Dict[str, Any] = Field(default_factory=dict)
+    baseline_test_number: int = Field(default=1, ge=1, le=8)
+    baseline_config: Optional[TestConfiguration] = None
+    baseline_result: Optional[TestResult] = None
+    baseline_quality: Optional[float] = Field(default=None, ge=0.0, le=1.0)
+    quality_improvement_pct: Optional[float] = None
+
+    model_config = ConfigDict(validate_assignment=True)
+
+    @field_validator("experiment_id")
+    @classmethod
+    def _validate_experiment_id(cls, value: str) -> str:
+        if not value:
+            msg = "experiment_id must be non-empty."
+            raise ValueError(msg)
+        stripped = value.strip()
+        if stripped != value:
+            msg = "experiment_id cannot contain leading or trailing whitespace."
+            raise ValueError(msg)
+        return value
+
+    @model_validator(mode="after")
+    def _validate_state(self) -> "ExperimentRun":
+        if len(self.test_configurations) != 8:
+            msg = "Exactly 8 test configurations are required for an L8 experiment."
+            raise ValueError(msg)
+
+        test_numbers = [config.test_number for config in self.test_configurations]
+        if len(set(test_numbers)) != len(test_numbers):
+            msg = "Test configuration numbers must be unique."
+            raise ValueError(msg)
+
+        if len(self.results) > len(self.test_configurations):
+            msg = "Result count cannot exceed generated test configurations."
+            raise ValueError(msg)
+
+        if self.status == "PENDING":
+            if self.results:
+                msg = "Pending experiments cannot contain results."
+                raise ValueError(msg)
+            if self.started_at is not None:
+                msg = "Pending experiments cannot define a start timestamp."
+                raise ValueError(msg)
+            if self.completed_at is not None:
+                msg = "Pending experiments cannot define a completion timestamp."
+                raise ValueError(msg)
+            if self.error is not None:
+                msg = "Pending experiments cannot contain errors."
+                raise ValueError(msg)
+
+        if self.status == "RUNNING":
+            if not 0 <= len(self.results) <= len(self.test_configurations):
+                msg = "Running experiments cannot store more results than test configurations."
+                raise ValueError(msg)
+            if self.completed_at is not None:
+                msg = "Running experiments cannot define a completion timestamp."
+                raise ValueError(msg)
+            if self.error is not None:
+                msg = "Running experiments cannot define an error."
+                raise ValueError(msg)
+            if self.started_at is None:
+                msg = "Running experiments must define a start timestamp."
+                raise ValueError(msg)
+
+        if self.status == "COMPLETED":
+            if len(self.results) != len(self.test_configurations):
+                msg = "Completed experiments must contain results for all configurations."
+                raise ValueError(msg)
+            if self.completed_at is None:
+                msg = "Completed experiments must define a completion timestamp."
+                raise ValueError(msg)
+            if self.error is not None:
+                msg = "Completed experiments cannot contain an error."
+                raise ValueError(msg)
+            if self.started_at is None:
+                msg = "Completed experiments must define a start timestamp."
+                raise ValueError(msg)
+
+        if self.status == "FAILED":
+            if self.error is None:
+                msg = "Failed experiments must include an error message."
+                raise ValueError(msg)
+            if self.started_at is None:
+                msg = "Failed experiments must define a start timestamp."
+                raise ValueError(msg)
+
+        first_config = min(self.test_configurations, key=lambda item: item.test_number)
+
+        if self.baseline_test_number != first_config.test_number:
+            object.__setattr__(self, "baseline_test_number", first_config.test_number)
+
+        if self.baseline_config is None or self.baseline_config.test_number != first_config.test_number:
+            object.__setattr__(self, "baseline_config", first_config)
+
+        if self.baseline_result is not None and self.baseline_result.test_number != first_config.test_number:
+            object.__setattr__(self, "baseline_result", None)
+            object.__setattr__(self, "baseline_quality", None)
+
+        return self
+
+    def mark_running(self, *, started_at: Optional[datetime] = None) -> "ExperimentRun":
+        """Transition the experiment to RUNNING state."""
+
+        if self.status not in {"PENDING", "FAILED"}:
+            msg = "Experiment can only transition to RUNNING from PENDING or FAILED."
+            raise ValueError(msg)
+
+        timestamp = started_at or datetime.now(tz=timezone.utc)
+        return self.model_copy(
+            update={"status": "RUNNING", "started_at": timestamp, "error": None}
+        )
+
+    def mark_failed(self, error: str) -> "ExperimentRun":
+        """Transition the experiment to FAILED state."""
+
+        if self.status != "RUNNING":
+            msg = "Failed state can only be set from RUNNING."
+            raise ValueError(msg)
+        stripped = error.strip()
+        if not stripped:
+            msg = "Error message must be non-empty."
+            raise ValueError(msg)
+
+        return self.model_copy(update={"status": "FAILED", "error": stripped})
+
+    def mark_completed(self, *, completed_at: Optional[datetime] = None) -> "ExperimentRun":
+        """Transition the experiment to COMPLETED state."""
+
+        if self.status != "RUNNING":
+            msg = "Experiment must be RUNNING before completion."
+            raise ValueError(msg)
+        if len(self.results) != len(self.test_configurations):
+            msg = "Cannot complete experiment until all results are recorded."
+            raise ValueError(msg)
+
+        timestamp = completed_at or datetime.now(tz=timezone.utc)
+        baseline_result = self.baseline_result
+        if baseline_result is None:
+            baseline_result = next(
+                (item for item in self.results if item.test_number == self.baseline_test_number),
+                None,
+            )
+        baseline_quality = (
+            baseline_result.quality_score.overall_score if baseline_result else self.baseline_quality
+        )
+        improvement = self._calculate_improvement(self.results, baseline_quality)
+
+        return self.model_copy(
+            update={
+                "status": "COMPLETED",
+                "completed_at": timestamp,
+                "baseline_result": baseline_result,
+                "baseline_quality": baseline_quality,
+                "quality_improvement_pct": improvement,
+            }
+        )
+
+    def record_result(self, result: TestResult) -> "ExperimentRun":
+        """Record a new test result and recalculate utilities."""
+
+        if self.status != "RUNNING":
+            msg = "Results can only be recorded while experiment is RUNNING."
+            raise ValueError(msg)
+
+        if any(existing.test_number == result.test_number for existing in self.results):
+            msg = "Result for this test number has already been recorded."
+            raise ValueError(msg)
+
+        if len(self.results) >= len(self.test_configurations):
+            msg = "All test results have already been recorded."
+            raise ValueError(msg)
+
+        updated_results = [*self.results, result]
+        recalculated_results, metadata = self._recalculate_utilities(updated_results)
+
+        baseline_result = self.baseline_result
+        if baseline_result is None or result.test_number == self.baseline_test_number:
+            baseline_result = next(
+                (item for item in recalculated_results if item.test_number == self.baseline_test_number),
+                None,
+            )
+
+        baseline_quality = (
+            baseline_result.quality_score.overall_score if baseline_result else self.baseline_quality
+        )
+        improvement = self._calculate_improvement(recalculated_results, baseline_quality)
+
+        updates = {
+            "results": recalculated_results,
+            "metadata": metadata,
+            "baseline_result": baseline_result,
+            "baseline_quality": baseline_quality,
+            "quality_improvement_pct": improvement,
+        }
+        return self.model_copy(update=updates)
+
+    def _recalculate_utilities(
+        self, results: Iterable[TestResult]
+    ) -> tuple[List[TestResult], Dict[str, Any]]:
+        results_list = sorted(results, key=lambda item: item.test_number)
+        if not results_list:
+            return results_list, self.metadata
+
+        utility_fn = UtilityFunction(self.config.utility_weights)
+        costs = [result.cost for result in results_list]
+        latencies = [result.latency for result in results_list]
+        normalized_costs, cost_stats = UtilityFunction.normalize_metrics(costs)
+        normalized_latencies, latency_stats = UtilityFunction.normalize_metrics(latencies)
+
+        updated_results = []
+        for result, normalized_cost, normalized_latency in zip(
+            results_list, normalized_costs, normalized_latencies
+        ):
+            utility = utility_fn.compute(
+                quality=result.quality_score.overall_score,
+                cost=normalized_cost,
+                latency=normalized_latency,
+            )
+            updated_results.append(result.with_utility(utility))
+
+        metadata = {
+            **self.metadata,
+            "normalization": {
+                "cost": cost_stats,
+                "latency": latency_stats,
+            },
+        }
+        return updated_results, metadata
+
+    @staticmethod
+    def _calculate_improvement(
+        results: Sequence[TestResult], baseline_quality: Optional[float]
+    ) -> Optional[float]:
+        if baseline_quality is None or baseline_quality <= 0.0:
+            return None
+        if not results:
+            return None
+        best_quality = max(result.quality_score.overall_score for result in results)
+        return ((best_quality - baseline_quality) / baseline_quality) * 100.0
+
+
+def _format_validation_errors(exc: ValidationError) -> List[str]:
+    """Return human-friendly strings describing validation failures."""
+
+    details: List[str] = []
+    for error in exc.errors():
+        location = " â†’ ".join(str(segment) for segment in error.get("loc", ()))
+        message = error.get("msg") or "Invalid value."
+        if location:
+            details.append(f"{location}: {message}")
+        else:
+            details.append(message)
+    if not details:
+        details.append(str(exc))
+    return details
+
+
+def _format_yaml_error(error: yaml.YAMLError) -> str:
+    """Provide a concise YAML parsing error message for users."""
+
+    problem_mark = getattr(error, "problem_mark", None)
+    if problem_mark is not None:
+        return (
+            f"{getattr(error, 'problem', error)} (line {problem_mark.line + 1}, column {problem_mark.column + 1})"
+        )
+    return str(error)
diff --git a/tesseract_flow/core/exceptions.py b/tesseract_flow/core/exceptions.py
new file mode 100644
index 0000000000000000000000000000000000000000..1d455df70b677cc849451aee2df729892552b5a4
--- /dev/null
+++ b/tesseract_flow/core/exceptions.py
@@ -0,0 +1,38 @@
+"""Custom exception hierarchy for TesseractFlow."""
+from typing import Iterable, Optional
+
+
+class TesseractFlowError(Exception):
+    """Base exception for all TesseractFlow errors."""
+
+
+class ConfigurationError(TesseractFlowError):
+    """Raised when configuration validation fails."""
+
+    def __init__(self, message: str, *, details: Optional[Iterable[str]] = None) -> None:
+        super().__init__(message)
+        self.details = list(details or [])
+
+
+class ExperimentError(TesseractFlowError):
+    """Raised when experiment execution encounters an unrecoverable error."""
+
+
+class WorkflowError(TesseractFlowError):
+    """Raised when workflow execution fails."""
+
+
+class WorkflowExecutionError(WorkflowError):
+    """Specific failure during workflow execution lifecycle."""
+
+
+class VisualizationError(TesseractFlowError):
+    """Raised when Pareto or other visualization rendering fails."""
+
+
+class EvaluationError(TesseractFlowError):
+    """Raised when rubric-based evaluation fails."""
+
+
+class CacheError(TesseractFlowError):
+    """Raised when cache operations fail."""
diff --git a/tesseract_flow/core/strategies.py b/tesseract_flow/core/strategies.py
new file mode 100644
index 0000000000000000000000000000000000000000..aeddd840b4fc43494f296ec5b42cacba80dffcd2
--- /dev/null
+++ b/tesseract_flow/core/strategies.py
@@ -0,0 +1,70 @@
+"""Generation strategy protocol and registry."""
+from __future__ import annotations
+
+from typing import Any, Dict, Mapping, Protocol, runtime_checkable
+
+import litellm
+
+
+@runtime_checkable
+class GenerationStrategy(Protocol):
+    """Protocol for workflow generation strategies."""
+
+    async def generate(
+        self,
+        prompt: str,
+        *,
+        model: str,
+        config: Mapping[str, Any] | None = None,
+    ) -> str:
+        """Generate text for the given prompt using provided model settings."""
+
+
+class StandardStrategy:
+    """Default generation strategy using LiteLLM completion API."""
+
+    async def generate(
+        self,
+        prompt: str,
+        *,
+        model: str,
+        config: Mapping[str, Any] | None = None,
+    ) -> str:
+        parameters: Dict[str, Any] = {}
+        if config is not None:
+            parameters.update(config)
+        temperature = parameters.pop("temperature", 0.0)
+        response = await litellm.acompletion(
+            model=model,
+            messages=[{"role": "user", "content": prompt}],
+            temperature=temperature,
+            **parameters,
+        )
+        choices = response.get("choices", [])
+        if not choices:
+            return ""
+        message = choices[0].get("message", {})
+        content = message.get("content", "")
+        if isinstance(content, list):  # LiteLLM may return content blocks
+            content = "".join(str(block) for block in content)
+        return str(content).strip()
+
+
+GENERATION_STRATEGIES: Dict[str, GenerationStrategy] = {
+    "standard": StandardStrategy(),
+}
+
+
+def get_strategy(name: str) -> GenerationStrategy:
+    """Retrieve a generation strategy by name."""
+
+    try:
+        return GENERATION_STRATEGIES[name]
+    except KeyError as exc:  # pragma: no cover - defensive guard
+        raise ValueError(f"Unknown generation strategy: {name}") from exc
+
+
+def register_strategy(name: str, strategy: GenerationStrategy) -> None:
+    """Register a custom generation strategy."""
+
+    GENERATION_STRATEGIES[name] = strategy
diff --git a/tesseract_flow/core/types.py b/tesseract_flow/core/types.py
new file mode 100644
index 0000000000000000000000000000000000000000..9d525f99b71d6564622ba8af9c9eb77a9f371456
--- /dev/null
+++ b/tesseract_flow/core/types.py
@@ -0,0 +1,25 @@
+"""Core shared types for TesseractFlow."""
+from __future__ import annotations
+
+from typing import Literal, TypedDict
+
+from pydantic import BaseModel, ConfigDict, Field
+
+
+ExperimentStatus = Literal["PENDING", "RUNNING", "COMPLETED", "FAILED"]
+
+
+class RubricDimension(TypedDict):
+    """Structure describing a rubric dimension used during evaluation."""
+
+    description: str
+    scale: str
+
+
+class UtilityWeights(BaseModel):
+    """Weights applied to quality, cost, and time when computing utility."""
+
+    quality: float = Field(default=1.0, ge=0.0)
+    cost: float = Field(default=0.1, ge=0.0)
+    time: float = Field(default=0.05, ge=0.0)
+    model_config = ConfigDict(frozen=True)
diff --git a/tesseract_flow/evaluation/__init__.py b/tesseract_flow/evaluation/__init__.py
index 9830fc4e678bc69243d05b2b9321913f05a172c2..75a9271cba713b1a0e0ac5d9eade03d84983b06f 100644
--- a/tesseract_flow/evaluation/__init__.py
+++ b/tesseract_flow/evaluation/__init__.py
@@ -1,13 +1,14 @@
-"""Quality evaluation and multi-objective optimization."""
+"""Quality evaluation utilities for TesseractFlow."""
 
-from tesseract_flow.evaluation.evaluators import (
-    Evaluator,
-    QualityScore,
-    RubricEvaluator,
-)
+from .cache import CacheBackend, FileCacheBackend, build_cache_key
+from .metrics import DimensionScore, QualityScore
+from .rubric import RubricEvaluator
 
 __all__ = [
-    "Evaluator",
+    "CacheBackend",
+    "DimensionScore",
+    "FileCacheBackend",
     "QualityScore",
     "RubricEvaluator",
+    "build_cache_key",
 ]
diff --git a/tesseract_flow/evaluation/cache.py b/tesseract_flow/evaluation/cache.py
new file mode 100644
index 0000000000000000000000000000000000000000..9a0c4e367bc0dbb99eeae960739b2be4e93fd1da
--- /dev/null
+++ b/tesseract_flow/evaluation/cache.py
@@ -0,0 +1,95 @@
+"""Response caching utilities for evaluation workflows."""
+from __future__ import annotations
+
+import contextlib
+import json
+import os
+from dataclasses import dataclass
+from hashlib import sha256
+from pathlib import Path
+from typing import Optional, Protocol, runtime_checkable
+
+from tesseract_flow.core.exceptions import CacheError
+
+
+@runtime_checkable
+class CacheBackend(Protocol):
+    """Protocol describing the evaluation response cache contract."""
+
+    def get(self, key: str) -> Optional[str]:
+        """Return cached payload for *key* if present, otherwise ``None``."""
+
+    def set(self, key: str, value: str) -> None:
+        """Persist ``value`` for *key* so future lookups return the payload."""
+
+    def clear(self) -> None:
+        """Remove all cached payloads managed by the backend."""
+
+
+def build_cache_key(prompt: str, model: str, temperature: float) -> str:
+    """Return a deterministic cache key for the evaluator request payload."""
+
+    fingerprint = {
+        "prompt": prompt,
+        "model": model,
+        "temperature": round(float(temperature), 6),
+    }
+    serialized = json.dumps(fingerprint, sort_keys=True, separators=(",", ":"))
+    return sha256(serialized.encode("utf-8")).hexdigest()
+
+
+@dataclass(slots=True)
+class FileCacheBackend:
+    """Filesystem-backed cache storing one JSON file per request hash."""
+
+    cache_dir: Path
+    encoding: str = "utf-8"
+
+    def __post_init__(self) -> None:
+        self.cache_dir = Path(self.cache_dir)
+        try:
+            self.cache_dir.mkdir(parents=True, exist_ok=True)
+        except OSError as exc:  # pragma: no cover - defensive guard
+            msg = f"Failed to initialize cache directory '{self.cache_dir}'."
+            raise CacheError(msg) from exc
+
+    def get(self, key: str) -> Optional[str]:
+        path = self._path_for_key(key)
+        try:
+            with path.open("r", encoding=self.encoding) as handle:
+                return handle.read()
+        except FileNotFoundError:
+            return None
+        except OSError as exc:  # pragma: no cover - defensive guard
+            msg = f"Failed to read cache entry '{key}'."
+            raise CacheError(msg) from exc
+
+    def set(self, key: str, value: str) -> None:
+        path = self._path_for_key(key)
+        tmp_path = path.with_suffix(".tmp")
+        try:
+            with tmp_path.open("w", encoding=self.encoding) as handle:
+                handle.write(value)
+            os.replace(tmp_path, path)
+        except OSError as exc:  # pragma: no cover - defensive guard
+            msg = f"Failed to write cache entry '{key}'."
+            raise CacheError(msg) from exc
+        finally:
+            with contextlib.suppress(FileNotFoundError):
+                tmp_path.unlink()
+
+    def clear(self) -> None:
+        try:
+            for item in self.cache_dir.glob("*.json"):
+                item.unlink(missing_ok=True)
+        except OSError as exc:  # pragma: no cover - defensive guard
+            msg = f"Failed to clear cache directory '{self.cache_dir}'."
+            raise CacheError(msg) from exc
+
+    def _path_for_key(self, key: str) -> Path:
+        safe_key = key.strip()
+        if not safe_key:
+            msg = "Cache key must be a non-empty string."
+            raise CacheError(msg)
+        return self.cache_dir / f"{safe_key}.json"
+
diff --git a/tesseract_flow/evaluation/metrics.py b/tesseract_flow/evaluation/metrics.py
new file mode 100644
index 0000000000000000000000000000000000000000..4e1f3ce29355bad42d91e53e6f897aba4e9ed601
--- /dev/null
+++ b/tesseract_flow/evaluation/metrics.py
@@ -0,0 +1,77 @@
+"""Evaluation metrics and scoring models."""
+from __future__ import annotations
+
+from datetime import datetime, timezone
+from statistics import fmean
+from typing import Any, Dict, Mapping, Optional
+
+from pydantic import BaseModel, ConfigDict, Field, field_validator, model_validator
+
+
+class DimensionScore(BaseModel):
+    """Score and rationale for a single evaluation rubric dimension."""
+
+    score: float = Field(..., ge=0.0, le=1.0)
+    reasoning: Optional[str] = None
+
+    model_config = ConfigDict(frozen=True)
+
+    @field_validator("reasoning")
+    @classmethod
+    def _normalize_reasoning(cls, value: Optional[str]) -> Optional[str]:
+        if value is None:
+            return None
+        stripped = value.strip()
+        return stripped or None
+
+
+class QualityScore(BaseModel):
+    """Aggregated quality evaluation result across rubric dimensions."""
+
+    dimension_scores: Dict[str, DimensionScore]
+    overall_score: float = Field(default=0.0, ge=0.0, le=1.0)
+    evaluator_model: str
+    timestamp: datetime = Field(default_factory=lambda: datetime.now(tz=timezone.utc))
+    metadata: Dict[str, Any] = Field(default_factory=dict)
+
+    model_config = ConfigDict(validate_assignment=True)
+
+    @field_validator("dimension_scores")
+    @classmethod
+    def _validate_dimension_scores(
+        cls, value: Mapping[str, DimensionScore]
+    ) -> Dict[str, DimensionScore]:
+        if not value:
+            msg = "At least one dimension score must be provided."
+            raise ValueError(msg)
+
+        normalized: Dict[str, DimensionScore] = {}
+        for name, score in value.items():
+            key = name.strip()
+            if not key:
+                msg = "Dimension names must be non-empty strings."
+                raise ValueError(msg)
+            normalized[key] = score
+        return normalized
+
+    @field_validator("evaluator_model")
+    @classmethod
+    def _validate_model(cls, value: str) -> str:
+        stripped = value.strip()
+        if not stripped:
+            msg = "Evaluator model identifier must be non-empty."
+            raise ValueError(msg)
+        return stripped
+
+    @model_validator(mode="after")
+    def _synchronize_overall(self) -> "QualityScore":
+        scores = [dimension.score for dimension in self.dimension_scores.values()]
+        mean_score = float(fmean(scores))
+        object.__setattr__(self, "overall_score", mean_score)
+        return self
+
+    def with_metadata(self, **metadata: Any) -> "QualityScore":
+        """Return a copy of the score with additional metadata merged in."""
+
+        merged = {**self.metadata, **metadata}
+        return self.model_copy(update={"metadata": merged})
diff --git a/tesseract_flow/evaluation/rubric.py b/tesseract_flow/evaluation/rubric.py
new file mode 100644
index 0000000000000000000000000000000000000000..f8ed79e6a7f0aab53555f74973a9f2a618bf1231
--- /dev/null
+++ b/tesseract_flow/evaluation/rubric.py
@@ -0,0 +1,370 @@
+"""LLM-powered rubric evaluator for workflow outputs."""
+from __future__ import annotations
+
+import asyncio
+import json
+import logging
+import math
+import random
+from typing import Any, Dict, Iterable, Mapping, Optional
+
+import litellm
+
+from tesseract_flow.core.exceptions import EvaluationError
+from tesseract_flow.core.types import RubricDimension
+
+from .cache import CacheBackend, build_cache_key
+from .metrics import DimensionScore, QualityScore
+
+
+class RubricEvaluator:
+    """Evaluate workflow output quality using a rubric and LiteLLM."""
+
+    DEFAULT_MODEL = "anthropic/claude-3.5-sonnet"
+    DEFAULT_TEMPERATURE = 0.3
+    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {
+        "clarity": {
+            "description": "Is the output clear and understandable?",
+            "scale": "1-10 where 1=incomprehensible, 10=crystal clear",
+        },
+        "accuracy": {
+            "description": "Is the output factually accurate?",
+            "scale": "1-10 where 1=many errors, 10=fully accurate",
+        },
+        "completeness": {
+            "description": "Does the output address all requirements?",
+            "scale": "1-10 where 1=missing major parts, 10=comprehensive",
+        },
+        "usefulness": {
+            "description": "Is the output actionable and useful?",
+            "scale": "1-10 where 1=not useful, 10=highly actionable",
+        },
+    }
+
+    def __init__(
+        self,
+        *,
+        model: Optional[str] = None,
+        temperature: float = DEFAULT_TEMPERATURE,
+        logger: Optional[logging.Logger] = None,
+        cache: Optional[CacheBackend] = None,
+        use_cache: bool = False,
+        record_cache: bool = False,
+        max_retries: int = 3,
+        retry_base_delay: float = 1.0,
+    ) -> None:
+        self._model = (model or self.DEFAULT_MODEL).strip()
+        if not self._model:
+            msg = "Evaluator model identifier must be non-empty."
+            raise ValueError(msg)
+
+        self._temperature = self._validate_temperature(temperature)
+        self._logger = logger or logging.getLogger(__name__)
+        self._cache = cache
+        self._default_use_cache = bool(use_cache)
+        self._default_record_cache = bool(record_cache)
+        if max_retries < 1:
+            msg = "max_retries must be at least 1."
+            raise ValueError(msg)
+        if retry_base_delay <= 0:
+            msg = "retry_base_delay must be greater than zero."
+            raise ValueError(msg)
+        self._max_retries = max_retries
+        self._retry_base_delay = retry_base_delay
+
+    async def evaluate(
+        self,
+        workflow_output: str,
+        rubric: Optional[Dict[str, RubricDimension]] = None,
+        *,
+        model: Optional[str] = None,
+        temperature: Optional[float] = None,
+        extra_instructions: Optional[str] = None,
+        use_cache: Optional[bool] = None,
+        record_cache: Optional[bool] = None,
+        cache_key: Optional[str] = None,
+    ) -> QualityScore:
+        """Evaluate workflow output using the provided rubric."""
+
+        if not workflow_output or not workflow_output.strip():
+            msg = "Workflow output must be a non-empty string for evaluation."
+            raise EvaluationError(msg)
+
+        rubric_definition = rubric or self.DEFAULT_RUBRIC
+        selected_model = (model or self._model).strip()
+        if not selected_model:
+            msg = "Model identifier must be non-empty."
+            raise EvaluationError(msg)
+        selected_temperature = self._validate_temperature(
+            temperature if temperature is not None else self._temperature
+        )
+
+        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)
+        effective_record_cache = (
+            self._default_record_cache if record_cache is None else bool(record_cache)
+        )
+        cache_backend = self._cache
+        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None
+        if requested_cache and cache_backend is None:
+            msg = "Caching requested but no cache backend configured."
+            raise EvaluationError(msg)
+
+        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)
+        messages = [
+            {"role": "system", "content": self._system_prompt()},
+            {"role": "user", "content": prompt},
+        ]
+
+        resolved_cache_key = None
+        if cache_backend is not None:
+            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()
+            if not resolved_cache_key:
+                msg = "Cache key must be a non-empty string."
+                raise EvaluationError(msg)
+
+        cache_hit = False
+        recorded_cache = False
+        content: Optional[str] = None
+
+        if cache_backend is not None and effective_use_cache:
+            assert resolved_cache_key is not None
+            cached = cache_backend.get(resolved_cache_key)
+            if cached is not None:
+                cache_hit = True
+                content = cached
+                self._logger.debug("Cache hit for rubric evaluation with key %s", resolved_cache_key)
+
+        if content is None:
+            response = await self._request_with_retry(
+                messages=messages,
+                model=selected_model,
+                temperature=selected_temperature,
+            )
+
+            content = self._extract_response_content(response)
+
+            if cache_backend is not None and effective_record_cache:
+                assert resolved_cache_key is not None
+                cache_backend.set(resolved_cache_key, content)
+                recorded_cache = True
+                self._logger.debug(
+                    "Recorded rubric evaluation response to cache key %s", resolved_cache_key
+                )
+
+        assert content is not None  # for type-checkers
+        parsed_payload = self._load_json(content)
+        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)
+
+        quality = QualityScore(
+            dimension_scores=dimension_scores,
+            evaluator_model=selected_model,
+        )
+        metadata = {
+            "rubric": rubric_definition,
+            "raw_response": parsed_payload,
+            "temperature": selected_temperature,
+        }
+        if resolved_cache_key is not None:
+            metadata.update(
+                {
+                    "cache_key": resolved_cache_key,
+                    "cache_hit": cache_hit,
+                }
+            )
+            if recorded_cache:
+                metadata["cache_recorded"] = True
+        return quality.with_metadata(**metadata)
+
+    async def _request_with_retry(
+        self,
+        *,
+        messages: Iterable[Mapping[str, Any]],
+        model: str,
+        temperature: float,
+    ) -> Any:
+        """Call LiteLLM with exponential backoff and jitter."""
+
+        attempt = 1
+        payload = list(messages)
+        last_error: Optional[Exception] = None
+        while attempt <= self._max_retries:
+            try:
+                self._logger.debug(
+                    "Rubric evaluation attempt %s/%s with model %s", attempt, self._max_retries, model
+                )
+                return await litellm.acompletion(
+                    model=model,
+                    messages=payload,
+                    temperature=temperature,
+                    response_format={"type": "json_object"},
+                )
+            except Exception as exc:  # pragma: no cover - LiteLLM errors are external
+                last_error = exc
+                if attempt >= self._max_retries:
+                    break
+                delay = self._compute_retry_delay(attempt)
+                self._logger.warning(
+                    "Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...",
+                    attempt,
+                    exc,
+                    delay,
+                )
+                await asyncio.sleep(delay)
+                attempt += 1
+
+        assert last_error is not None  # for type-checkers
+        self._logger.exception(
+            "Rubric evaluation failed after %s attempts: %s", self._max_retries, last_error
+        )
+        raise EvaluationError(
+            f"LLM evaluation request failed after {self._max_retries} attempts."
+        ) from last_error
+
+    def _compute_retry_delay(self, attempt: int) -> float:
+        base_delay = self._retry_base_delay * (2 ** (attempt - 1))
+        jitter = random.uniform(0, base_delay * 0.25)
+        return base_delay + jitter
+
+    def _build_prompt(
+        self,
+        workflow_output: str,
+        rubric: Mapping[str, RubricDimension],
+        extra_instructions: Optional[str],
+    ) -> str:
+        rubric_text = self._format_rubric(rubric.items())
+        instructions = "\n" + extra_instructions.strip() if extra_instructions else ""
+        return (
+            "You are an impartial expert evaluator."
+            " Assess the workflow output using the rubric."
+            " Think step-by-step before scoring each dimension."
+            "\n\nOUTPUT TO EVALUATE:\n" + workflow_output.strip() +
+            "\n\nRUBRIC:\n" + rubric_text +
+            "\nINSTRUCTIONS:\n"
+            "1. Reason carefully about each dimension.\n"
+            "2. Provide concise reasoning referencing the rubric.\n"
+            "3. Assign a score following the specified scale.\n"
+            "4. Respond ONLY in JSON with numeric scores."
+            "\n\nJSON RESPONSE TEMPLATE:\n"
+            "{\n"
+            "  \"dimension_name\": {\n"
+            "    \"score\": <number>,\n"
+            "    \"reasoning\": \"<why you chose the score>\"\n"
+            "  }, ...\n"
+            "}"
+            f"{instructions}\n"
+        )
+
+    def _system_prompt(self) -> str:
+        return (
+            "You are a meticulous and unbiased reviewer."
+            " Provide honest assessments and avoid revealing deliberation summaries."
+        )
+
+    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:
+        lines = []
+        for name, metadata in dimensions:
+            lines.append(
+                f"- {name}: {metadata['description']} (Scale: {metadata['scale']})"
+            )
+        return "\n".join(lines)
+
+    def _extract_response_content(self, response: Any) -> str:
+        choices = None
+        if isinstance(response, Mapping):
+            choices = response.get("choices")
+        else:
+            choices = getattr(response, "choices", None)
+
+        if not choices:
+            msg = "Evaluator response did not include any choices."
+            raise EvaluationError(msg)
+
+        first_choice = choices[0]
+        message = (
+            first_choice.get("message")
+            if isinstance(first_choice, Mapping)
+            else getattr(first_choice, "message", None)
+        )
+        if message is None:
+            msg = "Evaluator response choice missing message content."
+            raise EvaluationError(msg)
+
+        content = (
+            message.get("content")
+            if isinstance(message, Mapping)
+            else getattr(message, "content", None)
+        )
+
+        if isinstance(content, list):
+            combined = "".join(part.get("text", "") for part in content if isinstance(part, Mapping))
+            content = combined
+
+        if not isinstance(content, str):
+            msg = "Evaluator response content must be a string."
+            raise EvaluationError(msg)
+        return content
+
+    def _load_json(self, content: str) -> Mapping[str, Any]:
+        try:
+            return json.loads(content)
+        except json.JSONDecodeError as exc:
+            msg = "Evaluator response was not valid JSON."
+            raise EvaluationError(msg) from exc
+
+    def _parse_dimension_scores(
+        self,
+        payload: Mapping[str, Any],
+        rubric: Mapping[str, RubricDimension],
+    ) -> Dict[str, DimensionScore]:
+        scores: Dict[str, DimensionScore] = {}
+        for name in rubric:
+            entry = payload.get(name)
+            if entry is None:
+                msg = f"Evaluator response missing score for dimension '{name}'."
+                raise EvaluationError(msg)
+            if isinstance(entry, Mapping):
+                raw_score = entry.get("score")
+                reasoning = entry.get("reasoning")
+            else:
+                raw_score = entry
+                reasoning = None
+            normalized_score = self._normalize_score(raw_score)
+            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)
+        return scores
+
+    def _normalize_score(self, raw_score: Any) -> float:
+        try:
+            value = self._coerce_numeric(raw_score)
+        except (TypeError, ValueError) as exc:
+            msg = "Dimension score must be a numeric value."
+            raise EvaluationError(msg) from exc
+
+        if 0.0 <= value <= 1.0:
+            return value
+        if 1.0 <= value <= 10.0:
+            return value / 10.0
+
+        msg = "Dimension score must be between 0-1 or 1-10."
+        raise EvaluationError(msg)
+
+    def _coerce_numeric(self, value: Any) -> float:
+        if isinstance(value, (int, float)) and math.isfinite(value):
+            return float(value)
+        if isinstance(value, str):
+            stripped = value.strip()
+            if not stripped:
+                msg = "Dimension score string cannot be empty."
+                raise ValueError(msg)
+            numeric = float(stripped)
+            if not math.isfinite(numeric):
+                msg = "Dimension score must be finite."
+                raise ValueError(msg)
+            return numeric
+        msg = "Unsupported score type."
+        raise TypeError(msg)
+
+    def _validate_temperature(self, value: float) -> float:
+        if not 0.0 <= value <= 1.0:
+            msg = "Temperature must be between 0.0 and 1.0."
+            raise ValueError(msg)
+        return float(value)
diff --git a/tesseract_flow/experiments/__init__.py b/tesseract_flow/experiments/__init__.py
index b8e118737f2025adf16794a2cd7069a50beaaaa8..b6bfb6a232732a215cd497ef37350c006a75d7fe 100644
--- a/tesseract_flow/experiments/__init__.py
+++ b/tesseract_flow/experiments/__init__.py
@@ -1,8 +1,29 @@
-"""Experiment design and execution."""
+"""Experiment design and execution utilities."""
 
-from tesseract_flow.experiments.taguchi import TaguchiExperiment, ExperimentVariable
+from tesseract_flow.core.config import Variable as ExperimentVariable
+from tesseract_flow.experiments.analysis import (
+    Effect,
+    MainEffects,
+    MainEffectsAnalyzer,
+    calculate_quality_improvement,
+    compare_configurations,
+    export_optimal_config,
+    identify_optimal_config,
+)
+from tesseract_flow.experiments.executor import ExperimentExecutor
+from tesseract_flow.experiments.taguchi import L8_ARRAY, generate_l8_array, generate_test_configs
 
 __all__ = [
-    "TaguchiExperiment",
+    "L8_ARRAY",
+    "Effect",
+    "ExperimentExecutor",
     "ExperimentVariable",
+    "MainEffects",
+    "MainEffectsAnalyzer",
+    "calculate_quality_improvement",
+    "compare_configurations",
+    "export_optimal_config",
+    "generate_l8_array",
+    "generate_test_configs",
+    "identify_optimal_config",
 ]
diff --git a/tesseract_flow/experiments/analysis.py b/tesseract_flow/experiments/analysis.py
new file mode 100644
index 0000000000000000000000000000000000000000..cd65d486b98780432f982449163f7521bb434154
--- /dev/null
+++ b/tesseract_flow/experiments/analysis.py
@@ -0,0 +1,220 @@
+"""Analysis utilities for interpreting experiment results."""
+from __future__ import annotations
+
+import math
+from pathlib import Path
+from statistics import mean
+from typing import Dict, Iterable, Mapping, MutableMapping, Optional, Sequence
+
+import yaml
+from pydantic import BaseModel, ConfigDict, Field, model_validator
+
+from tesseract_flow.core.config import TestConfiguration, TestResult, Variable
+
+
+class Effect(BaseModel):
+    """Main effect metrics for a single experiment variable."""
+
+    variable: str
+    avg_level_1: float
+    avg_level_2: float
+    effect_size: float
+    sum_of_squares: float = Field(ge=0.0)
+    contribution_pct: float = Field(ge=0.0)
+
+    model_config = ConfigDict(frozen=True)
+
+    @model_validator(mode="after")
+    def _validate_effect(self) -> "Effect":
+        expected = self.avg_level_2 - self.avg_level_1
+        if not math.isclose(self.effect_size, expected, rel_tol=1e-6, abs_tol=1e-9):
+            msg = "effect_size must equal avg_level_2 - avg_level_1"
+            raise ValueError(msg)
+        return self
+
+
+class MainEffects(BaseModel):
+    """Collection of main effects for an experiment."""
+
+    experiment_id: str
+    effects: Dict[str, Effect]
+    total_ss: float = Field(ge=0.0)
+
+    model_config = ConfigDict(frozen=True)
+
+    @model_validator(mode="after")
+    def _validate_contributions(self) -> "MainEffects":
+        if not self.effects:
+            msg = "effects must contain at least one entry"
+            raise ValueError(msg)
+
+        total_pct = sum(effect.contribution_pct for effect in self.effects.values())
+        if self.total_ss == 0.0:
+            if total_pct != 0.0:
+                msg = "Contribution percentages must be zero when total sum of squares is zero."
+                raise ValueError(msg)
+            return self
+
+        if not math.isclose(total_pct, 100.0, rel_tol=1e-2, abs_tol=0.5):
+            msg = "Contribution percentages must sum to approximately 100%."
+            raise ValueError(msg)
+        return self
+
+
+class MainEffectsAnalyzer:
+    """Compute Taguchi main effects from experiment results."""
+
+    @staticmethod
+    def compute(
+        results: Sequence[TestResult],
+        variables: Sequence[Variable],
+        *,
+        experiment_id: str,
+    ) -> MainEffects:
+        """Return main effects for ``variables`` derived from ``results``."""
+
+        if len(results) != 8:
+            msg = "Main effects analysis requires exactly 8 results for L8 array."
+            raise ValueError(msg)
+        if not variables:
+            msg = "At least one variable is required for main effects analysis."
+            raise ValueError(msg)
+
+        effects: MutableMapping[str, Effect] = {}
+        total_ss = 0.0
+
+        for variable in variables:
+            name = variable.name
+            level_1, level_2 = variable.level_1, variable.level_2
+            level_1_utilities = _utilities_for_level(results, name, level_1)
+            level_2_utilities = _utilities_for_level(results, name, level_2)
+
+            if not level_1_utilities or not level_2_utilities:
+                msg = f"Results are missing level assignments for variable '{name}'."
+                raise ValueError(msg)
+
+            avg_1 = mean(level_1_utilities)
+            avg_2 = mean(level_2_utilities)
+            effect_size = avg_2 - avg_1
+            sum_of_squares = (effect_size**2) * len(level_1_utilities)
+            total_ss += sum_of_squares
+
+            effects[name] = Effect(
+                variable=name,
+                avg_level_1=avg_1,
+                avg_level_2=avg_2,
+                effect_size=effect_size,
+                sum_of_squares=sum_of_squares,
+                contribution_pct=0.0,
+            )
+
+        if total_ss > 0.0:
+            for name, effect in effects.items():
+                contribution = (effect.sum_of_squares / total_ss) * 100.0
+                effects[name] = effect.model_copy(update={"contribution_pct": contribution})
+        else:
+            # All utilities identical; contributions remain zero.
+            effects = {
+                name: effect.model_copy(update={"contribution_pct": 0.0})
+                for name, effect in effects.items()
+            }
+
+        return MainEffects(experiment_id=experiment_id, effects=dict(effects), total_ss=total_ss)
+
+
+def identify_optimal_config(main_effects: MainEffects, variables: Sequence[Variable]) -> Dict[str, object]:
+    """Return the recommended configuration selecting higher-utility levels."""
+
+    variable_lookup: Mapping[str, Variable] = {variable.name: variable for variable in variables}
+    optimal: Dict[str, object] = {}
+    for name, effect in main_effects.effects.items():
+        variable = variable_lookup.get(name)
+        if variable is None:
+            msg = f"Variable '{name}' not found in experiment definition."
+            raise ValueError(msg)
+        if effect.avg_level_1 >= effect.avg_level_2:
+            optimal[name] = variable.level_1
+        else:
+            optimal[name] = variable.level_2
+    return optimal
+
+
+def compare_configurations(
+    baseline: Mapping[str, object],
+    candidate: Mapping[str, object],
+) -> Dict[str, Dict[str, object]]:
+    """Return a comparison mapping highlighting configuration differences."""
+
+    comparison: Dict[str, Dict[str, object]] = {}
+    keys = sorted(set(baseline) | set(candidate))
+    for key in keys:
+        baseline_value = baseline.get(key)
+        candidate_value = candidate.get(key)
+        comparison[key] = {
+            "baseline": baseline_value,
+            "candidate": candidate_value,
+            "changed": baseline_value != candidate_value,
+        }
+    return comparison
+
+
+def calculate_quality_improvement(
+    baseline_quality: Optional[float],
+    optimal_quality: Optional[float],
+) -> Optional[float]:
+    """Compute percentage improvement from ``baseline_quality`` to ``optimal_quality``."""
+
+    if baseline_quality is None or optimal_quality is None:
+        return None
+    if baseline_quality <= 0.0:
+        return None
+    return ((optimal_quality - baseline_quality) / baseline_quality) * 100.0
+
+
+def export_optimal_config(
+    optimal_values: Mapping[str, object],
+    path: Path | str,
+    *,
+    experiment_name: str,
+    workflow: str,
+) -> Path:
+    """Persist the recommended configuration to a YAML file."""
+
+    destination = Path(path)
+    destination.parent.mkdir(parents=True, exist_ok=True)
+
+    payload = {
+        "experiment": experiment_name,
+        "workflow": workflow,
+        "configuration": dict(optimal_values),
+    }
+    destination.write_text(
+        yaml.safe_dump(payload, sort_keys=False),
+        encoding="utf-8",
+    )
+    return destination
+
+
+def _utilities_for_level(
+    results: Iterable[TestResult],
+    variable_name: str,
+    level_value: object,
+) -> list[float]:
+    utilities: list[float] = []
+    for result in results:
+        config: TestConfiguration = result.config
+        value = config.config_values.get(variable_name)
+        if value == level_value:
+            utilities.append(result.utility)
+    return utilities
+
+
+__all__ = [
+    "Effect",
+    "MainEffects",
+    "MainEffectsAnalyzer",
+    "calculate_quality_improvement",
+    "compare_configurations",
+    "export_optimal_config",
+    "identify_optimal_config",
+]
diff --git a/tesseract_flow/experiments/executor.py b/tesseract_flow/experiments/executor.py
new file mode 100644
index 0000000000000000000000000000000000000000..b1be61a5f74521e22db03a7364711e7574596498
--- /dev/null
+++ b/tesseract_flow/experiments/executor.py
@@ -0,0 +1,353 @@
+"""Experiment execution orchestration for Taguchi L8 runs."""
+from __future__ import annotations
+
+import asyncio
+import json
+import logging
+from datetime import datetime, timezone
+from pathlib import Path
+from time import perf_counter
+from typing import Any, Callable, Dict, Iterable, Mapping, Optional, Sequence
+
+from tesseract_flow.core.base_workflow import BaseWorkflowService
+from tesseract_flow.core.config import (
+    ExperimentConfig,
+    ExperimentMetadata,
+    ExperimentRun,
+    TestConfiguration,
+    TestResult,
+)
+from tesseract_flow.core.exceptions import ExperimentError, WorkflowExecutionError
+from tesseract_flow.core.types import RubricDimension
+from tesseract_flow.evaluation.rubric import RubricEvaluator
+from tesseract_flow.experiments.taguchi import generate_test_configs
+
+logger = logging.getLogger(__name__)
+
+
+class ExperimentExecutor:
+    """Coordinate workflow execution, evaluation, and persistence for experiments."""
+
+    def __init__(
+        self,
+        workflow_resolver: Callable[[str, ExperimentConfig], BaseWorkflowService],
+        evaluator: RubricEvaluator,
+        *,
+        dependency_versions: Optional[Mapping[str, str]] = None,
+        non_deterministic_sources: Optional[Sequence[str]] = None,
+    ) -> None:
+        self._workflow_resolver = workflow_resolver
+        self._evaluator = evaluator
+        self._dependency_versions = dict(dependency_versions or {})
+        self._non_deterministic_sources = list(non_deterministic_sources or ["llm_sampling"])
+
+    async def run(
+        self,
+        config: ExperimentConfig,
+        *,
+        experiment_id: Optional[str] = None,
+        resume_from: Optional[ExperimentRun] = None,
+        progress_callback: Optional[Callable[[int, int], None]] = None,
+        persist_path: Optional[Path | str] = None,
+        extra_instructions: Optional[str] = None,
+    ) -> ExperimentRun:
+        """Execute all test configurations defined by ``config`` sequentially."""
+
+        persistence_path = Path(persist_path) if persist_path is not None else None
+
+        if resume_from is not None:
+            self._validate_resume_config(config, resume_from)
+            run_state = resume_from
+            test_configurations = run_state.test_configurations
+            experiment_identifier = run_state.experiment_id
+            logger.info(
+                "Resuming experiment %s with %s/%s completed tests",
+                experiment_identifier,
+                len(run_state.results),
+                len(test_configurations),
+            )
+        else:
+            test_configurations = generate_test_configs(config)
+            metadata = ExperimentMetadata.from_config(
+                config,
+                dependencies=self._dependency_versions,
+                non_deterministic_sources=self._non_deterministic_sources,
+            )
+            experiment_identifier = experiment_id or self._build_experiment_id(config)
+            run_state = ExperimentRun(
+                experiment_id=experiment_identifier,
+                config=config,
+                test_configurations=test_configurations,
+                experiment_metadata=metadata,
+            )
+            logger.info(
+                "Starting experiment %s with %s tests", experiment_identifier, len(test_configurations)
+            )
+
+        total_tests = len(test_configurations)
+        run_state = self._ensure_running_state(run_state)
+
+        if persistence_path is not None:
+            self.save_run(run_state, persistence_path)
+
+        if progress_callback:
+            progress_callback(len(run_state.results), total_tests)
+
+        workflow_service = self._workflow_resolver(config.workflow, config)
+
+        try:
+            for test_config in test_configurations:
+                if any(result.test_number == test_config.test_number for result in run_state.results):
+                    continue
+
+                logger.debug(
+                    "Running test %s/%s (test #%s)",
+                    len(run_state.results) + 1,
+                    total_tests,
+                    test_config.test_number,
+                )
+                result = await self.run_single_test(
+                    test_config,
+                    workflow_service,
+                    self._evaluator,
+                    experiment_config=config,
+                    rubric=config.workflow_config.rubric if config.workflow_config else None,
+                    extra_instructions=extra_instructions,
+                )
+                run_state = run_state.record_result(result)
+
+                if persistence_path is not None:
+                    self.save_run(run_state, persistence_path)
+
+                if progress_callback:
+                    progress_callback(len(run_state.results), total_tests)
+
+            run_state = run_state.mark_completed()
+            logger.info("Experiment %s completed successfully", experiment_identifier)
+
+            if persistence_path is not None:
+                self.save_run(run_state, persistence_path)
+
+            return run_state
+        except Exception as exc:  # pragma: no cover - defensive guard
+            error_message = str(exc).strip() or exc.__class__.__name__
+            if run_state.status == "RUNNING":
+                try:
+                    run_state = run_state.mark_failed(error_message)
+                except ValueError:
+                    run_state = run_state.model_copy(update={"status": "FAILED", "error": error_message})
+                if persistence_path is not None:
+                    self.save_run(run_state, persistence_path)
+                logger.error(
+                    "Experiment %s failed after %s/%s tests: %s",
+                    experiment_identifier,
+                    len(run_state.results),
+                    total_tests,
+                    error_message,
+                )
+
+            if isinstance(exc, ExperimentError):
+                raise
+            raise ExperimentError("Experiment execution failed.") from exc
+
+    async def run_single_test(
+        self,
+        test_config: TestConfiguration,
+        workflow_service: BaseWorkflowService,
+        evaluator: RubricEvaluator,
+        *,
+        experiment_config: Optional[ExperimentConfig] = None,
+        rubric: Optional[Dict[str, RubricDimension]] = None,
+        extra_instructions: Optional[str] = None,
+    ) -> TestResult:
+        """Execute a single test configuration and evaluate its output."""
+
+        workflow_input = self._prepare_workflow_input(
+            workflow_service, test_config, experiment_config
+        )
+
+        loop = asyncio.get_running_loop()
+        start = perf_counter()
+
+        try:
+            workflow_output = await loop.run_in_executor(
+                None, workflow_service.run, workflow_input
+            )
+        except WorkflowExecutionError as exc:
+            raise ExperimentError("Workflow execution failed.") from exc
+        except Exception as exc:  # pragma: no cover - defensive guard
+            raise ExperimentError("Workflow execution raised an unexpected error.") from exc
+
+        duration_ms = (perf_counter() - start) * 1000.0
+        workflow_metadata = self._extract_workflow_metadata(workflow_service)
+        if "duration_seconds" in workflow_metadata:
+            duration_ms = float(workflow_metadata["duration_seconds"]) * 1000.0
+
+        output_text = self._render_for_evaluation(workflow_output)
+        quality_score = await evaluator.evaluate(
+            output_text,
+            rubric=rubric,
+            extra_instructions=extra_instructions,
+        )
+
+        cost = self._extract_numeric(workflow_output, workflow_metadata, ["cost", "total_cost"], default=0.0)
+        latency = self._extract_numeric(
+            workflow_output,
+            workflow_metadata,
+            ["latency_ms", "latency", "duration_ms"],
+            default=duration_ms,
+        )
+
+        metadata = self._merge_metadata(workflow_output, workflow_metadata)
+        evaluation_metadata = dict(quality_score.metadata)
+        evaluation_metadata["model"] = quality_score.evaluator_model
+        metadata.setdefault("evaluation", {}).update(evaluation_metadata)
+
+        enriched_score = quality_score.with_metadata(**evaluation_metadata)
+
+        logger.debug(
+            "Test %s completed with quality %.2f, cost %.4f, latency %.2fms",
+            test_config.test_number,
+            quality_score.overall_score,
+            cost,
+            latency,
+        )
+
+        return TestResult(
+            test_number=test_config.test_number,
+            config=test_config,
+            quality_score=enriched_score,
+            cost=max(0.0, cost),
+            latency=max(0.0, latency),
+            workflow_output=output_text,
+            metadata=metadata,
+        )
+
+    def save_run(self, run: ExperimentRun, path: Path | str) -> Path:
+        """Persist an experiment run to disk as JSON."""
+
+        file_path = Path(path)
+        file_path.parent.mkdir(parents=True, exist_ok=True)
+        serialized = run.model_dump_json(indent=2, exclude_none=True)
+        file_path.write_text(serialized, encoding="utf-8")
+        logger.debug("Persisted experiment run %s to %s", run.experiment_id, file_path)
+        return file_path
+
+    @staticmethod
+    def load_run(path: Path | str) -> ExperimentRun:
+        """Load an experiment run from a JSON file."""
+
+        file_path = Path(path)
+        data = json.loads(file_path.read_text(encoding="utf-8"))
+        return ExperimentRun.model_validate(data)
+
+    def _ensure_running_state(self, run: ExperimentRun) -> ExperimentRun:
+        if run.status == "COMPLETED":
+            return run
+        if run.status == "RUNNING":
+            return run
+        return run.mark_running(started_at=run.started_at)
+
+    def _prepare_workflow_input(
+        self,
+        workflow_service: BaseWorkflowService,
+        test_config: TestConfiguration,
+        experiment_config: Optional[ExperimentConfig],
+    ) -> Any:
+        if hasattr(workflow_service, "prepare_input"):
+            return workflow_service.prepare_input(test_config, experiment_config)
+        return test_config
+
+    def _extract_workflow_metadata(self, workflow_service: BaseWorkflowService) -> Dict[str, Any]:
+        metadata = getattr(workflow_service, "last_run_metadata", None)
+        if isinstance(metadata, Mapping):
+            return dict(metadata)
+        return {}
+
+    def _render_for_evaluation(self, workflow_output: Any) -> str:
+        if hasattr(workflow_output, "render_for_evaluation"):
+            rendered = workflow_output.render_for_evaluation()
+            if isinstance(rendered, str):
+                return rendered.strip()
+        if hasattr(workflow_output, "evaluation_text"):
+            value = getattr(workflow_output, "evaluation_text")
+            if isinstance(value, str):
+                return value.strip()
+        if hasattr(workflow_output, "model_dump"):
+            data = workflow_output.model_dump(mode="python")  # type: ignore[attr-defined]
+            for key in ("evaluation_text", "content", "output", "review"):
+                value = data.get(key)
+                if isinstance(value, str):
+                    return value.strip()
+        if isinstance(workflow_output, Mapping):
+            for key in ("evaluation_text", "content", "output", "review"):
+                value = workflow_output.get(key)
+                if isinstance(value, str):
+                    return value.strip()
+        return str(workflow_output).strip()
+
+    def _extract_numeric(
+        self,
+        workflow_output: Any,
+        workflow_metadata: Mapping[str, Any],
+        keys: Iterable[str],
+        *,
+        default: float,
+    ) -> float:
+        for key in keys:
+            if hasattr(workflow_output, key):
+                value = getattr(workflow_output, key)
+                numeric = self._coerce_float(value)
+                if numeric is not None:
+                    return numeric
+            if hasattr(workflow_output, "model_dump"):
+                data = workflow_output.model_dump(mode="python")  # type: ignore[attr-defined]
+                if key in data:
+                    numeric = self._coerce_float(data[key])
+                    if numeric is not None:
+                        return numeric
+            if key in workflow_metadata:
+                numeric = self._coerce_float(workflow_metadata[key])
+                if numeric is not None:
+                    return numeric
+        return default
+
+    def _merge_metadata(self, workflow_output: Any, workflow_metadata: Mapping[str, Any]) -> Dict[str, Any]:
+        metadata: Dict[str, Any] = {}
+        if hasattr(workflow_output, "metadata"):
+            candidate = getattr(workflow_output, "metadata")
+            if isinstance(candidate, Mapping):
+                metadata.update(candidate)
+        elif hasattr(workflow_output, "model_dump"):
+            data = workflow_output.model_dump(mode="python")  # type: ignore[attr-defined]
+            candidate = data.get("metadata")
+            if isinstance(candidate, Mapping):
+                metadata.update(candidate)
+        metadata.setdefault("workflow", {}).update(dict(workflow_metadata))
+        return metadata
+
+    def _coerce_float(self, value: Any) -> Optional[float]:
+        try:
+            numeric = float(value)
+        except (TypeError, ValueError):
+            return None
+        if not (numeric == numeric):  # NaN check
+            return None
+        return numeric
+
+    def _build_experiment_id(self, config: ExperimentConfig) -> str:
+        timestamp = datetime.now(tz=timezone.utc).strftime("%Y%m%dT%H%M%S")
+        sanitized = config.name.strip().replace(" ", "_")
+        return f"{sanitized or 'experiment'}-{timestamp}"
+
+    def _validate_resume_config(self, config: ExperimentConfig, run: ExperimentRun) -> None:
+        new_payload = config.model_dump(mode="json")
+        existing_payload = run.config.model_dump(mode="json")
+        if new_payload != existing_payload:
+            raise ExperimentError(
+                "Cannot resume experiment: configuration differs from saved state."
+            )
+
+
+__all__ = ["ExperimentExecutor"]
+
diff --git a/tesseract_flow/experiments/taguchi.py b/tesseract_flow/experiments/taguchi.py
new file mode 100644
index 0000000000000000000000000000000000000000..65b7d2eab9b6a5918b4759734d0921d0270d6c23
--- /dev/null
+++ b/tesseract_flow/experiments/taguchi.py
@@ -0,0 +1,71 @@
+"""Taguchi L8 orthogonal array utilities."""
+from __future__ import annotations
+
+from typing import Dict, Iterable, List
+
+import numpy as np
+from numpy.typing import NDArray
+
+from tesseract_flow.core.config import ExperimentConfig, TestConfiguration, Variable
+
+L8_ARRAY: NDArray[np.int_] = np.array(
+    [
+        [1, 1, 1, 1, 1, 1, 1],
+        [1, 1, 1, 2, 2, 2, 2],
+        [1, 2, 2, 1, 1, 2, 2],
+        [1, 2, 2, 2, 2, 1, 1],
+        [2, 1, 2, 1, 2, 1, 2],
+        [2, 1, 2, 2, 1, 2, 1],
+        [2, 2, 1, 1, 2, 2, 1],
+        [2, 2, 1, 2, 1, 1, 2],
+    ],
+    dtype=np.int_,
+)
+"""Standard Taguchi L8 orthogonal array supporting up to seven variables."""
+
+
+def generate_l8_array(num_variables: int) -> NDArray[np.int_]:
+    """Return the Taguchi L8 orthogonal array truncated to ``num_variables`` columns."""
+
+    if num_variables < 4 or num_variables > L8_ARRAY.shape[1]:
+        msg = "Taguchi L8 array requires between 4 and 7 variables."
+        raise ValueError(msg)
+    return np.array(L8_ARRAY[:, :num_variables], copy=True)
+
+
+def _build_variable_levels(variables: Iterable[Variable]) -> Dict[str, tuple[object, object]]:
+    """Map variable names to their allowed level values."""
+
+    return {
+        variable.name: (variable.level_1, variable.level_2)
+        for variable in variables
+    }
+
+
+def generate_test_configs(config: ExperimentConfig) -> List[TestConfiguration]:
+    """Generate :class:`TestConfiguration` objects for the provided experiment config."""
+
+    variables = config.variables
+    array = generate_l8_array(len(variables))
+    variable_levels = _build_variable_levels(variables)
+
+    test_configs: List[TestConfiguration] = []
+    for index, row in enumerate(array, start=1):
+        config_values = {
+            variable.name: (variable.level_1 if level == 1 else variable.level_2)
+            for variable, level in zip(variables, row, strict=True)
+        }
+        test_config = TestConfiguration.model_validate(
+            {
+                "test_number": index,
+                "workflow": config.workflow,
+                "config_values": config_values,
+            },
+            context={"variable_levels": variable_levels},
+        )
+        test_configs.append(test_config)
+
+    return test_configs
+
+
+__all__ = ["L8_ARRAY", "generate_l8_array", "generate_test_configs"]
diff --git a/tesseract_flow/optimization/__init__.py b/tesseract_flow/optimization/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..db00f3833d57f2b3fa2146f1b042899bdb6de93a
--- /dev/null
+++ b/tesseract_flow/optimization/__init__.py
@@ -0,0 +1,19 @@
+"""Optimization utilities for experiment results."""
+
+from importlib import import_module
+from typing import TYPE_CHECKING, Any
+
+from tesseract_flow.optimization.utility import UtilityFunction
+
+if TYPE_CHECKING:  # pragma: no cover - used for type checking only
+    from tesseract_flow.optimization.pareto import ParetoFrontier, ParetoPoint
+
+__all__ = ["ParetoFrontier", "ParetoPoint", "UtilityFunction"]
+
+
+def __getattr__(name: str) -> Any:
+    if name in {"ParetoFrontier", "ParetoPoint"}:
+        module = import_module("tesseract_flow.optimization.pareto")
+        return getattr(module, name)
+    msg = f"module 'tesseract_flow.optimization' has no attribute '{name}'"
+    raise AttributeError(msg)
diff --git a/tesseract_flow/optimization/pareto.py b/tesseract_flow/optimization/pareto.py
new file mode 100644
index 0000000000000000000000000000000000000000..0830c2a5ae729784b8e78019dd1191febb2af586
--- /dev/null
+++ b/tesseract_flow/optimization/pareto.py
@@ -0,0 +1,328 @@
+"""Pareto frontier computation and visualization utilities."""
+from __future__ import annotations
+
+from pathlib import Path
+from typing import List, Optional, Sequence
+
+import matplotlib
+
+matplotlib.use("Agg")
+
+import matplotlib.pyplot as plt
+from pydantic import BaseModel, ConfigDict, Field, ValidationError, model_validator
+
+from tesseract_flow.core.config import TestConfiguration, TestResult
+
+
+_TOLERANCE = 1e-9
+
+
+class ParetoPoint(BaseModel):
+    """Represents a single configuration evaluated in an experiment."""
+
+    config: TestConfiguration
+    quality: float = Field(..., ge=0.0, le=1.0)
+    cost: float = Field(..., ge=0.0)
+    latency: float = Field(..., ge=0.0)
+    utility: float
+    is_optimal: bool = False
+    dominated_by: Optional[int] = Field(default=None, ge=1, le=8)
+
+    model_config = ConfigDict(frozen=True)
+
+    @property
+    def test_number(self) -> int:
+        """Return the associated Taguchi test number."""
+
+        return self.config.test_number
+
+    @model_validator(mode="after")
+    def _validate_dominance(self) -> "ParetoPoint":
+        if self.dominated_by is not None and self.is_optimal:
+            msg = "Pareto-optimal points cannot specify dominated_by."
+            raise ValueError(msg)
+        if self.dominated_by is not None and self.dominated_by == self.test_number:
+            msg = "dominated_by cannot reference the same test number."
+            raise ValueError(msg)
+        return self
+
+
+class ParetoFrontier(BaseModel):
+    """Pareto frontier describing non-dominated experiment configurations."""
+
+    experiment_id: str
+    points: List[ParetoPoint]
+    optimal_points: List[ParetoPoint]
+    x_axis: str = "cost"
+    y_axis: str = "quality"
+
+    model_config = ConfigDict(frozen=True)
+
+    @model_validator(mode="after")
+    def _validate_frontier(self) -> "ParetoFrontier":
+        if not self.points:
+            msg = "At least one Pareto point is required."
+            raise ValueError(msg)
+        optimal_numbers = {point.test_number for point in self.optimal_points}
+        if not optimal_numbers:
+            msg = "At least one Pareto-optimal point must be present."
+            raise ValueError(msg)
+        for point in self.optimal_points:
+            if not point.is_optimal:
+                msg = "optimal_points must only contain optimal Pareto points."
+                raise ValueError(msg)
+        all_numbers = {point.test_number for point in self.points}
+        if not optimal_numbers.issubset(all_numbers):
+            msg = "optimal_points must be a subset of points."
+            raise ValueError(msg)
+        return self
+
+    @staticmethod
+    def compute(
+        results: Sequence[TestResult],
+        *,
+        experiment_id: str,
+        x_axis: str = "cost",
+        y_axis: str = "quality",
+    ) -> "ParetoFrontier":
+        """Compute a Pareto frontier for the provided experiment *results*."""
+
+        if not results:
+            msg = "At least one TestResult is required to compute the Pareto frontier."
+            raise ValueError(msg)
+
+        axis_x = x_axis.strip().lower()
+        axis_y = y_axis.strip().lower()
+
+        valid_x = {"cost", "latency"}
+        valid_y = {"quality", "utility"}
+        if axis_x not in valid_x:
+            msg = f"Unsupported x_axis '{x_axis}'. Expected one of: {sorted(valid_x)}."
+            raise ValueError(msg)
+        if axis_y not in valid_y:
+            msg = f"Unsupported y_axis '{y_axis}'. Expected one of: {sorted(valid_y)}."
+            raise ValueError(msg)
+
+        raw_points: List[ParetoPoint] = []
+        for result in results:
+            point = ParetoPoint(
+                config=result.config,
+                quality=result.quality_score.overall_score,
+                cost=result.cost,
+                latency=result.latency,
+                utility=result.utility,
+            )
+            raw_points.append(point)
+
+        sorted_points = sorted(
+            raw_points,
+            key=lambda point: (
+                _axis_value(point, axis_x),
+                -_axis_value(point, axis_y),
+            ),
+        )
+
+        optimal_numbers: List[int] = []
+        best_y = float("-inf")
+        for point in sorted_points:
+            value_y = _axis_value(point, axis_y)
+            if value_y > best_y + _TOLERANCE:
+                best_y = value_y
+                optimal_numbers.append(point.test_number)
+
+        dominance_map = _compute_dominance(sorted_points, axis_x, axis_y)
+
+        updated_points: List[ParetoPoint] = []
+        for point in raw_points:
+            test_number = point.test_number
+            is_optimal = test_number in optimal_numbers
+            dominated_by = dominance_map.get(test_number)
+            updated_points.append(
+                point.model_copy(update={"is_optimal": is_optimal, "dominated_by": dominated_by})
+            )
+
+        optimal_points = [point for point in updated_points if point.is_optimal]
+
+        try:
+            return ParetoFrontier(
+                experiment_id=experiment_id,
+                points=updated_points,
+                optimal_points=optimal_points,
+                x_axis=axis_x,
+                y_axis=axis_y,
+            )
+        except ValidationError as exc:
+            raise ValueError(str(exc)) from exc
+
+    def points_within_budget(self, budget: float) -> List[ParetoPoint]:
+        """Return Pareto points whose X-axis value does not exceed *budget*."""
+
+        if budget < 0:
+            msg = "budget must be non-negative"
+            raise ValueError(msg)
+        return [
+            point
+            for point in self.points
+            if _axis_value(point, self.x_axis) <= budget + _TOLERANCE
+        ]
+
+    def best_within_budget(self, budget: float) -> Optional[ParetoPoint]:
+        """Return the highest-quality Pareto-optimal point within the given *budget*."""
+
+        candidates = [
+            point
+            for point in self.optimal_points
+            if _axis_value(point, self.x_axis) <= budget + _TOLERANCE
+        ]
+        if not candidates:
+            return None
+        return max(candidates, key=lambda point: _axis_value(point, self.y_axis))
+
+    def visualize(
+        self,
+        output_path: Optional[Path | str] = None,
+        *,
+        show: bool = False,
+        budget_threshold: Optional[float] = None,
+        title: Optional[str] = None,
+    ) -> Path:
+        """Generate a Pareto frontier chart and persist it to *output_path*."""
+
+        destination = _resolve_output_path(output_path)
+        destination.parent.mkdir(parents=True, exist_ok=True)
+
+        fig, ax = plt.subplots(figsize=(8, 6), dpi=120)
+
+        sizes = _bubble_sizes([point.latency for point in self.points])
+        size_lookup = {point.test_number: size for point, size in zip(self.points, sizes)}
+
+        dominated = [point for point in self.points if not point.is_optimal]
+        optimal = [point for point in self.points if point.is_optimal]
+
+        if dominated:
+            ax.scatter(
+                [_axis_value(point, self.x_axis) for point in dominated],
+                [_axis_value(point, self.y_axis) for point in dominated],
+                s=[size_lookup[point.test_number] for point in dominated],
+                c="#9ca3af",
+                alpha=0.7,
+                edgecolors="none",
+                label="Dominated",
+            )
+
+        if optimal:
+            ax.scatter(
+                [_axis_value(point, self.x_axis) for point in optimal],
+                [_axis_value(point, self.y_axis) for point in optimal],
+                s=[size_lookup[point.test_number] for point in optimal],
+                c="#2563eb",
+                alpha=0.9,
+                edgecolors="#1f2937",
+                linewidths=0.8,
+                label="Pareto-optimal",
+            )
+
+        for index, point in enumerate(self.points):
+            ax.annotate(
+                f"#{point.test_number}",
+                (_axis_value(point, self.x_axis), _axis_value(point, self.y_axis)),
+                textcoords="offset points",
+                xytext=(6, 4),
+                fontsize=9,
+                color="#111827",
+            )
+
+        if budget_threshold is not None:
+            ax.axvline(
+                budget_threshold,
+                color="#f59e0b",
+                linestyle="--",
+                linewidth=1.5,
+                label=f"Budget â‰¤ {budget_threshold:.3f}",
+            )
+
+        ax.set_xlabel(_axis_label(self.x_axis))
+        ax.set_ylabel(_axis_label(self.y_axis))
+        chart_title = title or f"Pareto Frontier ({_axis_label(self.y_axis)} vs {_axis_label(self.x_axis)})"
+        ax.set_title(chart_title)
+        ax.grid(True, linestyle="--", linewidth=0.5, alpha=0.5)
+        ax.legend()
+
+        fig.tight_layout()
+        fig.savefig(destination, bbox_inches="tight")
+
+        if show:
+            plt.show()
+
+        plt.close(fig)
+
+        return destination
+
+
+def _axis_value(point: ParetoPoint, axis: str) -> float:
+    if axis == "cost":
+        return point.cost
+    if axis == "latency":
+        return point.latency
+    if axis == "quality":
+        return point.quality
+    if axis == "utility":
+        return point.utility
+    msg = f"Unsupported axis '{axis}'."
+    raise ValueError(msg)
+
+
+def _axis_label(axis: str) -> str:
+    if axis == "cost":
+        return "Cost (USD)"
+    if axis == "latency":
+        return "Latency (ms)"
+    if axis == "quality":
+        return "Quality Score"
+    if axis == "utility":
+        return "Utility"
+    return axis.title()
+
+
+def _compute_dominance(
+    points: Sequence[ParetoPoint],
+    axis_x: str,
+    axis_y: str,
+) -> dict[int, Optional[int]]:
+    dominance: dict[int, Optional[int]] = {point.test_number: None for point in points}
+    for point in points:
+        for candidate in points:
+            if point.test_number == candidate.test_number:
+                continue
+            dominates = (
+                _axis_value(candidate, axis_x) <= _axis_value(point, axis_x) + _TOLERANCE
+                and _axis_value(candidate, axis_y) >= _axis_value(point, axis_y) - _TOLERANCE
+                and (
+                    _axis_value(candidate, axis_x) < _axis_value(point, axis_x) - _TOLERANCE
+                    or _axis_value(candidate, axis_y) > _axis_value(point, axis_y) + _TOLERANCE
+                )
+            )
+            if dominates:
+                dominance[point.test_number] = candidate.test_number
+                break
+    return dominance
+
+
+def _bubble_sizes(latencies: Sequence[float]) -> List[float]:
+    if not latencies:
+        return []
+    minimum = min(latencies)
+    maximum = max(latencies)
+    if maximum == minimum:
+        return [300.0 for _ in latencies]
+    span = maximum - minimum
+    return [200.0 + ((latency - minimum) / span) * 600.0 for latency in latencies]
+
+
+def _resolve_output_path(path: Optional[Path | str]) -> Path:
+    if path is None:
+        return Path("pareto_frontier.png")
+    return Path(path)
+
+
+__all__ = ["ParetoFrontier", "ParetoPoint"]
diff --git a/tesseract_flow/optimization/utility.py b/tesseract_flow/optimization/utility.py
new file mode 100644
index 0000000000000000000000000000000000000000..2ea2cd01b160eed392a9fc7ccad3c2a169cca08e
--- /dev/null
+++ b/tesseract_flow/optimization/utility.py
@@ -0,0 +1,72 @@
+"""Utility calculations for balancing quality, cost, and latency."""
+from __future__ import annotations
+
+from typing import Iterable, List, Sequence, Tuple
+
+from tesseract_flow.core.types import UtilityWeights
+
+
+class UtilityFunction:
+    """Compute weighted utility scores for experiment results."""
+
+    def __init__(self, weights: UtilityWeights) -> None:
+        self.weights = weights
+
+    def compute(self, *, quality: float, cost: float, latency: float) -> float:
+        """Compute utility using normalized cost and latency values."""
+
+        return (
+            self.weights.quality * quality
+            - self.weights.cost * cost
+            - self.weights.time * latency
+        )
+
+    @staticmethod
+    def normalize_metrics(
+        values: Sequence[float], method: str = "min-max"
+    ) -> Tuple[List[float], dict[str, float]]:
+        """Normalize metric values to the [0, 1] range."""
+
+        if method != "min-max":
+            msg = f"Unsupported normalization method: {method}"
+            raise ValueError(msg)
+
+        values_list = list(values)
+        if not values_list:
+            return [], {"min": 0.0, "max": 0.0}
+
+        minimum = min(values_list)
+        maximum = max(values_list)
+        if maximum == minimum:
+            return [0.0 for _ in values_list], {"min": minimum, "max": maximum}
+
+        scale = maximum - minimum
+        normalized = [(value - minimum) / scale for value in values_list]
+        return normalized, {"min": minimum, "max": maximum}
+
+    def compute_for_sequences(
+        self,
+        qualities: Iterable[float],
+        costs: Sequence[float],
+        latencies: Sequence[float],
+    ) -> List[float]:
+        """Compute utilities for aligned quality, cost, and latency sequences."""
+
+        normalized_costs, _ = self.normalize_metrics(costs)
+        normalized_latencies, _ = self.normalize_metrics(latencies)
+        normalized_len = len(normalized_costs)
+        if normalized_len != len(normalized_latencies):
+            msg = "Cost and latency sequences must share the same length."
+            raise ValueError(msg)
+
+        quality_list = list(qualities)
+        if len(quality_list) != normalized_len:
+            msg = "Quality, cost, and latency sequences must be aligned."
+            raise ValueError(msg)
+
+        return [
+            self.compute(quality=quality, cost=cost_norm, latency=latency_norm)
+            for quality, cost_norm, latency_norm in zip(
+                quality_list, normalized_costs, normalized_latencies
+            )
+        ]
diff --git a/tesseract_flow/workflows/__init__.py b/tesseract_flow/workflows/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..8aed6a92148b1f6337539704c79604e3563a5c14
--- /dev/null
+++ b/tesseract_flow/workflows/__init__.py
@@ -0,0 +1,9 @@
+"""Workflow implementations shipped with TesseractFlow."""
+from .code_review import CodeIssue, CodeReviewInput, CodeReviewOutput, CodeReviewWorkflow
+
+__all__ = [
+    "CodeIssue",
+    "CodeReviewInput",
+    "CodeReviewOutput",
+    "CodeReviewWorkflow",
+]
diff --git a/tesseract_flow/workflows/code_review.py b/tesseract_flow/workflows/code_review.py
new file mode 100644
index 0000000000000000000000000000000000000000..a1ec69e7c6a6020de7d1145536abfa30d6ca808c
--- /dev/null
+++ b/tesseract_flow/workflows/code_review.py
@@ -0,0 +1,603 @@
+"""Code review workflow built on LangGraph."""
+from __future__ import annotations
+
+import asyncio
+import json
+from dataclasses import dataclass
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Mapping, Optional
+
+from jinja2 import Template
+from langgraph.graph import END, StateGraph
+from pydantic import BaseModel, ConfigDict, Field, field_validator
+
+from tesseract_flow.core.base_workflow import BaseWorkflowService
+from tesseract_flow.core.config import ExperimentConfig, TestConfiguration, WorkflowConfig
+from tesseract_flow.core.exceptions import WorkflowExecutionError
+from tesseract_flow.core.strategies import GenerationStrategy, get_strategy
+
+
+_ALLOWED_SEVERITIES = {"low", "medium", "high", "critical"}
+
+
+class CodeReviewInput(BaseModel):
+    """Input payload for the code review workflow."""
+
+    code: str = Field(..., min_length=1)
+    language: str = Field(default="python", min_length=1)
+    context: Optional[str] = None
+    metadata: Dict[str, Any] = Field(default_factory=dict)
+
+    model_config = ConfigDict(extra="allow")
+
+    @field_validator("code")
+    @classmethod
+    def _strip_code(cls, value: str) -> str:
+        stripped = value.strip()
+        if not stripped:
+            msg = "Code to review must be a non-empty string."
+            raise ValueError(msg)
+        return stripped
+
+    @field_validator("language")
+    @classmethod
+    def _normalize_language(cls, value: str) -> str:
+        return value.strip()
+
+    @field_validator("context")
+    @classmethod
+    def _normalize_context(cls, value: Optional[str]) -> Optional[str]:
+        if value is None:
+            return None
+        stripped = value.strip()
+        return stripped or None
+
+
+class CodeIssue(BaseModel):
+    """Structured representation of an issue found during review."""
+
+    type: str = Field(default="general", min_length=1)
+    severity: str = Field(default="medium")
+    description: str = Field(..., min_length=1)
+    line_number: Optional[int] = Field(default=None, ge=1)
+    suggestion: Optional[str] = None
+
+    model_config = ConfigDict(extra="forbid")
+
+    @field_validator("type")
+    @classmethod
+    def _normalize_type(cls, value: str) -> str:
+        stripped = value.strip()
+        return stripped or "general"
+
+    @field_validator("severity")
+    @classmethod
+    def _validate_severity(cls, value: str) -> str:
+        normalized = value.strip().lower()
+        if normalized not in _ALLOWED_SEVERITIES:
+            return "medium"
+        return normalized
+
+    @field_validator("description")
+    @classmethod
+    def _normalize_description(cls, value: str) -> str:
+        stripped = value.strip()
+        if not stripped:
+            msg = "Issue description must be a non-empty string."
+            raise ValueError(msg)
+        return stripped
+
+    @field_validator("line_number", mode="before")
+    @classmethod
+    def _coerce_line_number(cls, value: Any) -> Optional[int]:
+        if value is None:
+            return None
+        try:
+            numeric = int(value)
+        except (TypeError, ValueError):
+            return None
+        if numeric <= 0:
+            return None
+        return numeric
+
+    @field_validator("suggestion")
+    @classmethod
+    def _normalize_suggestion(cls, value: Optional[str]) -> Optional[str]:
+        if value is None:
+            return None
+        stripped = value.strip()
+        return stripped or None
+
+
+class CodeReviewOutput(BaseModel):
+    """Workflow output containing structured review feedback."""
+
+    summary: str
+    issues: List[CodeIssue] = Field(default_factory=list)
+    suggestions: List[str] = Field(default_factory=list)
+    evaluation_text: str
+    metadata: Dict[str, Any] = Field(default_factory=dict)
+
+    model_config = ConfigDict(validate_assignment=True)
+
+    @field_validator("summary")
+    @classmethod
+    def _normalize_summary(cls, value: str) -> str:
+        stripped = value.strip()
+        if not stripped:
+            msg = "Summary must be a non-empty string."
+            raise ValueError(msg)
+        return stripped
+
+    @field_validator("suggestions", mode="before")
+    @classmethod
+    def _coerce_suggestions(cls, value: Any) -> List[str]:
+        if value is None:
+            return []
+        if isinstance(value, str):
+            candidates = value.splitlines()
+        else:
+            candidates = list(value)
+        normalized: List[str] = []
+        for candidate in candidates:
+            if not isinstance(candidate, str):
+                candidate = str(candidate)
+            stripped = candidate.strip()
+            if stripped:
+                normalized.append(stripped)
+        return normalized
+
+    @field_validator("evaluation_text")
+    @classmethod
+    def _normalize_evaluation_text(cls, value: str) -> str:
+        stripped = value.strip()
+        if not stripped:
+            msg = "evaluation_text must be non-empty."
+            raise ValueError(msg)
+        return stripped
+
+    def render_for_evaluation(self) -> str:
+        """Return textual representation used for rubric evaluation."""
+
+        return self.evaluation_text
+
+
+@dataclass
+class _RuntimeSettings:
+    model: str
+    temperature: float
+    context_size: str
+    strategy_name: str
+    language: str
+    sample_code_path: Optional[str]
+    metadata: Dict[str, Any]
+
+
+class CodeReviewWorkflow(BaseWorkflowService[CodeReviewInput, CodeReviewOutput]):
+    """LangGraph workflow that analyzes code and returns structured feedback."""
+
+    DEFAULT_PROMPTS: Dict[str, str] = {
+        "analyze": (
+            "You are an expert reviewer of {{language}} code.\n"
+            "{% if context %}Context: {{context}}\n{% endif %}"
+            "Review the following snippet and identify issues.\n"
+            "```{{language}}\n{{code}}\n```\n"
+            "Return JSON with keys 'summary', 'issues', and 'suggestions'. Each issue should "
+            "include type, severity, description, optional line_number, and suggestion."
+        ),
+        "suggest": (
+            "Given the analysis below, enumerate actionable suggestions in JSON under "
+            "the key 'suggestions'. Analysis: {{analysis}}"
+        ),
+    }
+
+    def __init__(
+        self,
+        *,
+        config: Optional[WorkflowConfig] = None,
+        default_model: str = "anthropic/claude-3.5-sonnet",
+        default_temperature: float = 0.3,
+    ) -> None:
+        super().__init__(config=config)
+        self._default_model = default_model
+        self._default_temperature = default_temperature
+        self._runtime: Optional[_RuntimeSettings] = None
+        self._analysis_text: Optional[str] = None
+        self._analysis_prompt: Optional[str] = None
+        self._suggest_prompt: Optional[str] = None
+
+    def prepare_input(
+        self,
+        test_config: TestConfiguration,
+        experiment_config: ExperimentConfig | None,
+    ) -> CodeReviewInput:
+        """Prepare workflow input based on test configuration."""
+
+        values = {str(key): value for key, value in test_config.config_values.items()}
+        temperature = self._coerce_float(values.get("temperature"), self._default_temperature)
+        model_name = str(values.get("model", self._default_model))
+        context_size = str(values.get("context_size", values.get("context", "file_only")))
+        strategy_name = str(values.get("generation_strategy", values.get("strategy", "standard")))
+
+        workflow_config = experiment_config.workflow_config if experiment_config else self.config
+        language = self._resolve_language(values, workflow_config)
+        code, sample_path = self._load_source(workflow_config)
+        context_description = self._context_description(context_size, workflow_config)
+
+        metadata = {
+            "test_number": test_config.test_number,
+            "config_values": dict(values),
+        }
+        runtime_metadata = {
+            "test_number": test_config.test_number,
+            "config": dict(values),
+        }
+
+        self._runtime = _RuntimeSettings(
+            model=model_name,
+            temperature=temperature,
+            context_size=context_size,
+            strategy_name=strategy_name,
+            language=language,
+            sample_code_path=sample_path,
+            metadata=runtime_metadata,
+        )
+        self._analysis_text = None
+        self._analysis_prompt = None
+        self._suggest_prompt = None
+
+        return CodeReviewInput(
+            code=code,
+            language=language,
+            context=context_description,
+            metadata=metadata,
+        )
+
+    def _build_workflow(self) -> StateGraph:
+        graph: StateGraph = StateGraph(dict)
+        graph.add_node("initialize", self._initialize_state)
+        graph.add_node("analyze", self._analyze_code)
+        graph.add_node("synthesize", self._synthesize_feedback)
+        graph.add_node("finalize", self._finalize_output)
+
+        graph.set_entry_point("initialize")
+        graph.add_edge("initialize", "analyze")
+        graph.add_edge("analyze", "synthesize")
+        graph.add_edge("synthesize", "finalize")
+        graph.add_edge("finalize", END)
+
+        return graph
+
+    def _validate_output(self, result: Any) -> CodeReviewOutput:
+        return CodeReviewOutput.model_validate(result)
+
+    def _initialize_state(self, payload: Dict[str, Any]) -> Dict[str, Any]:
+        runtime = self._runtime or _RuntimeSettings(
+            model=self._default_model,
+            temperature=self._default_temperature,
+            context_size="file_only",
+            strategy_name="standard",
+            language="python",
+            sample_code_path=None,
+            metadata={},
+        )
+        input_model = CodeReviewInput.model_validate(payload)
+        return {
+            "input": input_model,
+            "settings": runtime,
+            "issues": [],
+            "suggestions": [],
+            "summary": "",
+            "analysis": "",
+            "analysis_structured": {},
+            "test_config": runtime.metadata,
+        }
+
+    def _analyze_code(self, state: Dict[str, Any]) -> Dict[str, Any]:
+        runtime: _RuntimeSettings = state["settings"]
+        input_model: CodeReviewInput = state["input"]
+
+        prompt = self._render_prompt(
+            "analyze",
+            {
+                "code": input_model.code,
+                "language": runtime.language,
+                "context": input_model.context or "",
+            },
+        )
+        self._analysis_prompt = prompt
+        analysis_text = self._invoke_strategy(prompt, runtime)
+        self._analysis_text = analysis_text
+        structured = self._parse_analysis(analysis_text)
+        issues = [self._convert_issue(item) for item in structured.get("issues", [])]
+        summary = structured.get("summary") or self._summarize_issues(issues)
+
+        state.update(
+            {
+                "analysis": analysis_text,
+                "analysis_structured": structured,
+                "issues": issues,
+                "summary": summary,
+            }
+        )
+        return state
+
+    def _synthesize_feedback(self, state: Dict[str, Any]) -> Dict[str, Any]:
+        runtime: _RuntimeSettings = state["settings"]
+        structured = state.get("analysis_structured", {})
+        suggestions = structured.get("suggestions")
+
+        if suggestions is None:
+            prompt = self._render_prompt(
+                "suggest",
+                {
+                    "analysis": state.get("analysis", ""),
+                    "code": state["input"].code,
+                },
+            )
+            self._suggest_prompt = prompt
+            suggestions_text = self._invoke_strategy(prompt, runtime)
+            suggestions = self._parse_suggestions(suggestions_text)
+        else:
+            self._suggest_prompt = None
+
+        normalized = self._normalize_suggestions(suggestions)
+        state["suggestions"] = normalized
+        if not state.get("summary"):
+            state["summary"] = self._summarize_issues(state.get("issues", []))
+        return state
+
+    def _finalize_output(self, state: Dict[str, Any]) -> CodeReviewOutput:
+        issues: List[CodeIssue] = state.get("issues", [])
+        suggestions: List[str] = state.get("suggestions", [])
+        summary: str = state.get("summary") or self._summarize_issues(issues)
+        evaluation_text = self._build_evaluation_text(summary, issues, suggestions)
+
+        metadata = {
+            "strategy": state["settings"].strategy_name,
+            "model": state["settings"].model,
+            "temperature": state["settings"].temperature,
+            "context_size": state["settings"].context_size,
+            "language": state["settings"].language,
+            "sample_code_path": state["settings"].sample_code_path,
+            "analysis_prompt": self._analysis_prompt,
+            "analysis_raw": self._analysis_text,
+            "suggest_prompt": self._suggest_prompt,
+            "test_config": state.get("test_config"),
+        }
+
+        clean_metadata: Dict[str, Any] = {}
+        for key, value in metadata.items():
+            if value is None:
+                continue
+            if isinstance(value, str) and not value.strip():
+                continue
+            clean_metadata[key] = value
+        return CodeReviewOutput(
+            summary=summary,
+            issues=issues,
+            suggestions=suggestions,
+            evaluation_text=evaluation_text,
+            metadata=clean_metadata,
+        )
+
+    def _render_prompt(self, name: str, context: Mapping[str, Any]) -> str:
+        template_source = self._prompt_templates().get(name) or self.DEFAULT_PROMPTS[name]
+        template = Template(template_source)
+        return template.render(**context).strip()
+
+    def _prompt_templates(self) -> Mapping[str, str]:
+        extra = getattr(self.config, "model_extra", {})
+        prompts = extra.get("prompts")
+        if isinstance(prompts, Mapping):
+            return {str(key): str(value) for key, value in prompts.items()}
+        return {}
+
+    def _invoke_strategy(self, prompt: str, runtime: _RuntimeSettings) -> str:
+        strategy = self._resolve_strategy(runtime.strategy_name)
+        parameters = {"temperature": runtime.temperature}
+        try:
+            return self._await_coroutine(
+                strategy.generate(
+                    prompt,
+                    model=runtime.model,
+                    config=parameters,
+                )
+            )
+        except Exception as exc:  # pragma: no cover - defensive guard
+            raise WorkflowExecutionError("Generation strategy failed.") from exc
+
+    def _resolve_strategy(self, name: str) -> GenerationStrategy:
+        try:
+            return get_strategy(name)
+        except ValueError as exc:
+            raise WorkflowExecutionError(f"Unknown generation strategy: {name}") from exc
+
+    def _await_coroutine(self, coroutine: Any) -> str:
+        try:
+            return asyncio.run(coroutine)
+        except RuntimeError:
+            loop = asyncio.new_event_loop()
+            try:
+                return loop.run_until_complete(coroutine)
+            finally:
+                loop.close()
+
+    def _parse_analysis(self, response: str) -> Dict[str, Any]:
+        cleaned = self._strip_code_fence(response)
+        try:
+            data = json.loads(cleaned)
+        except json.JSONDecodeError:
+            return {
+                "summary": cleaned.strip() or "No significant issues identified.",
+                "issues": [],
+                "suggestions": [],
+            }
+        if not isinstance(data, Mapping):
+            return {
+                "summary": str(data),
+                "issues": [],
+                "suggestions": [],
+            }
+        normalized: Dict[str, Any] = {
+            "summary": str(data.get("summary", "")).strip(),
+        }
+        raw_issues = data.get("issues") or []
+        if isinstance(raw_issues, Mapping):
+            raw_issues = list(raw_issues.values())
+        normalized["issues"] = list(raw_issues)
+        normalized["suggestions"] = data.get("suggestions")
+        return normalized
+
+    def _parse_suggestions(self, response: str) -> Iterable[str]:
+        cleaned = self._strip_code_fence(response)
+        try:
+            data = json.loads(cleaned)
+        except json.JSONDecodeError:
+            return cleaned.splitlines()
+        if isinstance(data, Mapping):
+            candidate = data.get("suggestions")
+        else:
+            candidate = data
+        if isinstance(candidate, Mapping):
+            return candidate.values()
+        if isinstance(candidate, Iterable) and not isinstance(candidate, (str, bytes)):
+            return candidate
+        return [str(candidate)]
+
+    def _normalize_suggestions(self, suggestions: Any) -> List[str]:
+        if suggestions is None:
+            return []
+        if isinstance(suggestions, Iterable) and not isinstance(suggestions, (str, bytes)):
+            candidates = suggestions
+        else:
+            candidates = [suggestions]
+        normalized: List[str] = []
+        for candidate in candidates:
+            if not isinstance(candidate, str):
+                candidate = str(candidate)
+            stripped = candidate.strip()
+            if stripped:
+                normalized.append(stripped)
+        return normalized
+
+    def _convert_issue(self, payload: Any) -> CodeIssue:
+        if isinstance(payload, CodeIssue):
+            return payload
+        if not isinstance(payload, Mapping):
+            return CodeIssue(description=str(payload))
+        data = {
+            "type": payload.get("type") or payload.get("category") or "general",
+            "severity": payload.get("severity") or payload.get("level") or "medium",
+            "description": payload.get("description")
+            or payload.get("details")
+            or payload.get("message")
+            or "Issue requires attention.",
+            "line_number": payload.get("line_number")
+            or payload.get("line")
+            or payload.get("lineNo"),
+            "suggestion": payload.get("suggestion") or payload.get("fix"),
+        }
+        return CodeIssue(**data)
+
+    def _summarize_issues(self, issues: Iterable[CodeIssue]) -> str:
+        issue_list = list(issues)
+        if not issue_list:
+            return "No significant issues identified."
+        severity_counts: Dict[str, int] = {}
+        for issue in issue_list:
+            severity_counts[issue.severity] = severity_counts.get(issue.severity, 0) + 1
+        parts = [
+            f"Identified {len(issue_list)} issue{'s' if len(issue_list) != 1 else ''}.",
+            "Severity breakdown: "
+            + ", ".join(f"{key}: {value}" for key, value in sorted(severity_counts.items())),
+        ]
+        return " ".join(parts)
+
+    def _build_evaluation_text(
+        self, summary: str, issues: Iterable[CodeIssue], suggestions: Iterable[str]
+    ) -> str:
+        lines = [summary]
+        issues_list = list(issues)
+        if issues_list:
+            lines.append("Issues:")
+            for issue in issues_list:
+                prefix = f"- ({issue.severity}) {issue.description}"
+                if issue.line_number is not None:
+                    prefix += f" [line {issue.line_number}]"
+                if issue.suggestion:
+                    prefix += f" Suggested fix: {issue.suggestion}"
+                lines.append(prefix)
+        suggestions_list = list(suggestions)
+        if suggestions_list:
+            lines.append("Suggestions:")
+            lines.extend(f"- {item}" for item in suggestions_list)
+        return "\n".join(lines).strip()
+
+    def _strip_code_fence(self, value: str) -> str:
+        stripped = value.strip()
+        if stripped.startswith("```") and stripped.endswith("```"):
+            stripped = stripped.strip("`")
+            if stripped.startswith("json"):
+                stripped = stripped[4:]
+        return stripped.strip()
+
+    def _load_source(self, workflow_config: WorkflowConfig) -> tuple[str, Optional[str]]:
+        sample_path = workflow_config.sample_code_path
+        if not sample_path:
+            msg = "Workflow configuration must define sample_code_path for code review."
+            raise WorkflowExecutionError(msg)
+        file_path = Path(sample_path)
+        if not file_path.is_absolute():
+            file_path = Path.cwd() / file_path
+        try:
+            content = file_path.read_text(encoding="utf-8")
+        except FileNotFoundError as exc:  # pragma: no cover - depends on env
+            msg = f"Sample code file not found: {file_path}"
+            raise WorkflowExecutionError(msg) from exc
+        except OSError as exc:  # pragma: no cover - depends on env
+            msg = f"Failed to read sample code: {file_path}"
+            raise WorkflowExecutionError(msg) from exc
+        return content, str(file_path)
+
+    def _context_description(
+        self, context_size: str, workflow_config: WorkflowConfig
+    ) -> Optional[str]:
+        context_map = {
+            "file_only": "Review limited to the provided file.",
+            "full_module": "Consider surrounding module context for the review.",
+        }
+        if context_size in context_map:
+            return context_map[context_size]
+        extra = getattr(workflow_config, "model_extra", {})
+        descriptions = extra.get("context_descriptions")
+        if isinstance(descriptions, Mapping):
+            candidate = descriptions.get(context_size)
+            if isinstance(candidate, str):
+                return candidate.strip() or None
+        return None
+
+    def _resolve_language(
+        self, values: Mapping[str, Any], workflow_config: WorkflowConfig
+    ) -> str:
+        if "language" in values and isinstance(values["language"], str):
+            return values["language"].strip() or "python"
+        extra = getattr(workflow_config, "model_extra", {})
+        language = extra.get("language")
+        if isinstance(language, str) and language.strip():
+            return language.strip()
+        return "python"
+
+    def _coerce_float(self, value: Any, default: float) -> float:
+        try:
+            return float(value)
+        except (TypeError, ValueError):
+            return default
+
+
+__all__ = [
+    "CodeIssue",
+    "CodeReviewInput",
+    "CodeReviewOutput",
+    "CodeReviewWorkflow",
+]
diff --git a/tests/conftest.py b/tests/conftest.py
new file mode 100644
index 0000000000000000000000000000000000000000..caea044daa3d84b87a8ca598c5205b93e3246110
--- /dev/null
+++ b/tests/conftest.py
@@ -0,0 +1,32 @@
+"""Pytest configuration for test suite."""
+from __future__ import annotations
+
+import asyncio
+import inspect
+import sys
+from pathlib import Path
+
+ROOT = Path(__file__).resolve().parents[1]
+if str(ROOT) not in sys.path:
+    sys.path.insert(0, str(ROOT))
+
+
+def pytest_configure(config) -> None:  # pragma: no cover - pytest hook
+    config.addinivalue_line("markers", "asyncio: mark test as requiring asyncio event loop")
+
+
+def pytest_pyfunc_call(pyfuncitem):  # pragma: no cover - pytest hook
+    if asyncio.iscoroutinefunction(pyfuncitem.obj):
+        loop = asyncio.new_event_loop()
+        try:
+            signature = inspect.signature(pyfuncitem.obj)
+            kwargs = {
+                name: pyfuncitem.funcargs[name]
+                for name in signature.parameters
+                if name in pyfuncitem.funcargs
+            }
+            loop.run_until_complete(pyfuncitem.obj(**kwargs))
+        finally:
+            loop.close()
+        return True
+    return None
diff --git a/tests/fixtures/experiment_results.json b/tests/fixtures/experiment_results.json
new file mode 100644
index 0000000000000000000000000000000000000000..30781f606e3cf54f6deae5ac292b923f17cddbef
--- /dev/null
+++ b/tests/fixtures/experiment_results.json
@@ -0,0 +1 @@
+{"experiments": []}
diff --git a/tests/integration/test_cli.py b/tests/integration/test_cli.py
new file mode 100644
index 0000000000000000000000000000000000000000..5063e100ab3eba1d6bb81cb7b561e2aceed22c3e
--- /dev/null
+++ b/tests/integration/test_cli.py
@@ -0,0 +1,292 @@
+from __future__ import annotations
+
+import json
+from pathlib import Path
+
+import yaml
+
+from typer.testing import CliRunner
+
+from tesseract_flow.cli.main import app
+from tesseract_flow.core.config import ExperimentConfig, ExperimentRun, TestResult
+from tesseract_flow.evaluation.metrics import DimensionScore, QualityScore
+from tesseract_flow.experiments.taguchi import generate_test_configs
+
+runner = CliRunner()
+
+
+def _build_completed_run(config: ExperimentConfig) -> ExperimentRun:
+    test_configs = generate_test_configs(config)
+    run = ExperimentRun(
+        experiment_id="demo-run",
+        config=config,
+        test_configurations=test_configs,
+    ).mark_running()
+
+    for index, test_config in enumerate(test_configs, start=1):
+        quality = QualityScore(
+            dimension_scores={"clarity": DimensionScore(score=min(0.5 + index * 0.05, 1.0))},
+            evaluator_model="stub-model",
+        )
+        result = TestResult(
+            test_number=test_config.test_number,
+            config=test_config,
+            quality_score=quality,
+            cost=0.01 * index,
+            latency=100.0 * index,
+            workflow_output=f"output {index}",
+        )
+        run = run.record_result(result)
+
+    return run.mark_completed()
+
+
+def _build_running_run(config: ExperimentConfig, *, completed: int = 2) -> ExperimentRun:
+    test_configs = generate_test_configs(config)
+    run = ExperimentRun(
+        experiment_id="demo-run",
+        config=config,
+        test_configurations=test_configs,
+    ).mark_running()
+
+    for index, test_config in enumerate(test_configs[:completed], start=1):
+        quality = QualityScore(
+            dimension_scores={"clarity": DimensionScore(score=0.6 + index * 0.05)},
+            evaluator_model="stub-model",
+        )
+        result = TestResult(
+            test_number=test_config.test_number,
+            config=test_config,
+            quality_score=quality,
+            cost=0.02 * index,
+            latency=120.0 * index,
+            workflow_output=f"output {index}",
+        )
+        run = run.record_result(result)
+
+    return run
+
+
+def test_experiment_run_dry_run() -> None:
+    config_path = Path("examples/code_review/experiment_config.yaml")
+
+    result = runner.invoke(
+        app,
+        ["experiment", "run", str(config_path), "--dry-run"],
+    )
+
+    assert result.exit_code == 0
+    assert "Generated 8 test configurations" in result.stdout
+    assert "Test Configurations" in result.stdout
+
+
+def test_experiment_run_success(monkeypatch, tmp_path) -> None:
+    config_path = Path("examples/code_review/experiment_config.yaml")
+    config = ExperimentConfig.from_yaml(config_path)
+    completed_run = _build_completed_run(config)
+
+    async def fake_execute(*args, **kwargs):
+        output_path: Path = kwargs["output_path"]
+        output_path.write_text(
+            completed_run.model_dump_json(indent=2, exclude_none=True),
+            encoding="utf-8",
+        )
+        return completed_run
+
+    monkeypatch.setattr("tesseract_flow.cli.experiment._execute_experiment", fake_execute)
+
+    output_path = tmp_path / "results.json"
+    cache_dir = tmp_path / "cache"
+    result = runner.invoke(
+        app,
+        [
+            "experiment",
+            "run",
+            str(config_path),
+            "--output",
+            str(output_path),
+            "--use-cache",
+            "--record-cache",
+            "--cache-dir",
+            str(cache_dir),
+        ],
+    )
+
+    assert result.exit_code == 0
+    assert output_path.exists()
+    stdout = result.stdout
+    assert "All tests completed successfully" in stdout
+    assert "Experiment Summary" in stdout
+    assert f"tesseract analyze {output_path}" in stdout
+    assert "Best test" in stdout
+
+
+def test_experiment_analyze_command(tmp_path) -> None:
+    config_path = Path("examples/code_review/experiment_config.yaml")
+    config = ExperimentConfig.from_yaml(config_path)
+    completed_run = _build_completed_run(config)
+
+    results_path = tmp_path / "results.json"
+    results_path.write_text(
+        completed_run.model_dump_json(indent=2, exclude_none=True),
+        encoding="utf-8",
+    )
+
+    export_path = tmp_path / "optimal.yaml"
+    result = runner.invoke(
+        app,
+        [
+            "experiment",
+            "analyze",
+            str(results_path),
+            "--format",
+            "json",
+            "--export",
+            str(export_path),
+        ],
+        color=False,
+    )
+
+    assert result.exit_code == 0
+    stdout = result.stdout
+    json_output, *_ = stdout.split("âœ“ Exported", 1)
+    data = json.loads(json_output)
+    assert data["experiment_id"] == "demo-run"
+    assert data["baseline"]["config"]
+    assert data["optimal_observed"]["config"]
+    assert data["recommended_configuration"]
+    assert export_path.exists()
+    exported = yaml.safe_load(export_path.read_text(encoding="utf-8"))
+    assert exported["workflow"] == config.workflow
+
+
+def test_visualize_pareto_command(tmp_path) -> None:
+    config_path = Path("examples/code_review/experiment_config.yaml")
+    config = ExperimentConfig.from_yaml(config_path)
+    completed_run = _build_completed_run(config)
+
+    results_path = tmp_path / "results.json"
+    results_path.write_text(
+        completed_run.model_dump_json(indent=2, exclude_none=True),
+        encoding="utf-8",
+    )
+
+    output_path = tmp_path / "pareto.png"
+    result = runner.invoke(
+        app,
+        [
+            "visualize",
+            "pareto",
+            str(results_path),
+            "--output",
+            str(output_path),
+            "--budget",
+            "0.06",
+        ],
+        color=False,
+    )
+
+    assert result.exit_code == 0
+    assert output_path.exists()
+
+    stdout = result.stdout
+    assert "Computed Pareto frontier" in stdout
+    assert "Pareto-optimal configurations" in stdout
+    assert "Recommendation" in stdout
+
+
+def test_experiment_status_running(tmp_path) -> None:
+    config_path = Path("examples/code_review/experiment_config.yaml")
+    config = ExperimentConfig.from_yaml(config_path)
+    running_run = _build_running_run(config, completed=3)
+
+    results_path = tmp_path / "partial.json"
+    results_path.write_text(
+        running_run.model_dump_json(indent=2, exclude_none=True),
+        encoding="utf-8",
+    )
+
+    result = runner.invoke(
+        app,
+        [
+            "experiment",
+            "status",
+            str(results_path),
+        ],
+        color=False,
+    )
+
+    assert result.exit_code == 0
+    stdout = result.stdout
+    assert "Progress Overview" in stdout
+    assert "Pending tests" in stdout
+    assert "Last recorded result" in stdout
+    assert "Resume with" in stdout
+
+
+def test_experiment_status_completed_details(tmp_path) -> None:
+    config_path = Path("examples/code_review/experiment_config.yaml")
+    config = ExperimentConfig.from_yaml(config_path)
+    completed_run = _build_completed_run(config)
+
+    results_path = tmp_path / "complete.json"
+    results_path.write_text(
+        completed_run.model_dump_json(indent=2, exclude_none=True),
+        encoding="utf-8",
+    )
+
+    result = runner.invoke(
+        app,
+        [
+            "experiment",
+            "status",
+            str(results_path),
+            "--details",
+        ],
+        color=False,
+    )
+
+    assert result.exit_code == 0
+    stdout = result.stdout
+    assert "Recorded Results" in stdout
+    assert "Experiment completed" in stdout
+
+
+def test_experiment_validate_success() -> None:
+    config_path = Path("examples/code_review/experiment_config.yaml")
+
+    result = runner.invoke(
+        app,
+        [
+            "experiment",
+            "validate",
+            str(config_path),
+        ],
+        color=False,
+    )
+
+    assert result.exit_code == 0
+    stdout = result.stdout
+    assert "Configuration is valid" in stdout
+    assert "Use --show-tests" in stdout
+
+
+def test_experiment_validate_failure(tmp_path) -> None:
+    invalid_path = tmp_path / "invalid.yaml"
+    invalid_path.write_text(
+        "name: invalid\nworkflow: code_review\nvariables: []\n",
+        encoding="utf-8",
+    )
+
+    result = runner.invoke(
+        app,
+        [
+            "experiment",
+            "validate",
+            str(invalid_path),
+        ],
+        color=False,
+    )
+
+    assert result.exit_code != 0
+    assert "Invalid experiment configuration" in result.stdout
diff --git a/tests/integration/test_cli_help.py b/tests/integration/test_cli_help.py
new file mode 100644
index 0000000000000000000000000000000000000000..7a8a8ee53e1080ca8ba5209b4b024b78a46020e8
--- /dev/null
+++ b/tests/integration/test_cli_help.py
@@ -0,0 +1,23 @@
+from typer.testing import CliRunner
+
+from tesseract_flow.cli.main import app
+
+runner = CliRunner()
+
+
+def test_root_help_includes_global_options() -> None:
+    result = runner.invoke(app, ["--help"], color=False)
+
+    assert result.exit_code == 0
+    stdout = result.stdout
+    assert "--log-level" in stdout
+    assert "--version" in stdout
+
+
+def test_experiment_help_lists_management_commands() -> None:
+    result = runner.invoke(app, ["experiment", "--help"], color=False)
+
+    assert result.exit_code == 0
+    stdout = result.stdout
+    assert "status" in stdout
+    assert "validate" in stdout
diff --git a/tests/integration/test_code_review.py b/tests/integration/test_code_review.py
new file mode 100644
index 0000000000000000000000000000000000000000..8a04dc9b1d6122a5c0dbea2584861095145a069e
--- /dev/null
+++ b/tests/integration/test_code_review.py
@@ -0,0 +1,118 @@
+"""Integration tests for the code review workflow."""
+from __future__ import annotations
+
+import json
+from pathlib import Path
+from typing import Any, Dict, List
+
+import pytest
+
+from tesseract_flow.core.config import (
+    ExperimentConfig,
+    TestConfiguration,
+    UtilityWeights,
+    Variable,
+    WorkflowConfig,
+)
+from tesseract_flow.workflows import CodeReviewOutput, CodeReviewWorkflow
+
+
+class _FakeStrategy:
+    def __init__(self) -> None:
+        self.calls: List[Dict[str, Any]] = []
+
+    async def generate(
+        self,
+        prompt: str,
+        *,
+        model: str,
+        config: Dict[str, Any] | None = None,
+    ) -> str:
+        self.calls.append({"prompt": prompt, "model": model, "config": dict(config or {})})
+        payload = {
+            "summary": "Highlights spacing issues and missing validation.",
+            "issues": [
+                {
+                    "type": "style",
+                    "severity": "HIGH",
+                    "line_number": "3",
+                    "description": "The function body is not formatted with consistent spacing.",
+                    "suggestion": "Apply PEP8 formatting and add blank lines.",
+                },
+                {
+                    "type": "bug",
+                    "severity": "medium",
+                    "description": "Inputs are not validated before use.",
+                },
+            ],
+            "suggestions": [
+                "Add input validation to guard against None values.",
+                "Format the function with standard Python conventions.",
+            ],
+        }
+        return json.dumps(payload)
+
+
+def test_code_review_workflow_generates_structured_feedback(
+    monkeypatch: pytest.MonkeyPatch, tmp_path: Path
+) -> None:
+    sample_code = """\ndef add(a,b):\n    return a+b\n"""
+    sample_path = tmp_path / "sample.py"
+    sample_path.write_text(sample_code, encoding="utf-8")
+
+    workflow_config = WorkflowConfig(sample_code_path=str(sample_path))
+    workflow = CodeReviewWorkflow(config=workflow_config)
+
+    fake_strategy = _FakeStrategy()
+    monkeypatch.setattr(
+        "tesseract_flow.workflows.code_review.get_strategy",
+        lambda name: fake_strategy,
+    )
+
+    experiment_config = ExperimentConfig(
+        name="code_review_experiment",
+        workflow="code_review",
+        variables=[
+            Variable(name="temperature", level_1=0.3, level_2=0.7),
+            Variable(name="model", level_1="model/a", level_2="model/b"),
+            Variable(name="context_size", level_1="file_only", level_2="full_module"),
+            Variable(name="generation_strategy", level_1="standard", level_2="chain_of_thought"),
+        ],
+        utility_weights=UtilityWeights(quality=1.0, cost=0.1, time=0.05),
+        workflow_config=workflow_config,
+    )
+
+    test_config = TestConfiguration(
+        test_number=1,
+        config_values={
+            "temperature": 0.3,
+            "model": "model/a",
+            "context_size": "file_only",
+            "generation_strategy": "standard",
+        },
+        workflow="code_review",
+    )
+
+    prepared_input = workflow.prepare_input(test_config, experiment_config)
+    assert prepared_input.language == "python"
+    assert "test_number" in prepared_input.metadata
+    assert prepared_input.metadata["config_values"]["model"] == "model/a"
+
+    output = workflow.run(prepared_input)
+    assert isinstance(output, CodeReviewOutput)
+    assert output.summary.startswith("Identified 2 issues") or "Highlights" in output.summary
+    assert len(output.issues) == 2
+    assert output.issues[0].severity == "high"
+    assert output.issues[0].line_number == 3
+    assert output.issues[1].description.startswith("Inputs are not validated")
+    assert len(output.suggestions) == 2
+    assert "Suggestions:" in output.evaluation_text
+
+    metadata = output.metadata
+    assert metadata["strategy"] == "standard"
+    assert metadata["model"] == "model/a"
+    assert metadata["context_size"] == "file_only"
+    assert Path(metadata["sample_code_path"]) == sample_path
+    assert workflow.last_run_metadata is not None
+    assert fake_strategy.calls[0]["config"]["temperature"] == pytest.approx(0.3)
+    assert sample_code.strip() in fake_strategy.calls[0]["prompt"]
diff --git a/tests/integration/test_evaluation.py b/tests/integration/test_evaluation.py
new file mode 100644
index 0000000000000000000000000000000000000000..7ff32b6c0ffd8830b04f05aa03548995aaf8e511
--- /dev/null
+++ b/tests/integration/test_evaluation.py
@@ -0,0 +1,73 @@
+"""Integration tests for the rubric evaluator."""
+from __future__ import annotations
+
+import json
+from typing import Any, Dict
+
+import pytest
+
+from tesseract_flow.core.exceptions import EvaluationError
+from tesseract_flow.core.types import RubricDimension
+from tesseract_flow.evaluation.rubric import RubricEvaluator
+
+
+@pytest.mark.asyncio
+async def test_rubric_evaluator_parses_litellm_response(monkeypatch: pytest.MonkeyPatch) -> None:
+    captured_kwargs: Dict[str, Any] = {}
+
+    async def fake_acompletion(*args: Any, **kwargs: Any) -> Dict[str, Any]:
+        captured_kwargs.update(kwargs)
+        payload = {
+            "clarity": {"score": "9", "reasoning": "Clear messaging."},
+            "accuracy": {"score": 8, "reasoning": "Minor nitpicks."},
+            "completeness": {"score": 7.5, "reasoning": "Covers most areas."},
+            "usefulness": {"score": 10, "reasoning": "Extremely actionable."},
+        }
+        return {"choices": [{"message": {"content": json.dumps(payload)}}]}
+
+    monkeypatch.setattr(
+        "tesseract_flow.evaluation.rubric.litellm.acompletion",
+        fake_acompletion,
+    )
+
+    custom_rubric: Dict[str, RubricDimension] = {
+        "clarity": RubricEvaluator.DEFAULT_RUBRIC["clarity"],
+        "accuracy": RubricEvaluator.DEFAULT_RUBRIC["accuracy"],
+        "completeness": RubricEvaluator.DEFAULT_RUBRIC["completeness"],
+        "usefulness": RubricEvaluator.DEFAULT_RUBRIC["usefulness"],
+    }
+
+    evaluator = RubricEvaluator(model="test/model", temperature=0.4)
+    score = await evaluator.evaluate(
+        "Final workflow output",
+        rubric=custom_rubric,
+        temperature=0.2,
+        extra_instructions="Highlight security issues first.",
+    )
+
+    assert score.evaluator_model == "test/model"
+    assert score.overall_score == pytest.approx((0.9 + 0.8 + 0.75 + 1.0) / 4)
+    assert score.dimension_scores["usefulness"].score == pytest.approx(1.0)
+    assert "raw_response" in score.metadata
+    assert score.metadata["temperature"] == 0.2
+    assert score.metadata["rubric"] == custom_rubric
+
+    assert captured_kwargs["model"] == "test/model"
+    assert pytest.approx(captured_kwargs["temperature"]) == 0.2
+    assert captured_kwargs["response_format"] == {"type": "json_object"}
+    assert isinstance(captured_kwargs["messages"], list)
+
+
+@pytest.mark.asyncio
+async def test_rubric_evaluator_raises_on_invalid_json(monkeypatch: pytest.MonkeyPatch) -> None:
+    async def fake_acompletion(*args: Any, **kwargs: Any) -> Dict[str, Any]:  # pragma: no cover - trivial stub
+        return {"choices": [{"message": {"content": "not-json"}}]}
+
+    monkeypatch.setattr(
+        "tesseract_flow.evaluation.rubric.litellm.acompletion",
+        fake_acompletion,
+    )
+
+    evaluator = RubricEvaluator()
+    with pytest.raises(EvaluationError):
+        await evaluator.evaluate("Example output")
diff --git a/tests/unit/test_analysis.py b/tests/unit/test_analysis.py
new file mode 100644
index 0000000000000000000000000000000000000000..cccd28fd2d1418c4bad433c6109e25da4db5282b
--- /dev/null
+++ b/tests/unit/test_analysis.py
@@ -0,0 +1,14 @@
+import pytest
+
+from tesseract_flow.experiments.analysis import calculate_quality_improvement
+
+
+def test_quality_improvement_percentage() -> None:
+    improvement = calculate_quality_improvement(0.5, 0.6)
+    assert improvement == pytest.approx(20.0)
+
+
+def test_quality_improvement_handles_missing_values() -> None:
+    assert calculate_quality_improvement(None, 0.6) is None
+    assert calculate_quality_improvement(0.5, None) is None
+    assert calculate_quality_improvement(0.0, 0.6) is None
diff --git a/tests/unit/test_base_workflow.py b/tests/unit/test_base_workflow.py
new file mode 100644
index 0000000000000000000000000000000000000000..cb277ad032a8cbb5216c66d5949aa45055618dbc
--- /dev/null
+++ b/tests/unit/test_base_workflow.py
@@ -0,0 +1,85 @@
+from __future__ import annotations
+
+from typing import Any
+
+import pytest
+from pydantic import BaseModel
+
+from tesseract_flow.core.base_workflow import BaseWorkflowService
+from tesseract_flow.core.config import WorkflowConfig
+from tesseract_flow.core.exceptions import WorkflowExecutionError
+
+
+class ExampleInput(BaseModel):
+    value: int
+
+
+class ExampleOutput(BaseModel):
+    doubled: int
+
+
+class _CompiledWorkflow:
+    def invoke(self, payload: dict[str, Any]) -> dict[str, Any]:
+        return {"doubled": payload["value"] * 2}
+
+
+class _StateGraph:
+    def compile(self) -> _CompiledWorkflow:
+        return _CompiledWorkflow()
+
+
+class ExampleWorkflow(BaseWorkflowService[ExampleInput, ExampleOutput]):
+    def _build_workflow(self) -> Any:  # type: ignore[override]
+        self._build_count = getattr(self, "_build_count", 0) + 1
+        return _StateGraph()
+
+    def _validate_output(self, result: Any) -> ExampleOutput:
+        return ExampleOutput.model_validate(result)
+
+
+class FailingCompiled:
+    def invoke(self, payload: dict[str, Any]) -> dict[str, Any]:
+        raise RuntimeError("boom")
+
+
+class FailingGraph:
+    def compile(self) -> FailingCompiled:
+        return FailingCompiled()
+
+
+class FailingWorkflow(BaseWorkflowService[ExampleInput, ExampleOutput]):
+    def _build_workflow(self) -> Any:  # type: ignore[override]
+        return FailingGraph()
+
+    def _validate_output(self, result: Any) -> ExampleOutput:
+        return ExampleOutput.model_validate(result)
+
+
+def test_base_workflow_run_executes_and_records_metadata() -> None:
+    workflow = ExampleWorkflow(config=WorkflowConfig())
+    output = workflow.run(ExampleInput(value=3))
+    assert output.doubled == 6
+    metadata = workflow.last_run_metadata
+    assert metadata is not None
+    assert metadata["completed_at"] >= metadata["started_at"]
+    assert metadata["duration_seconds"] >= 0.0
+    assert workflow._build_count == 1
+
+    # Cached compiled graph should be reused
+    workflow.run(ExampleInput(value=2))
+    assert workflow._build_count == 1
+
+
+def test_base_workflow_reset_rebuilds_graph() -> None:
+    workflow = ExampleWorkflow(config=WorkflowConfig())
+    workflow.run(ExampleInput(value=1))
+    workflow.reset()
+    assert workflow.last_run_metadata is None
+    workflow.run(ExampleInput(value=2))
+    assert workflow._build_count == 2
+
+
+def test_base_workflow_wraps_exceptions() -> None:
+    workflow = FailingWorkflow(config=WorkflowConfig())
+    with pytest.raises(WorkflowExecutionError):
+        workflow.run(ExampleInput(value=1))
diff --git a/tests/unit/test_cache.py b/tests/unit/test_cache.py
new file mode 100644
index 0000000000000000000000000000000000000000..f279dda7e8332f6117f3b1a63c7744cc4498cb7d
--- /dev/null
+++ b/tests/unit/test_cache.py
@@ -0,0 +1,38 @@
+"""Unit tests for evaluation caching utilities."""
+from __future__ import annotations
+
+from pathlib import Path
+
+import pytest
+
+from tesseract_flow.core.exceptions import CacheError
+from tesseract_flow.evaluation.cache import FileCacheBackend, build_cache_key
+
+
+def test_build_cache_key_is_deterministic() -> None:
+    key_one = build_cache_key("prompt", "model", 0.1234567)
+    key_two = build_cache_key("prompt", "model", 0.1234567)
+    assert key_one == key_two
+
+
+def test_build_cache_key_changes_with_input() -> None:
+    base = build_cache_key("prompt", "model", 0.1)
+    assert base != build_cache_key("prompt!", "model", 0.1)
+    assert base != build_cache_key("prompt", "other-model", 0.1)
+    assert base != build_cache_key("prompt", "model", 0.2)
+
+
+def test_file_cache_backend_round_trip(tmp_path: Path) -> None:
+    backend = FileCacheBackend(tmp_path)
+    key = "abc123"
+    assert backend.get(key) is None
+    backend.set(key, "{\"value\": 1}")
+    assert backend.get(key) == "{\"value\": 1}"
+    backend.clear()
+    assert backend.get(key) is None
+
+
+def test_file_cache_backend_rejects_empty_key(tmp_path: Path) -> None:
+    backend = FileCacheBackend(tmp_path)
+    with pytest.raises(CacheError):
+        backend.get("   ")  # type: ignore[arg-type]
diff --git a/tests/unit/test_config.py b/tests/unit/test_config.py
new file mode 100644
index 0000000000000000000000000000000000000000..268e96c89891f55a8b6d4a4a8d7a4f2c497baa49
--- /dev/null
+++ b/tests/unit/test_config.py
@@ -0,0 +1,419 @@
+from pathlib import Path
+
+import pytest
+
+from tesseract_flow.core.config import (
+    ExperimentConfig,
+    ExperimentMetadata,
+    ExperimentRun,
+    TestConfiguration,
+    TestResult,
+    UtilityWeights,
+    Variable,
+    WorkflowConfig,
+)
+from tesseract_flow.core.exceptions import ConfigurationError
+from tesseract_flow.evaluation.metrics import DimensionScore, QualityScore
+
+
+@pytest.fixture
+def variables() -> list[Variable]:
+    return [
+        Variable(name="temperature", level_1=0.3, level_2=0.7),
+        Variable(name="model", level_1="gpt-4", level_2="claude"),
+        Variable(name="context", level_1="file", level_2="module"),
+        Variable(name="strategy", level_1="standard", level_2="cot"),
+    ]
+
+
+@pytest.fixture
+def experiment_config(variables: list[Variable]) -> ExperimentConfig:
+    return ExperimentConfig(
+        name="experiment",
+        workflow="code_review",
+        variables=variables,
+    )
+
+
+def _quality_score(overall: float = 0.75) -> QualityScore:
+    return QualityScore(
+        dimension_scores={
+            "clarity": DimensionScore(score=overall, reasoning=""),
+        },
+        evaluator_model="gpt-4o",
+    )
+
+
+def _test_configurations(config: ExperimentConfig) -> list[TestConfiguration]:
+    base_values = {
+        variable.name: variable.level_1 for variable in config.variables
+    }
+    return [
+        TestConfiguration(
+            test_number=index + 1,
+            config_values=dict(base_values),
+            workflow=config.workflow,
+        )
+        for index in range(8)
+    ]
+
+
+def test_variable_valid_creation() -> None:
+    variable = Variable(name="temperature", level_1=0.3, level_2=0.7)
+    assert variable.level_1 == 0.3
+    assert variable.level_2 == 0.7
+
+
+def test_variable_rejects_identical_levels() -> None:
+    with pytest.raises(ValueError):
+        Variable(name="temperature", level_1=0.3, level_2=0.3)
+
+
+def test_variable_rejects_mismatched_types() -> None:
+    with pytest.raises(ValueError):
+        Variable(name="temperature", level_1=0.3, level_2="0.7")
+
+
+def test_variable_validate_name_helpers() -> None:
+    with pytest.raises(ValueError):
+        Variable._validate_name("")
+
+
+def test_variable_rejects_invalid_names() -> None:
+    with pytest.raises(ValueError):
+        Variable(name=" temperature ", level_1=0.1, level_2=0.9)
+
+    with pytest.raises(ValueError):
+        Variable(name="_hidden", level_1=0.1, level_2=0.9)
+
+    with pytest.raises(ValueError):
+        Variable(name="invalid-name", level_1=0.1, level_2=0.9)
+
+
+def test_utility_weights_requires_positive_weight() -> None:
+    with pytest.raises(ValueError):
+        UtilityWeights(quality=0.0, cost=0.0, time=0.0)
+
+
+def test_workflow_config_allows_null_sample_code_path() -> None:
+    config = WorkflowConfig(sample_code_path=None)
+    assert config.sample_code_path is None
+
+
+def test_workflow_config_rejects_blank_sample_code_path() -> None:
+    with pytest.raises(ValueError):
+        WorkflowConfig(sample_code_path="   ")
+
+
+def test_experiment_metadata_from_config_generates_stable_hash(
+    experiment_config: ExperimentConfig,
+) -> None:
+    metadata = ExperimentMetadata.from_config(
+        experiment_config, dependencies={"pydantic": "2.0.0"}
+    )
+    assert metadata.config_hash
+    assert metadata.seed == experiment_config.seed
+
+    second = ExperimentMetadata.from_config(
+        experiment_config, dependencies={"pydantic": "2.0.0"}
+    )
+    assert metadata.config_hash == second.config_hash
+
+
+def test_experiment_metadata_normalizes_sources(experiment_config: ExperimentConfig) -> None:
+    metadata = ExperimentMetadata.from_config(
+        experiment_config,
+        non_deterministic_sources=["llm_sampling", "  rate_limits  ", "llm_sampling"],
+    )
+    assert metadata.non_deterministic_sources == ["llm_sampling", "rate_limits"]
+
+
+def test_experiment_config_validates_variable_count(variables: list[Variable]) -> None:
+    config = ExperimentConfig(
+        name="experiment",
+        workflow="code_review",
+        variables=variables,
+        utility_weights=UtilityWeights(),
+    )
+    assert config.name == "experiment"
+    assert len(config.variables) == 4
+
+
+@pytest.mark.parametrize("count", [3, 8])
+def test_experiment_config_rejects_invalid_variable_counts(
+    count: int, variables: list[Variable]
+) -> None:
+    if count > len(variables):
+        extra = [
+            Variable(name=f"extra_{index}", level_1=index, level_2=index + 1)
+            for index in range(len(variables), count)
+        ]
+        selected = variables + extra
+    else:
+        selected = variables[:count]
+    with pytest.raises(ValueError):
+        ExperimentConfig(
+            name="experiment",
+            workflow="code_review",
+            variables=selected,
+        )
+
+
+def test_experiment_config_requires_unique_variable_names(variables: list[Variable]) -> None:
+    duplicate = variables + [Variable(name="temperature", level_1=0.1, level_2=0.9)]
+    with pytest.raises(ValueError):
+        ExperimentConfig(
+            name="experiment",
+            workflow="code_review",
+            variables=duplicate,
+        )
+
+
+def test_experiment_config_coerces_workflow_config_dict(tmp_path: Path, variables: list[Variable]) -> None:
+    config = ExperimentConfig(
+        name="experiment",
+        workflow="code_review",
+        variables=variables,
+        workflow_config={
+            "rubric": {
+                "clarity": {
+                    "description": "Clarity of review",
+                    "scale": "1-5",
+                }
+            },
+            "sample_code_path": " examples/code.py ",
+            "extra_setting": True,
+        },
+    )
+    assert isinstance(config.workflow_config, WorkflowConfig)
+    assert config.workflow_config.sample_code_path == "examples/code.py"
+    assert config.workflow_config.rubric["clarity"]["scale"] == "1-5"
+    assert getattr(config.workflow_config, "extra_setting") is True
+
+    output_path = tmp_path / "config.yaml"
+    config.to_yaml(output_path)
+    loaded = ExperimentConfig.from_yaml(output_path)
+    assert loaded.name == config.name
+    assert len(loaded.variables) == len(config.variables)
+
+
+@pytest.mark.parametrize("field", ["name", "workflow"])
+def test_experiment_config_rejects_blank_identifiers(
+    field: str, variables: list[Variable]
+) -> None:
+    kwargs: dict[str, object] = {
+        "name": "experiment",
+        "workflow": "code_review",
+        "variables": variables,
+    }
+    kwargs[field] = " test "
+    with pytest.raises(ValueError):
+        ExperimentConfig(**kwargs)  # type: ignore[arg-type]
+
+    kwargs[field] = ""
+    with pytest.raises(ValueError):
+        ExperimentConfig(**kwargs)  # type: ignore[arg-type]
+
+
+def test_experiment_config_from_yaml_requires_mapping(tmp_path: Path) -> None:
+    path = tmp_path / "invalid.yaml"
+    path.write_text("- item\n", encoding="utf-8")
+    with pytest.raises(ConfigurationError):
+        ExperimentConfig.from_yaml(path)
+
+
+def test_test_configuration_without_context_skips_validation() -> None:
+    config = TestConfiguration(test_number=1, config_values={"temperature": 0.1}, workflow="wf")
+    assert config.config_values["temperature"] == 0.1
+
+
+def test_test_configuration_requires_matching_variables() -> None:
+    with pytest.raises(ValueError):
+        TestConfiguration.model_validate(
+            {
+                "test_number": 1,
+                "config_values": {"temperature": 0.1},
+                "workflow": "wf",
+            },
+            context={
+                "variable_levels": {
+                    "temperature": {0.1, 0.9},
+                    "model": {"gpt-4", "claude"},
+                }
+            },
+        )
+
+
+def test_test_configuration_requires_defined_levels() -> None:
+    with pytest.raises(ValueError):
+        TestConfiguration.model_validate(
+            {
+                "test_number": 1,
+                "config_values": {
+                    "temperature": 0.5,
+                    "model": "gpt-4",
+                },
+                "workflow": "wf",
+            },
+            context={
+                "variable_levels": {
+                    "temperature": {0.1, 0.9},
+                    "model": {"gpt-4", "claude"},
+                }
+            },
+        )
+
+
+def test_test_configuration_rejects_extra_variables() -> None:
+    with pytest.raises(ValueError):
+        TestConfiguration.model_validate(
+            {
+                "test_number": 1,
+                "config_values": {"temperature": 0.1, "extra": "value"},
+                "workflow": "wf",
+            },
+            context={"variable_levels": {"temperature": {0.1, 0.9}}},
+        )
+
+
+def test_test_configuration_rejects_invalid_workflow_identifier() -> None:
+    with pytest.raises(ValueError):
+        TestConfiguration(test_number=1, config_values={}, workflow="  invalid  ")
+
+    with pytest.raises(ValueError):
+        TestConfiguration(test_number=1, config_values={}, workflow="")
+
+
+def test_test_result_trims_workflow_output(experiment_config: ExperimentConfig) -> None:
+    test_config = _test_configurations(experiment_config)[0]
+    score = _quality_score(0.8)
+    result = TestResult(
+        test_number=1,
+        config=test_config,
+        quality_score=score,
+        cost=0.01,
+        latency=1200,
+        workflow_output="   Analysis complete   ",
+    )
+    assert result.workflow_output == "Analysis complete"
+
+
+def test_experiment_run_records_result_and_updates_utility(
+    experiment_config: ExperimentConfig,
+) -> None:
+    test_configs = _test_configurations(experiment_config)
+    run = ExperimentRun(
+        experiment_id="exp-1",
+        config=experiment_config,
+        test_configurations=test_configs,
+    ).mark_running()
+
+    score = _quality_score(0.9)
+    result = TestResult(
+        test_number=1,
+        config=test_configs[0],
+        quality_score=score,
+        cost=0.015,
+        latency=1500,
+        workflow_output="Result",
+    )
+
+    updated = run.record_result(result)
+    assert pytest.approx(updated.results[0].utility, rel=1e-6) == pytest.approx(0.9)
+    assert updated.metadata["normalization"]["cost"] == {"min": 0.015, "max": 0.015}
+    assert updated.metadata["normalization"]["latency"] == {"min": 1500, "max": 1500}
+    assert updated.baseline_result is not None
+    assert updated.baseline_result.test_number == 1
+    assert updated.baseline_quality == pytest.approx(0.9)
+    assert updated.quality_improvement_pct == pytest.approx(0.0)
+
+
+def test_experiment_run_requires_unique_results(experiment_config: ExperimentConfig) -> None:
+    test_configs = _test_configurations(experiment_config)
+    run = ExperimentRun(
+        experiment_id="exp-1",
+        config=experiment_config,
+        test_configurations=test_configs,
+    ).mark_running()
+
+    score = _quality_score(0.7)
+    result = TestResult(
+        test_number=1,
+        config=test_configs[0],
+        quality_score=score,
+        cost=0.01,
+        latency=1200,
+        workflow_output="Result",
+    )
+
+    run = run.record_result(result)
+    with pytest.raises(ValueError):
+        run.record_result(result)
+
+
+def test_experiment_run_completion_requires_all_results(
+    experiment_config: ExperimentConfig,
+) -> None:
+    test_configs = _test_configurations(experiment_config)
+    run = ExperimentRun(
+        experiment_id="exp-1",
+        config=experiment_config,
+        test_configurations=test_configs,
+    ).mark_running()
+
+    with pytest.raises(ValueError):
+        run.mark_completed()
+
+
+def test_experiment_run_mark_failed_requires_error_message(
+    experiment_config: ExperimentConfig,
+) -> None:
+    test_configs = _test_configurations(experiment_config)
+    run = ExperimentRun(
+        experiment_id="exp-1",
+        config=experiment_config,
+        test_configurations=test_configs,
+    ).mark_running()
+
+    with pytest.raises(ValueError):
+        run.mark_failed("   ")
+
+
+def test_experiment_run_can_complete_after_all_results(
+    experiment_config: ExperimentConfig,
+) -> None:
+    test_configs = _test_configurations(experiment_config)
+    run = ExperimentRun(
+        experiment_id="exp-1",
+        config=experiment_config,
+        test_configurations=test_configs,
+    ).mark_running()
+
+    baseline_quality: float | None = None
+    for index, config_item in enumerate(test_configs, start=1):
+        score = _quality_score(0.6 + index * 0.03)
+        result = TestResult(
+            test_number=config_item.test_number,
+            config=config_item,
+            quality_score=score,
+            cost=0.01 + index * 0.001,
+            latency=1000 + index * 25,
+            workflow_output=f"Result {index}",
+        )
+        if baseline_quality is None:
+            baseline_quality = score.overall_score
+        run = run.record_result(result)
+
+    completed = run.mark_completed()
+    assert completed.status == "COMPLETED"
+    assert completed.completed_at is not None
+    utilities = [result.utility for result in completed.results]
+    assert utilities == sorted(utilities)
+    assert completed.metadata["normalization"]["cost"]["min"] == pytest.approx(0.011)
+    assert completed.metadata["normalization"]["latency"]["max"] == pytest.approx(1200)
+    assert completed.baseline_result is not None
+    assert completed.baseline_result.test_number == 1
+    assert baseline_quality is not None
+    assert completed.baseline_quality == pytest.approx(baseline_quality)
+    assert completed.quality_improvement_pct is not None
+    assert completed.quality_improvement_pct > 0
diff --git a/tests/unit/test_error_handling.py b/tests/unit/test_error_handling.py
new file mode 100644
index 0000000000000000000000000000000000000000..756c96484c0b6d64a8e607649086ac17ffe32342
--- /dev/null
+++ b/tests/unit/test_error_handling.py
@@ -0,0 +1,197 @@
+"""Unit tests covering error handling and retry behaviour."""
+
+from __future__ import annotations
+
+import asyncio
+import json
+from pathlib import Path
+from typing import Any, Dict, Iterable
+
+import pytest
+from pydantic import BaseModel
+
+from tesseract_flow.core.base_workflow import BaseWorkflowService
+from tesseract_flow.core.config import (
+    ExperimentConfig,
+    TestConfiguration,
+    UtilityWeights,
+    Variable,
+)
+from tesseract_flow.core.exceptions import ConfigurationError, EvaluationError, ExperimentError
+from tesseract_flow.evaluation.metrics import DimensionScore, QualityScore
+from tesseract_flow.evaluation.rubric import RubricEvaluator
+from tesseract_flow.experiments.executor import ExperimentExecutor
+
+
+@pytest.mark.asyncio
+async def test_rubric_evaluator_retries_before_success(monkeypatch: pytest.MonkeyPatch) -> None:
+    attempts = 0
+
+    async def failing_completion(**_: Any) -> Dict[str, Any]:
+        nonlocal attempts
+        attempts += 1
+        if attempts < 3:
+            raise RuntimeError("temporary failure")
+        payload = {
+            dimension: {"score": 8, "reasoning": "ok"}
+            for dimension in RubricEvaluator.DEFAULT_RUBRIC
+        }
+        return {"choices": [{"message": {"content": json.dumps(payload)}}]}
+
+    async def no_sleep(_: float) -> None:  # pragma: no cover - deterministic stub
+        return None
+
+    monkeypatch.setattr(
+        "tesseract_flow.evaluation.rubric.litellm.acompletion",
+        failing_completion,
+    )
+    monkeypatch.setattr("tesseract_flow.evaluation.rubric.asyncio.sleep", no_sleep)
+    monkeypatch.setattr("tesseract_flow.evaluation.rubric.random.uniform", lambda *_: 0.0)
+
+    evaluator = RubricEvaluator(max_retries=3, retry_base_delay=0.1)
+    score = await evaluator.evaluate("Example output")
+
+    assert pytest.approx(score.overall_score, rel=1e-5) == 0.8
+    assert attempts == 3
+
+
+@pytest.mark.asyncio
+async def test_rubric_evaluator_raises_after_max_retries(monkeypatch: pytest.MonkeyPatch) -> None:
+    async def always_fail(**_: Any) -> Dict[str, Any]:
+        raise RuntimeError("boom")
+
+    async def no_sleep(_: float) -> None:  # pragma: no cover - deterministic stub
+        return None
+
+    monkeypatch.setattr(
+        "tesseract_flow.evaluation.rubric.litellm.acompletion",
+        always_fail,
+    )
+    monkeypatch.setattr("tesseract_flow.evaluation.rubric.asyncio.sleep", no_sleep)
+    monkeypatch.setattr("tesseract_flow.evaluation.rubric.random.uniform", lambda *_: 0.0)
+
+    evaluator = RubricEvaluator(max_retries=2, retry_base_delay=0.01)
+
+    with pytest.raises(EvaluationError) as exc_info:
+        await evaluator.evaluate("Example output")
+
+    assert "failed after 2 attempts" in str(exc_info.value)
+
+
+def test_experiment_config_from_yaml_provides_error_details(tmp_path: Path) -> None:
+    payload = {"name": "exp", "variables": []}
+    path = tmp_path / "config.yaml"
+    path.write_text(json.dumps(payload), encoding="utf-8")
+
+    with pytest.raises(ConfigurationError) as exc_info:
+        ExperimentConfig.from_yaml(path)
+
+    assert exc_info.value.details
+    assert any("workflow" in detail.lower() for detail in exc_info.value.details)
+
+
+def test_executor_persists_partial_results_on_failure(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
+    variables = [
+        Variable(name="temperature", level_1=0.1, level_2=0.9),
+        Variable(name="model", level_1="gpt-4", level_2="claude"),
+        Variable(name="context", level_1="file", level_2="module"),
+        Variable(name="strategy", level_1="standard", level_2="cot"),
+    ]
+    config = ExperimentConfig(
+        name="experiment",
+        workflow="code_review",
+        variables=variables,
+        utility_weights=UtilityWeights(quality=1.0, cost=0.0, time=0.0),
+    )
+
+    def fake_generate(_: ExperimentConfig) -> list[TestConfiguration]:
+        tests: list[TestConfiguration] = []
+        for index in range(8):
+            config_values = {var.name: var.level_1 for var in variables}
+            if index % 2 == 1:
+                config_values[variables[0].name] = variables[0].level_2
+            tests.append(
+                TestConfiguration(
+                    test_number=index + 1,
+                    workflow="code_review",
+                    config_values=config_values,
+                )
+            )
+        return tests
+
+    monkeypatch.setattr(
+        "tesseract_flow.experiments.executor.generate_test_configs",
+        fake_generate,
+    )
+
+    class DummyOutput(BaseModel):
+        review: str
+        cost: float = 0.0
+        latency_ms: float = 5.0
+
+        def render_for_evaluation(self) -> str:
+            return self.review
+
+    class DummyWorkflow(BaseWorkflowService[TestConfiguration, DummyOutput]):
+        def __init__(self, outputs: Iterable[str]) -> None:
+            super().__init__()
+            self._outputs = list(outputs)
+            self._index = 0
+
+        def _build_workflow(self) -> Any:  # type: ignore[override]
+            workflow = self
+
+            class _Graph:
+                def compile(self_inner) -> DummyWorkflow:
+                    return workflow
+
+            return _Graph()
+
+        def _validate_output(self, result: Any) -> DummyOutput:
+            return DummyOutput(review=result)
+
+        # BaseWorkflowService expects compiled graph with invoke method.
+        def invoke(self, payload: Dict[str, Any]) -> str:  # type: ignore[override]
+            value = self._outputs[self._index]
+            self._index += 1
+            return value
+
+    class FakeEvaluator:
+        def __init__(self) -> None:
+            self.calls = 0
+
+        async def evaluate(self, *_: Any, **__: Any) -> QualityScore:
+            self.calls += 1
+            if self.calls == 2:
+                raise EvaluationError("forced failure")
+            return QualityScore(
+                dimension_scores={
+                    "clarity": DimensionScore(score=0.7, reasoning="ok"),
+                },
+                evaluator_model="test-model",
+            )
+
+    outputs = [f"Run {idx}" for idx in range(8)]
+    workflow = DummyWorkflow(outputs)
+
+    def resolver(_: str, __: ExperimentConfig) -> DummyWorkflow:
+        return workflow
+
+    executor = ExperimentExecutor(resolver, FakeEvaluator())
+
+    persist_path = tmp_path / "partial_results.json"
+
+    with pytest.raises(ExperimentError):
+        asyncio.run(
+            executor.run(
+                config,
+                persist_path=persist_path,
+            )
+        )
+
+    assert persist_path.exists()
+    run_state = ExperimentExecutor.load_run(persist_path)
+    assert run_state.status == "FAILED"
+    assert len(run_state.results) == 1
+    assert run_state.error == "forced failure"
+
diff --git a/tests/unit/test_executor.py b/tests/unit/test_executor.py
new file mode 100644
index 0000000000000000000000000000000000000000..86211afcbf95e4755ee468b7c128eace41db53f3
--- /dev/null
+++ b/tests/unit/test_executor.py
@@ -0,0 +1,237 @@
+from __future__ import annotations
+
+from datetime import datetime, timezone
+from pathlib import Path
+from typing import Any, Dict, List
+
+import pytest
+
+from tesseract_flow.core.config import (
+    ExperimentConfig,
+    ExperimentMetadata,
+    ExperimentRun,
+    TestConfiguration,
+    TestResult,
+    UtilityWeights,
+    Variable,
+    WorkflowConfig,
+)
+from tesseract_flow.core.exceptions import ExperimentError
+from tesseract_flow.evaluation.metrics import DimensionScore, QualityScore
+from tesseract_flow.experiments.executor import ExperimentExecutor
+from tesseract_flow.experiments.taguchi import generate_test_configs
+
+
+class DummyWorkflowOutput:
+    def __init__(self, *, evaluation_text: str, cost: float, latency_ms: float, metadata: Dict[str, Any] | None = None) -> None:
+        self.evaluation_text = evaluation_text
+        self.cost = cost
+        self.latency_ms = latency_ms
+        self.metadata = metadata or {}
+
+    def render_for_evaluation(self) -> str:
+        return self.evaluation_text
+
+
+class StubWorkflowService:
+    def __init__(self, outputs: List[DummyWorkflowOutput]) -> None:
+        self._outputs = outputs
+        self._index = 0
+        self.last_run_metadata: Dict[str, Any] | None = None
+        self.config = WorkflowConfig()
+
+    def prepare_input(self, test_config: TestConfiguration, experiment_config: ExperimentConfig | None) -> Dict[str, Any]:
+        return {"test_number": test_config.test_number, "config": test_config.config_values}
+
+    def run(self, input_data: Dict[str, Any]) -> DummyWorkflowOutput:
+        output = self._outputs[self._index]
+        self._index += 1
+        self.last_run_metadata = {"duration_seconds": output.latency_ms / 1000.0}
+        return output
+
+
+class StubEvaluator:
+    def __init__(self) -> None:
+        self.calls = 0
+
+    async def evaluate(
+        self,
+        workflow_output: str,
+        rubric: Dict[str, Any] | None = None,
+        *,
+        extra_instructions: str | None = None,
+    ) -> QualityScore:
+        self.calls += 1
+        score = QualityScore(
+            dimension_scores={"clarity": DimensionScore(score=0.8, reasoning="solid")},
+            evaluator_model="stub-model",
+        )
+        return score.with_metadata(call=self.calls, rubric=rubric)
+
+
+@pytest.fixture
+def experiment_config() -> ExperimentConfig:
+    variables = [
+        Variable(name="temperature", level_1=0.3, level_2=0.7),
+        Variable(name="model", level_1="gpt-4", level_2="claude"),
+        Variable(name="context", level_1="file", level_2="module"),
+        Variable(name="strategy", level_1="standard", level_2="cot"),
+    ]
+    return ExperimentConfig(
+        name="executor_test",
+        workflow="code_review",
+        variables=variables,
+        utility_weights=UtilityWeights(),
+    )
+
+
+def _quality_score(value: float) -> QualityScore:
+    return QualityScore(
+        dimension_scores={"clarity": DimensionScore(score=value, reasoning="")},
+        evaluator_model="stub",
+    )
+
+
+@pytest.mark.asyncio
+async def test_run_single_test_returns_result(experiment_config: ExperimentConfig) -> None:
+    test_config = generate_test_configs(experiment_config)[0]
+    outputs = [
+        DummyWorkflowOutput(
+            evaluation_text="Comprehensive review",
+            cost=0.005,
+            latency_ms=2100,
+            metadata={"tokens": 1500},
+        )
+    ]
+    service = StubWorkflowService(outputs)
+    evaluator = StubEvaluator()
+
+    executor = ExperimentExecutor(lambda workflow, config: service, evaluator)
+    result = await executor.run_single_test(
+        test_config,
+        service,
+        evaluator,
+        experiment_config=experiment_config,
+    )
+
+    assert result.cost == pytest.approx(0.005)
+    assert result.latency == pytest.approx(2100)
+    assert "workflow" in result.metadata
+    assert result.quality_score.overall_score == pytest.approx(0.8)
+    assert result.metadata["evaluation"]["model"] == "stub-model"
+
+
+@pytest.mark.asyncio
+async def test_run_executes_all_tests_and_persists(
+    experiment_config: ExperimentConfig, tmp_path: Path
+) -> None:
+    outputs = [
+        DummyWorkflowOutput(
+            evaluation_text=f"Review {index}",
+            cost=0.001 * index,
+            latency_ms=1500 + (index * 10),
+            metadata={"index": index},
+        )
+        for index in range(1, 9)
+    ]
+    service = StubWorkflowService(outputs)
+    evaluator = StubEvaluator()
+    executor = ExperimentExecutor(lambda workflow, config: service, evaluator)
+
+    progress: List[int] = []
+
+    def track_progress(current: int, total: int) -> None:
+        assert total == 8
+        progress.append(current)
+
+    output_path = tmp_path / "experiment_run.json"
+    run = await executor.run(
+        experiment_config,
+        progress_callback=track_progress,
+        persist_path=output_path,
+    )
+
+    assert run.status == "COMPLETED"
+    assert len(run.results) == 8
+    assert progress[0] == 0
+    assert progress[-1] == 8
+    assert output_path.exists()
+
+    loaded = ExperimentExecutor.load_run(output_path)
+    assert loaded.experiment_id == run.experiment_id
+    assert len(loaded.results) == 8
+
+
+@pytest.mark.asyncio
+async def test_run_resumes_from_partial_state(
+    experiment_config: ExperimentConfig, tmp_path: Path
+) -> None:
+    test_configs = generate_test_configs(experiment_config)
+    base_run = ExperimentRun(
+        experiment_id="resume-test",
+        config=experiment_config,
+        test_configurations=test_configs,
+        experiment_metadata=ExperimentMetadata.from_config(
+            experiment_config, dependencies={"pydantic": "2"}
+        ),
+    ).mark_running(started_at=datetime.now(tz=timezone.utc))
+
+    first_result = TestResult(
+        test_number=1,
+        config=test_configs[0],
+        quality_score=_quality_score(0.7),
+        cost=0.004,
+        latency=2000,
+        workflow_output="First",
+    )
+    second_result = TestResult(
+        test_number=2,
+        config=test_configs[1],
+        quality_score=_quality_score(0.75),
+        cost=0.005,
+        latency=2100,
+        workflow_output="Second",
+    )
+
+    base_run = base_run.record_result(first_result)
+    base_run = base_run.record_result(second_result)
+
+    outputs = [
+        DummyWorkflowOutput(
+            evaluation_text=f"Resume {index}",
+            cost=0.002 * index,
+            latency_ms=1800 + (index * 5),
+        )
+        for index in range(3, 9)
+    ]
+    service = StubWorkflowService(outputs)
+    evaluator = StubEvaluator()
+    executor = ExperimentExecutor(lambda workflow, config: service, evaluator)
+
+    resumed = await executor.run(
+        experiment_config,
+        resume_from=base_run,
+        persist_path=tmp_path / "resume.json",
+    )
+
+    assert resumed.status == "COMPLETED"
+    assert len(resumed.results) == 8
+    assert resumed.experiment_id == "resume-test"
+    assert resumed.results[0].utility != 0.0
+
+
+@pytest.mark.asyncio
+async def test_resume_rejects_mismatched_config(experiment_config: ExperimentConfig) -> None:
+    test_configs = generate_test_configs(experiment_config)
+    run = ExperimentRun(
+        experiment_id="resume-fail",
+        config=experiment_config,
+        test_configurations=test_configs,
+    )
+
+    modified_config = experiment_config.model_copy(update={"name": "different"})
+    executor = ExperimentExecutor(lambda workflow, config: StubWorkflowService([]), StubEvaluator())
+
+    with pytest.raises(ExperimentError):
+        await executor.run(modified_config, resume_from=run)
+
diff --git a/tests/unit/test_main_effects.py b/tests/unit/test_main_effects.py
new file mode 100644
index 0000000000000000000000000000000000000000..ab819d46a6d73779fb770b6f523214766f009b79
--- /dev/null
+++ b/tests/unit/test_main_effects.py
@@ -0,0 +1,106 @@
+import pytest
+import yaml
+
+from tesseract_flow.core.config import ExperimentConfig, ExperimentRun, TestResult, UtilityWeights, Variable
+from tesseract_flow.evaluation.metrics import DimensionScore, QualityScore
+from tesseract_flow.experiments.analysis import (
+    MainEffectsAnalyzer,
+    calculate_quality_improvement,
+    compare_configurations,
+    export_optimal_config,
+    identify_optimal_config,
+)
+from tesseract_flow.experiments.taguchi import generate_test_configs
+
+
+def _quality_score(value: float) -> QualityScore:
+    return QualityScore(
+        dimension_scores={"clarity": DimensionScore(score=value, reasoning="solid")},
+        evaluator_model="stub",
+    )
+
+
+def test_main_effects_analysis_and_exports(tmp_path) -> None:
+    variables = [
+        Variable(name="temperature", level_1=0.2, level_2=0.8),
+        Variable(name="model", level_1="gpt-4", level_2="claude"),
+        Variable(name="context", level_1="file", level_2="module"),
+        Variable(name="strategy", level_1="standard", level_2="cot"),
+    ]
+    config = ExperimentConfig(
+        name="analysis-test",
+        workflow="code_review",
+        variables=variables,
+        utility_weights=UtilityWeights(quality=1.0, cost=0.0, time=0.0),
+    )
+    test_configs = generate_test_configs(config)
+    run = ExperimentRun(
+        experiment_id="analysis-run",
+        config=config,
+        test_configurations=test_configs,
+    ).mark_running()
+
+    quality_values = {
+        1: 0.55,
+        2: 0.60,
+        3: 0.65,
+        4: 0.62,
+        5: 0.78,
+        6: 0.80,
+        7: 0.88,
+        8: 0.90,
+    }
+
+    for test_config in test_configs:
+        quality = _quality_score(quality_values[test_config.test_number])
+        result = TestResult(
+            test_number=test_config.test_number,
+            config=test_config,
+            quality_score=quality,
+            cost=0.01,
+            latency=1000.0,
+            workflow_output=f"output {test_config.test_number}",
+        )
+        run = run.record_result(result)
+
+    run = run.mark_completed()
+
+    main_effects = MainEffectsAnalyzer.compute(
+        run.results, config.variables, experiment_id=run.experiment_id
+    )
+    assert main_effects.experiment_id == "analysis-run"
+    total_contribution = sum(effect.contribution_pct for effect in main_effects.effects.values())
+    assert total_contribution == pytest.approx(100.0, abs=0.5)
+
+    temperature_effect = main_effects.effects["temperature"]
+    assert temperature_effect.effect_size > 0
+    assert temperature_effect.avg_level_2 > temperature_effect.avg_level_1
+
+    recommended = identify_optimal_config(main_effects, config.variables)
+    assert recommended["temperature"] == 0.8
+    assert recommended["model"] in {"gpt-4", "claude"}
+
+    optimal_result = max(run.results, key=lambda item: item.utility)
+    comparison = compare_configurations(
+        run.baseline_config.config_values, optimal_result.config.config_values
+    )
+    assert comparison["temperature"]["changed"] is True
+    assert comparison["model"]["changed"] in {True, False}
+
+    improvement = calculate_quality_improvement(
+        run.baseline_quality, optimal_result.quality_score.overall_score
+    )
+    assert improvement == pytest.approx(run.quality_improvement_pct)
+
+    export_path = tmp_path / "optimal.yaml"
+    exported = export_optimal_config(
+        recommended,
+        export_path,
+        experiment_name=config.name,
+        workflow=config.workflow,
+    )
+    assert exported == export_path
+    exported_payload = yaml.safe_load(export_path.read_text(encoding="utf-8"))
+    assert exported_payload["experiment"] == "analysis-test"
+    assert exported_payload["workflow"] == "code_review"
+    assert exported_payload["configuration"]["temperature"] == 0.8
diff --git a/tests/unit/test_metrics.py b/tests/unit/test_metrics.py
new file mode 100644
index 0000000000000000000000000000000000000000..48a3ef9a4e0720af965db290c47986b1f505ab36
--- /dev/null
+++ b/tests/unit/test_metrics.py
@@ -0,0 +1,55 @@
+"""Unit tests for evaluation metrics models."""
+from __future__ import annotations
+
+from datetime import datetime, timezone
+
+import pytest
+
+from tesseract_flow.evaluation.metrics import DimensionScore, QualityScore
+
+
+def test_quality_score_computes_overall_mean() -> None:
+    dimension_scores = {
+        "clarity": DimensionScore(score=0.8, reasoning="Clear"),
+        "accuracy": DimensionScore(score=0.6, reasoning="Minor mistakes"),
+        "usefulness": DimensionScore(score=1.0, reasoning="Highly actionable"),
+    }
+    score = QualityScore(
+        dimension_scores=dimension_scores,
+        evaluator_model="anthropic/claude-3.5-sonnet",
+        overall_score=0.0,  # Should be recalculated to the mean
+    )
+    expected_mean = (0.8 + 0.6 + 1.0) / 3
+    assert score.overall_score == pytest.approx(expected_mean)
+
+
+def test_quality_score_rejects_empty_dimensions() -> None:
+    with pytest.raises(ValueError):
+        QualityScore(
+            dimension_scores={},
+            evaluator_model="anthropic/claude-3.5-sonnet",
+        )
+
+
+def test_dimension_score_normalizes_reasoning_whitespace() -> None:
+    score = DimensionScore(score=0.75, reasoning="  Detailed reasoning.  ")
+    assert score.reasoning == "Detailed reasoning."
+
+
+def test_quality_score_requires_model_identifier() -> None:
+    dimension_scores = {"clarity": DimensionScore(score=0.9)}
+    with pytest.raises(ValueError):
+        QualityScore(dimension_scores=dimension_scores, evaluator_model="  ")
+
+
+def test_quality_score_uses_utc_timestamp() -> None:
+    dimension_scores = {"clarity": DimensionScore(score=0.9)}
+    score = QualityScore(dimension_scores=dimension_scores, evaluator_model="test/model")
+    assert score.timestamp.tzinfo is timezone.utc
+    assert isinstance(score.timestamp, datetime)
+
+
+def test_quality_score_rejects_blank_dimension_name() -> None:
+    dimension_scores = {"   ": DimensionScore(score=0.5)}
+    with pytest.raises(ValueError):
+        QualityScore(dimension_scores=dimension_scores, evaluator_model="model/test")
diff --git a/tests/unit/test_pareto.py b/tests/unit/test_pareto.py
new file mode 100644
index 0000000000000000000000000000000000000000..ea564e90a5523792c4fad98e0c65284fd905705b
--- /dev/null
+++ b/tests/unit/test_pareto.py
@@ -0,0 +1,131 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pytest
+
+from tesseract_flow.core.config import TestConfiguration, TestResult
+from tesseract_flow.evaluation.metrics import DimensionScore, QualityScore
+from tesseract_flow.optimization.pareto import ParetoFrontier
+
+
+def _make_result(
+    test_number: int,
+    *,
+    quality: float,
+    cost: float,
+    latency: float,
+    workflow: str = "code_review",
+    utility: float | None = None,
+) -> TestResult:
+    config = TestConfiguration(
+        test_number=test_number,
+        config_values={"model": "gpt"},
+        workflow=workflow,
+    )
+    score = QualityScore(
+        dimension_scores={"overall": DimensionScore(score=quality)},
+        evaluator_model="test-model",
+    )
+    return TestResult(
+        test_number=test_number,
+        config=config,
+        quality_score=score,
+        cost=cost,
+        latency=latency,
+        utility=utility if utility is not None else quality,
+        workflow_output="ok",
+    )
+
+
+def test_compute_pareto_frontier_identifies_optimal_points() -> None:
+    results = [
+        _make_result(1, quality=0.70, cost=0.002, latency=120.0),
+        _make_result(2, quality=0.85, cost=0.004, latency=160.0),
+        _make_result(3, quality=0.83, cost=0.003, latency=140.0),
+        _make_result(4, quality=0.90, cost=0.006, latency=220.0),
+        _make_result(5, quality=0.78, cost=0.005, latency=180.0),
+    ]
+
+    frontier = ParetoFrontier.compute(results, experiment_id="run-123")
+
+    optimal_numbers = {point.test_number for point in frontier.optimal_points}
+    assert optimal_numbers == {1, 2, 3, 4}
+
+    dominated = {
+        point.test_number: point.dominated_by
+        for point in frontier.points
+        if not point.is_optimal
+    }
+    assert dominated.keys() == {5}
+    assert dominated[5] in {2, 3, 4}
+
+    assert frontier.x_axis == "cost"
+    assert frontier.y_axis == "quality"
+
+
+def test_compute_pareto_frontier_with_single_optimal() -> None:
+    results = [
+        _make_result(1, quality=0.92, cost=0.003, latency=150.0),
+        _make_result(2, quality=0.80, cost=0.004, latency=210.0),
+        _make_result(3, quality=0.78, cost=0.006, latency=240.0),
+    ]
+
+    frontier = ParetoFrontier.compute(results, experiment_id="run-456")
+
+    assert [point.test_number for point in frontier.optimal_points] == [1]
+    dominated = {point.test_number: point.dominated_by for point in frontier.points if not point.is_optimal}
+    assert dominated == {2: 1, 3: 1}
+
+
+def test_budget_helpers_identify_candidates() -> None:
+    results = [
+        _make_result(1, quality=0.70, cost=0.002, latency=120.0),
+        _make_result(2, quality=0.85, cost=0.004, latency=160.0),
+        _make_result(3, quality=0.83, cost=0.003, latency=140.0),
+        _make_result(4, quality=0.90, cost=0.006, latency=220.0),
+    ]
+    frontier = ParetoFrontier.compute(results, experiment_id="run-budget")
+
+    within = frontier.points_within_budget(0.0045)
+    assert [point.test_number for point in within] == [1, 2, 3]
+
+    best = frontier.best_within_budget(0.0045)
+    assert best is not None
+    assert best.test_number == 2
+
+    with pytest.raises(ValueError):
+        frontier.points_within_budget(-0.1)
+
+
+def test_visualize_generates_image(tmp_path: Path) -> None:
+    results = [
+        _make_result(1, quality=0.70, cost=0.002, latency=120.0),
+        _make_result(2, quality=0.85, cost=0.004, latency=160.0),
+        _make_result(3, quality=0.83, cost=0.003, latency=140.0),
+        _make_result(4, quality=0.90, cost=0.006, latency=220.0),
+    ]
+    frontier = ParetoFrontier.compute(results, experiment_id="run-visual")
+
+    output_path = tmp_path / "pareto.png"
+    image_path = frontier.visualize(output_path, budget_threshold=0.0045)
+
+    assert image_path == output_path
+    assert output_path.exists()
+
+
+def test_compute_with_latency_axis() -> None:
+    results = [
+        _make_result(1, quality=0.88, cost=0.004, latency=300.0),
+        _make_result(2, quality=0.86, cost=0.003, latency=220.0),
+        _make_result(3, quality=0.92, cost=0.006, latency=250.0),
+    ]
+
+    frontier = ParetoFrontier.compute(
+        results,
+        experiment_id="run-latency",
+        x_axis="latency",
+    )
+
+    assert frontier.x_axis == "latency"
+    assert {point.test_number for point in frontier.optimal_points} == {2, 3}
diff --git a/tests/unit/test_rubric.py b/tests/unit/test_rubric.py
new file mode 100644
index 0000000000000000000000000000000000000000..94a553591ddd6c5e1d2b2fadda03be06e40546d2
--- /dev/null
+++ b/tests/unit/test_rubric.py
@@ -0,0 +1,220 @@
+"""Unit tests for rubric evaluator utility methods."""
+from __future__ import annotations
+
+import json
+from typing import Dict
+
+import pytest
+
+from tesseract_flow.core.exceptions import EvaluationError
+from tesseract_flow.evaluation.rubric import RubricEvaluator
+
+
+class _InMemoryCache:
+    def __init__(self) -> None:
+        self.store: Dict[str, str] = {}
+
+    def get(self, key: str) -> str | None:
+        return self.store.get(key)
+
+    def set(self, key: str, value: str) -> None:
+        self.store[key] = value
+
+    def clear(self) -> None:  # pragma: no cover - helper API completeness
+        self.store.clear()
+
+
+def test_constructor_requires_model_identifier() -> None:
+    with pytest.raises(ValueError):
+        RubricEvaluator(model="   ")
+
+
+@pytest.mark.asyncio
+async def test_evaluate_requires_non_empty_output() -> None:
+    evaluator = RubricEvaluator()
+    with pytest.raises(EvaluationError):
+        await evaluator.evaluate("   ")
+
+
+@pytest.mark.asyncio
+async def test_evaluate_requires_non_empty_model_override() -> None:
+    evaluator = RubricEvaluator()
+    with pytest.raises(EvaluationError):
+        await evaluator.evaluate("valid output", model="  ")
+
+
+def test_extract_response_content_requires_choices() -> None:
+    evaluator = RubricEvaluator()
+    with pytest.raises(EvaluationError):
+        evaluator._extract_response_content({})  # type: ignore[arg-type]
+
+
+def test_extract_response_content_requires_message() -> None:
+    evaluator = RubricEvaluator()
+    response = {"choices": [{}]}
+    with pytest.raises(EvaluationError):
+        evaluator._extract_response_content(response)
+
+
+def test_extract_response_content_concatenates_list_parts() -> None:
+    evaluator = RubricEvaluator()
+    response = {
+        "choices": [
+            {
+                "message": {
+                    "content": [
+                        {"text": "part 1 "},
+                        {"text": "and part 2"},
+                    ]
+                }
+            }
+        ]
+    }
+    content = evaluator._extract_response_content(response)
+    assert content == "part 1 and part 2"
+
+
+def test_parse_dimension_scores_requires_all_dimensions() -> None:
+    evaluator = RubricEvaluator()
+    payload = {"clarity": {"score": 5}}
+    rubric = {"clarity": RubricEvaluator.DEFAULT_RUBRIC["clarity"], "accuracy": RubricEvaluator.DEFAULT_RUBRIC["accuracy"]}
+    with pytest.raises(EvaluationError):
+        evaluator._parse_dimension_scores(payload, rubric)
+
+
+def test_normalize_score_rejects_out_of_range_values() -> None:
+    evaluator = RubricEvaluator()
+    with pytest.raises(EvaluationError):
+        evaluator._normalize_score(11)
+
+
+def test_normalize_score_rejects_invalid_types() -> None:
+    evaluator = RubricEvaluator()
+    with pytest.raises(EvaluationError):
+        evaluator._normalize_score(["not", "numeric"])  # type: ignore[arg-type]
+
+
+def test_validate_temperature_bounds() -> None:
+    evaluator = RubricEvaluator()
+    with pytest.raises(ValueError):
+        evaluator._validate_temperature(-0.1)
+    with pytest.raises(ValueError):
+        evaluator._validate_temperature(1.5)
+
+
+def test_extract_response_content_supports_choice_objects() -> None:
+    evaluator = RubricEvaluator()
+
+    class _Message:
+        def __init__(self, content: str) -> None:
+            self.content = content
+
+    class _Choice:
+        def __init__(self, content: str) -> None:
+            self.message = _Message(content)
+
+    class _Response:
+        def __init__(self, content: str) -> None:
+            self.choices = [_Choice(content)]
+
+    response = _Response("object based content")
+    assert evaluator._extract_response_content(response) == "object based content"
+
+
+def test_extract_response_content_requires_string_content() -> None:
+    evaluator = RubricEvaluator()
+    response = {"choices": [{"message": {"content": 123}}]}
+    with pytest.raises(EvaluationError):
+        evaluator._extract_response_content(response)
+
+
+def test_parse_dimension_scores_accepts_numeric_entries() -> None:
+    evaluator = RubricEvaluator()
+    rubric = {"clarity": RubricEvaluator.DEFAULT_RUBRIC["clarity"]}
+    payload = {"clarity": 8}
+    scores = evaluator._parse_dimension_scores(payload, rubric)
+    assert scores["clarity"].score == pytest.approx(0.8)
+
+
+def test_normalize_score_accepts_unit_interval() -> None:
+    evaluator = RubricEvaluator()
+    assert evaluator._normalize_score(0.4) == pytest.approx(0.4)
+
+
+def test_normalize_score_rejects_empty_string() -> None:
+    evaluator = RubricEvaluator()
+    with pytest.raises(EvaluationError):
+        evaluator._normalize_score("   ")
+
+
+def test_normalize_score_rejects_non_finite() -> None:
+    evaluator = RubricEvaluator()
+    with pytest.raises(EvaluationError):
+        evaluator._normalize_score(float("nan"))
+
+def test_normalize_score_rejects_non_finite_string() -> None:
+    evaluator = RubricEvaluator()
+    with pytest.raises(EvaluationError):
+        evaluator._normalize_score("nan")
+
+
+@pytest.mark.asyncio
+async def test_evaluate_records_cache_entry(monkeypatch: pytest.MonkeyPatch) -> None:
+    payload = {
+        "clarity": {"score": 8, "reasoning": "Clear enough."},
+        "accuracy": {"score": 7, "reasoning": "Mostly correct."},
+        "completeness": {"score": 9, "reasoning": "Thorough."},
+        "usefulness": {"score": 6, "reasoning": "Actionable."},
+    }
+
+    async def fake_acompletion(*args: object, **kwargs: object) -> dict[str, object]:
+        return {"choices": [{"message": {"content": json.dumps(payload)}}]}
+
+    monkeypatch.setattr(
+        "tesseract_flow.evaluation.rubric.litellm.acompletion",
+        fake_acompletion,
+    )
+
+    cache = _InMemoryCache()
+    evaluator = RubricEvaluator(cache=cache, record_cache=True)
+    score = await evaluator.evaluate("Example output")
+
+    cache_key = score.metadata["cache_key"]
+    assert cache.store[cache_key] == json.dumps(payload)
+    assert score.metadata["cache_hit"] is False
+    assert score.metadata["cache_recorded"] is True
+
+
+@pytest.mark.asyncio
+async def test_evaluate_uses_cached_response(monkeypatch: pytest.MonkeyPatch) -> None:
+    payload = {
+        "clarity": {"score": 6, "reasoning": "Ok."},
+        "accuracy": {"score": 6, "reasoning": "Ok."},
+        "completeness": {"score": 6, "reasoning": "Ok."},
+        "usefulness": {"score": 6, "reasoning": "Ok."},
+    }
+
+    cache = _InMemoryCache()
+    cache.set("manual-key", json.dumps(payload))
+
+    async def failing_acompletion(*args: object, **kwargs: object) -> dict[str, object]:
+        raise AssertionError("LLM should not be invoked when cache is hit")
+
+    monkeypatch.setattr(
+        "tesseract_flow.evaluation.rubric.litellm.acompletion",
+        failing_acompletion,
+    )
+
+    evaluator = RubricEvaluator(cache=cache)
+    score = await evaluator.evaluate("Example output", use_cache=True, cache_key="manual-key")
+
+    assert score.metadata["cache_hit"] is True
+    assert "cache_recorded" not in score.metadata
+    assert score.overall_score == pytest.approx(0.6)
+
+
+@pytest.mark.asyncio
+async def test_evaluate_requires_cache_backend_when_requested() -> None:
+    evaluator = RubricEvaluator()
+    with pytest.raises(EvaluationError):
+        await evaluator.evaluate("Output", use_cache=True)
diff --git a/tests/unit/test_strategies.py b/tests/unit/test_strategies.py
new file mode 100644
index 0000000000000000000000000000000000000000..e33147302b4560fd08bb521012af37fa97e1bc54
--- /dev/null
+++ b/tests/unit/test_strategies.py
@@ -0,0 +1,82 @@
+import asyncio
+from typing import Any
+
+import pytest
+
+from tesseract_flow.core.strategies import (
+    GENERATION_STRATEGIES,
+    StandardStrategy,
+    GenerationStrategy,
+    get_strategy,
+    register_strategy,
+)
+
+
+def test_standard_strategy_uses_litellm(monkeypatch: pytest.MonkeyPatch) -> None:
+    captured_args: dict[str, Any] = {}
+
+    async def fake_completion(**kwargs: Any) -> dict[str, Any]:
+        captured_args.update(kwargs)
+        return {"choices": [{"message": {"content": " result "}}]}
+
+    monkeypatch.setattr(
+        "tesseract_flow.core.strategies.litellm.acompletion", fake_completion, raising=False
+    )
+
+    strategy = StandardStrategy()
+    result = asyncio.run(
+        strategy.generate("Prompt", model="test-model", config={"temperature": 0.5})
+    )
+    assert result == "result"
+    assert captured_args["temperature"] == 0.5
+    assert captured_args["model"] == "test-model"
+
+
+def test_standard_strategy_returns_empty_string(monkeypatch: pytest.MonkeyPatch) -> None:
+    async def fake_completion(**kwargs: Any) -> dict[str, Any]:
+        return {"choices": []}
+
+    monkeypatch.setattr(
+        "tesseract_flow.core.strategies.litellm.acompletion", fake_completion, raising=False
+    )
+
+    strategy = StandardStrategy()
+    result = asyncio.run(strategy.generate("Prompt", model="test-model"))
+    assert result == ""
+
+
+def test_standard_strategy_flattens_list_content(monkeypatch: pytest.MonkeyPatch) -> None:
+    async def fake_completion(**kwargs: Any) -> dict[str, Any]:
+        return {"choices": [{"message": {"content": [" part1", " part2 "]}}]}
+
+    monkeypatch.setattr(
+        "tesseract_flow.core.strategies.litellm.acompletion", fake_completion, raising=False
+    )
+
+    strategy = StandardStrategy()
+    result = asyncio.run(strategy.generate("Prompt", model="test-model"))
+    assert result == "part1 part2"
+
+
+def test_get_strategy_returns_registered_strategy() -> None:
+    strategy = get_strategy("standard")
+    assert isinstance(strategy, GenerationStrategy)
+
+
+def test_register_strategy_adds_new_strategy() -> None:
+    class DummyStrategy:
+        async def generate(self, prompt: str, *, model: str, config: dict[str, Any] | None = None) -> str:
+            return f"{model}:{prompt}"
+
+    register_strategy("dummy", DummyStrategy())
+    try:
+        strategy = get_strategy("dummy")
+        assert isinstance(strategy, DummyStrategy)
+        assert asyncio.run(strategy.generate("hello", model="model")) == "model:hello"
+    finally:
+        GENERATION_STRATEGIES.pop("dummy", None)
+
+
+def test_get_strategy_unknown_name_raises() -> None:
+    with pytest.raises(ValueError):
+        get_strategy("missing")
diff --git a/tests/unit/test_taguchi.py b/tests/unit/test_taguchi.py
new file mode 100644
index 0000000000000000000000000000000000000000..0db256385603b8bf6d5c54f7ea3c466c88e545e5
--- /dev/null
+++ b/tests/unit/test_taguchi.py
@@ -0,0 +1,68 @@
+from __future__ import annotations
+
+from collections import Counter
+from itertools import combinations
+
+import numpy as np
+import pytest
+
+from tesseract_flow.core.config import ExperimentConfig, UtilityWeights, Variable
+from tesseract_flow.experiments.taguchi import L8_ARRAY, generate_l8_array, generate_test_configs
+
+
+def test_l8_array_has_expected_orthogonality() -> None:
+    """Every pair of columns in the L8 array should contain each combination twice."""
+
+    for left, right in combinations(range(L8_ARRAY.shape[1]), 2):
+        pairs = Counter(tuple(row) for row in np.stack((L8_ARRAY[:, left], L8_ARRAY[:, right]), axis=1))
+        for combination in [(1, 1), (1, 2), (2, 1), (2, 2)]:
+            assert pairs[combination] == 2
+
+
+def test_generate_l8_array_validates_variable_count() -> None:
+    with pytest.raises(ValueError):
+        generate_l8_array(3)
+    with pytest.raises(ValueError):
+        generate_l8_array(8)
+
+
+def test_generate_l8_array_truncates_columns() -> None:
+    truncated = generate_l8_array(4)
+    assert truncated.shape == (8, 4)
+    assert np.array_equal(truncated, L8_ARRAY[:, :4])
+    assert truncated is not L8_ARRAY
+
+
+def test_generate_test_configs_matches_array() -> None:
+    variables = [
+        Variable(name="temperature", level_1=0.1, level_2=0.9),
+        Variable(name="model", level_1="gpt-4", level_2="claude"),
+        Variable(name="context", level_1="summary", level_2="full"),
+        Variable(name="strategy", level_1="standard", level_2="cot"),
+    ]
+    config = ExperimentConfig(
+        name="code_review_trial",
+        workflow="code_review",
+        variables=variables,
+        utility_weights=UtilityWeights(),
+    )
+
+    test_configs = generate_test_configs(config)
+    assert len(test_configs) == 8
+
+    variable_levels = {
+        variable.name: {variable.level_1: 1, variable.level_2: 2} for variable in variables
+    }
+    variable_indices = {variable.name: position for position, variable in enumerate(variables)}
+
+    for index, (row, test_config) in enumerate(
+        zip(generate_l8_array(len(variables)), test_configs, strict=True), start=1
+    ):
+        assert test_config.test_number == index
+        assert set(test_config.config_values) == {variable.name for variable in variables}
+        for variable in variables:
+            value = test_config.config_values[variable.name]
+            assert value in {variable.level_1, variable.level_2}
+            assert variable_levels[variable.name][value] == row[variable_indices[variable.name]]
+        assert test_config.workflow == config.workflow
+
diff --git a/tests/unit/test_utility.py b/tests/unit/test_utility.py
new file mode 100644
index 0000000000000000000000000000000000000000..56f4da073d1255b0c89ad1f65658378099be8373
--- /dev/null
+++ b/tests/unit/test_utility.py
@@ -0,0 +1,37 @@
+import pytest
+
+from tesseract_flow.core.types import UtilityWeights
+from tesseract_flow.optimization.utility import UtilityFunction
+
+
+def test_utility_function_compute_for_sequences_normalizes_values() -> None:
+    weights = UtilityWeights(quality=1.0, cost=0.5, time=0.25)
+    utility_fn = UtilityFunction(weights)
+
+    qualities = [0.5, 0.75, 0.9]
+    costs = [0.01, 0.02, 0.03]
+    latencies = [1000, 1500, 2000]
+
+    utilities = utility_fn.compute_for_sequences(qualities, costs, latencies)
+
+    assert len(utilities) == 3
+    normalized_costs, _ = UtilityFunction.normalize_metrics(costs)
+    normalized_latencies, _ = UtilityFunction.normalize_metrics(latencies)
+    expected = [
+        weights.quality * q
+        - weights.cost * cost
+        - weights.time * latency
+        for q, cost, latency in zip(qualities, normalized_costs, normalized_latencies)
+    ]
+    assert utilities == pytest.approx(expected)
+
+
+def test_normalize_metrics_handles_constant_values() -> None:
+    normalized, stats = UtilityFunction.normalize_metrics([0.5, 0.5, 0.5])
+    assert normalized == [0.0, 0.0, 0.0]
+    assert stats == {"min": 0.5, "max": 0.5}
+
+
+def test_normalize_metrics_rejects_unknown_method() -> None:
+    with pytest.raises(ValueError):
+        UtilityFunction.normalize_metrics([1.0], method="z-score")
