{
  "experiment_id": "deepseek_r1_optimization-20251028T015042",
  "config": {
    "name": "deepseek_r1_optimization",
    "workflow": "code_review",
    "variables": [
      {
        "name": "temperature",
        "level_1": 0.3,
        "level_2": 0.7
      },
      {
        "name": "context_size",
        "level_1": "file_only",
        "level_2": "full_module"
      },
      {
        "name": "generation_strategy",
        "level_1": "standard",
        "level_2": "chain_of_thought"
      },
      {
        "name": "model",
        "level_1": "openrouter/deepseek/deepseek-chat",
        "level_2": "openrouter/anthropic/claude-haiku-4.5"
      }
    ],
    "utility_weights": {
      "quality": 0.7,
      "cost": 0.2,
      "time": 0.1
    },
    "workflow_config": {
      "rubric": {
        "functionality": {
          "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
          "scale": "0-100 points",
          "weight": 0.25
        },
        "error_handling": {
          "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
          "scale": "0-100 points",
          "weight": 0.2
        },
        "code_quality": {
          "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
          "scale": "0-100 points",
          "weight": 0.2
        },
        "performance": {
          "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
          "scale": "0-100 points",
          "weight": 0.15
        },
        "testability": {
          "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
          "scale": "0-100 points",
          "weight": 0.1
        },
        "documentation": {
          "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
          "scale": "0-100 points",
          "weight": 0.1
        }
      },
      "sample_code_path": "tesseract_flow/evaluation/rubric.py",
      "evaluator_model": "openrouter/deepseek/deepseek-chat",
      "language": "python",
      "context_files": [
        "tesseract_flow/evaluation/__init__.py",
        "tesseract_flow/evaluation/metrics.py",
        "tesseract_flow/evaluation/cache.py",
        "tesseract_flow/core/types.py",
        "tesseract_flow/core/exceptions.py"
      ],
      "evaluator_temperature": 0.1,
      "context_descriptions": {
        "file_only": "Review limited to the provided file.",
        "full_module": "Review with full module context including imports and related files."
      },
      "strategy_descriptions": {
        "standard": "Review the code directly and provide your assessment.",
        "chain_of_thought": "Think step-by-step before providing your assessment:\n1. First, understand what the code does\n2. Identify potential issues in each dimension\n3. Consider severity and impact\n4. Then provide your final assessment\n"
      }
    }
  },
  "test_configurations": [
    {
      "test_number": 1,
      "config_values": {
        "temperature": 0.3,
        "context_size": "file_only",
        "generation_strategy": "standard",
        "model": "openrouter/deepseek/deepseek-chat"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 2,
      "config_values": {
        "temperature": 0.3,
        "context_size": "file_only",
        "generation_strategy": "standard",
        "model": "openrouter/anthropic/claude-haiku-4.5"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 3,
      "config_values": {
        "temperature": 0.3,
        "context_size": "full_module",
        "generation_strategy": "chain_of_thought",
        "model": "openrouter/deepseek/deepseek-chat"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 4,
      "config_values": {
        "temperature": 0.3,
        "context_size": "full_module",
        "generation_strategy": "chain_of_thought",
        "model": "openrouter/anthropic/claude-haiku-4.5"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 5,
      "config_values": {
        "temperature": 0.7,
        "context_size": "file_only",
        "generation_strategy": "chain_of_thought",
        "model": "openrouter/deepseek/deepseek-chat"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 6,
      "config_values": {
        "temperature": 0.7,
        "context_size": "file_only",
        "generation_strategy": "chain_of_thought",
        "model": "openrouter/anthropic/claude-haiku-4.5"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 7,
      "config_values": {
        "temperature": 0.7,
        "context_size": "full_module",
        "generation_strategy": "standard",
        "model": "openrouter/deepseek/deepseek-chat"
      },
      "workflow": "code_review"
    },
    {
      "test_number": 8,
      "config_values": {
        "temperature": 0.7,
        "context_size": "full_module",
        "generation_strategy": "standard",
        "model": "openrouter/anthropic/claude-haiku-4.5"
      },
      "workflow": "code_review"
    }
  ],
  "results": [
    {
      "test_number": 1,
      "config": {
        "test_number": 1,
        "config_values": {
          "temperature": 0.3,
          "context_size": "file_only",
          "generation_strategy": "standard",
          "model": "openrouter/deepseek/deepseek-chat"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.85,
            "reasoning": "The code accomplishes its intended purpose and handles most edge cases effectively. However, the lack of validation for JSON responses and incomplete test coverage for certain edge cases slightly reduces the score."
          },
          "error_handling": {
            "score": 0.8,
            "reasoning": "The code includes proper exception handling and defensive programming, but it lacks validation for JSON responses and could benefit from more detailed logging and a timeout mechanism for API calls."
          },
          "code_quality": {
            "score": 0.9,
            "reasoning": "The code is well-structured, follows best practices, and maintains readability. There are minor areas for improvement, such as reducing method complexity and improving abstractions."
          },
          "performance": {
            "score": 0.75,
            "reasoning": "The code is generally efficient but could be optimized further, particularly in the retry mechanism's jitter calculation and caching strategy to reduce computational overhead."
          },
          "testability": {
            "score": 0.7,
            "reasoning": "The code has potential for good test coverage but lacks explicit test cases for certain edge cases, such as malformed JSON responses and cache key collisions."
          },
          "documentation": {
            "score": 0.85,
            "reasoning": "The code is well-documented with clear docstrings and inline comments. However, adding more examples and improving variable/method naming could enhance clarity further."
          }
        },
        "overall_score": 0.8083333333333332,
        "evaluator_model": "openrouter/deepseek/deepseek-chat",
        "timestamp": "2025-10-28T01:51:07.277619Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 85,
              "reasoning": "The code accomplishes its intended purpose and handles most edge cases effectively. However, the lack of validation for JSON responses and incomplete test coverage for certain edge cases slightly reduces the score."
            },
            "error_handling": {
              "score": 80,
              "reasoning": "The code includes proper exception handling and defensive programming, but it lacks validation for JSON responses and could benefit from more detailed logging and a timeout mechanism for API calls."
            },
            "code_quality": {
              "score": 90,
              "reasoning": "The code is well-structured, follows best practices, and maintains readability. There are minor areas for improvement, such as reducing method complexity and improving abstractions."
            },
            "performance": {
              "score": 75,
              "reasoning": "The code is generally efficient but could be optimized further, particularly in the retry mechanism's jitter calculation and caching strategy to reduce computational overhead."
            },
            "testability": {
              "score": 70,
              "reasoning": "The code has potential for good test coverage but lacks explicit test cases for certain edge cases, such as malformed JSON responses and cache key collisions."
            },
            "documentation": {
              "score": 85,
              "reasoning": "The code is well-documented with clear docstrings and inline comments. However, adding more examples and improving variable/method naming could enhance clarity further."
            }
          },
          "temperature": 0.3,
          "cache_key": "7c2e295193de5494bde9c44e4e685c75aefc4719e012ac359a42d255dd923654",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "cost": 0.0,
      "latency": 20200.36191600957,
      "utility": 0.4802953753258779,
      "workflow_output": "The code is well-structured and follows best practices in Python. It handles edge cases effectively, uses proper error handling, and is designed for readability and maintainability. However, there are some areas for improvement in terms of performance optimization, security, and testing coverage.\nIssues:\n- (medium) The retry mechanism uses exponential backoff with jitter, which is good, but the jitter calculation could be optimized to reduce unnecessary delays. [line 237] Suggested fix: Consider using a fixed jitter percentage or a more efficient randomization method to reduce computational overhead.\n- (medium) The code does not validate the content of the JSON response from the LLM, which could potentially lead to security vulnerabilities if the response is malformed or malicious. [line 276] Suggested fix: Implement additional validation to ensure the JSON response does not contain unexpected or harmful content.\n- (medium) The code lacks explicit test cases for edge cases such as malformed JSON responses, extreme temperature values, and cache key collisions. Suggested fix: Add comprehensive unit tests to cover these edge cases and ensure robustness.\nSuggestions:\n- {'description': 'Consider adding a timeout mechanism for the LLM API calls to prevent hanging indefinitely in case of network issues.', 'line_number': 194}\n- {'description': 'Add more detailed logging for cache operations to help with debugging and monitoring.', 'line_number': 156}\n- {'description': 'Refactor the `_extract_response_content` method to reduce complexity and improve readability.', 'line_number': 248}\n- {'description': 'Consider adding a method to validate the rubric dimensions to ensure they are correctly formatted before evaluation.', 'line_number': 284}",
      "metadata": {
        "strategy": "standard",
        "model": "openrouter/deepseek/deepseek-chat",
        "temperature": 0.3,
        "context_size": "file_only",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Review limited to the provided file.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"The code is well-structured and follows best practices in Python. It handles edge cases effectively, uses proper error handling, and is designed for readability and maintainability. However, there are some areas for improvement in terms of performance optimization, security, and testing coverage.\",\n  \"issues\": [\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"medium\",\n      \"description\": \"The retry mechanism uses exponential backoff with jitter, which is good, but the jitter calculation could be optimized to reduce unnecessary delays.\",\n      \"line_number\": 237,\n      \"suggestion\": \"Consider using a fixed jitter percentage or a more efficient randomization method to reduce computational overhead.\"\n    },\n    {\n      \"type\": \"Security\",\n      \"severity\": \"medium\",\n      \"description\": \"The code does not validate the content of the JSON response from the LLM, which could potentially lead to security vulnerabilities if the response is malformed or malicious.\",\n      \"line_number\": 276,\n      \"suggestion\": \"Implement additional validation to ensure the JSON response does not contain unexpected or harmful content.\"\n    },\n    {\n      \"type\": \"Testing\",\n      \"severity\": \"medium\",\n      \"description\": \"The code lacks explicit test cases for edge cases such as malformed JSON responses, extreme temperature values, and cache key collisions.\",\n      \"suggestion\": \"Add comprehensive unit tests to cover these edge cases and ensure robustness.\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"description\": \"Consider adding a timeout mechanism for the LLM API calls to prevent hanging indefinitely in case of network issues.\",\n      \"line_number\": 194\n    },\n    {\n      \"description\": \"Add more detailed logging for cache operations to help with debugging and monitoring.\",\n      \"line_number\": 156\n    },\n    {\n      \"description\": \"Refactor the `_extract_response_content` method to reduce complexity and improve readability.\",\n      \"line_number\": 248\n    },\n    {\n      \"description\": \"Consider adding a method to validate the rubric dimensions to ensure they are correctly formatted before evaluation.\",\n      \"line_number\": 284\n    }\n  ]\n}\n```",
        "test_config": {
          "test_number": 1,
          "config": {
            "temperature": 0.3,
            "context_size": "file_only",
            "generation_strategy": "standard",
            "model": "openrouter/deepseek/deepseek-chat"
          }
        },
        "workflow": {
          "started_at": "2025-10-28T01:50:42.106193Z",
          "completed_at": "2025-10-28T01:51:02.306527Z",
          "duration_seconds": 20.20036191600957
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 85,
              "reasoning": "The code accomplishes its intended purpose and handles most edge cases effectively. However, the lack of validation for JSON responses and incomplete test coverage for certain edge cases slightly reduces the score."
            },
            "error_handling": {
              "score": 80,
              "reasoning": "The code includes proper exception handling and defensive programming, but it lacks validation for JSON responses and could benefit from more detailed logging and a timeout mechanism for API calls."
            },
            "code_quality": {
              "score": 90,
              "reasoning": "The code is well-structured, follows best practices, and maintains readability. There are minor areas for improvement, such as reducing method complexity and improving abstractions."
            },
            "performance": {
              "score": 75,
              "reasoning": "The code is generally efficient but could be optimized further, particularly in the retry mechanism's jitter calculation and caching strategy to reduce computational overhead."
            },
            "testability": {
              "score": 70,
              "reasoning": "The code has potential for good test coverage but lacks explicit test cases for certain edge cases, such as malformed JSON responses and cache key collisions."
            },
            "documentation": {
              "score": 85,
              "reasoning": "The code is well-documented with clear docstrings and inline comments. However, adding more examples and improving variable/method naming could enhance clarity further."
            }
          },
          "temperature": 0.3,
          "cache_key": "7c2e295193de5494bde9c44e4e685c75aefc4719e012ac359a42d255dd923654",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "timestamp": "2025-10-28T01:51:07.278025Z"
    },
    {
      "test_number": 2,
      "config": {
        "test_number": 2,
        "config_values": {
          "temperature": 0.3,
          "context_size": "file_only",
          "generation_strategy": "standard",
          "model": "openrouter/anthropic/claude-haiku-4.5"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.75,
            "reasoning": "The code generally accomplishes its intended purpose of evaluating workflow outputs, but there are notable issues with edge case handling, type safety, and assertion usage that could lead to incorrect behavior in production. The score extraction and normalization logic is mostly robust, but the default rubric and handling of malformed responses could be improved."
          },
          "error_handling": {
            "score": 0.7,
            "reasoning": "The code has some error handling in place, but it is not comprehensive. The use of bare `except` clauses and assertions for runtime validation is problematic. Error messages could provide more context, and logging for debugging is insufficient. The handling of LLM failures is somewhat graceful but could be more robust."
          },
          "code_quality": {
            "score": 0.65,
            "reasoning": "The code is generally clean and maintainable, but there are several issues with type safety, assertion usage, and complex nested conditionals. The separation of concerns is clear, but the use of type hints could be improved. The code could benefit from cleaner abstractions and better adherence to the Single Responsibility Principle."
          },
          "performance": {
            "score": 0.8,
            "reasoning": "The code is efficient in terms of minimizing unnecessary LLM calls and using an appropriate caching strategy. However, there are some inefficiencies in regex patterns and potential performance bottlenecks in certain methods. Overall, the performance is good but could be optimized further."
          },
          "testability": {
            "score": 0.6,
            "reasoning": "The code has potential for good test coverage, but the use of assertions and broad exception handlers makes it harder to test. The interfaces for mocking are clear, but some methods are not pure functions, making them harder to test. The testability of edge cases could be improved."
          },
          "documentation": {
            "score": 0.7,
            "reasoning": "The code lacks comprehensive docstrings for public APIs and detailed inline comments for complex logic. Type hints are present but could be more accurate. Variable and method names are clear, but the documentation could benefit from examples and more detailed explanations of the code's behavior."
          }
        },
        "overall_score": 0.7000000000000001,
        "evaluator_model": "openrouter/deepseek/deepseek-chat",
        "timestamp": "2025-10-28T01:51:34.764610Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 75,
              "reasoning": "The code generally accomplishes its intended purpose of evaluating workflow outputs, but there are notable issues with edge case handling, type safety, and assertion usage that could lead to incorrect behavior in production. The score extraction and normalization logic is mostly robust, but the default rubric and handling of malformed responses could be improved."
            },
            "error_handling": {
              "score": 70,
              "reasoning": "The code has some error handling in place, but it is not comprehensive. The use of bare `except` clauses and assertions for runtime validation is problematic. Error messages could provide more context, and logging for debugging is insufficient. The handling of LLM failures is somewhat graceful but could be more robust."
            },
            "code_quality": {
              "score": 65,
              "reasoning": "The code is generally clean and maintainable, but there are several issues with type safety, assertion usage, and complex nested conditionals. The separation of concerns is clear, but the use of type hints could be improved. The code could benefit from cleaner abstractions and better adherence to the Single Responsibility Principle."
            },
            "performance": {
              "score": 80,
              "reasoning": "The code is efficient in terms of minimizing unnecessary LLM calls and using an appropriate caching strategy. However, there are some inefficiencies in regex patterns and potential performance bottlenecks in certain methods. Overall, the performance is good but could be optimized further."
            },
            "testability": {
              "score": 60,
              "reasoning": "The code has potential for good test coverage, but the use of assertions and broad exception handlers makes it harder to test. The interfaces for mocking are clear, but some methods are not pure functions, making them harder to test. The testability of edge cases could be improved."
            },
            "documentation": {
              "score": 70,
              "reasoning": "The code lacks comprehensive docstrings for public APIs and detailed inline comments for complex logic. Type hints are present but could be more accurate. Variable and method names are clear, but the documentation could benefit from examples and more detailed explanations of the code's behavior."
            }
          },
          "temperature": 0.3,
          "cache_key": "363da3fe988f649e9b4343f61a3902300dfb742d75ec6dc9021156c1b910dc8d",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "cost": 0.0,
      "latency": 18967.505541979335,
      "utility": 0.412912330287052,
      "workflow_output": "Well-structured LLM-powered evaluator with solid error handling and caching support. Code is generally clean and maintainable, but has several issues around type safety, assertion usage, and edge case handling that should be addressed.\nIssues:\n- (medium) Use of `assert` statements for type narrowing in production code. Assertions can be disabled with Python's -O flag, making them unreliable for runtime validation. Suggested fix: Replace `assert resolved_cache_key is not None` and `assert content is not None` with explicit `if` checks that raise `EvaluationError`. Example: `if resolved_cache_key is None: raise EvaluationError('Cache key must be set')`\n- (medium) Bare `except Exception` clause in `_request_with_retry` catches all exceptions including KeyboardInterrupt and SystemExit, which should not be caught. [line 116] Suggested fix: Catch specific exceptions from litellm instead. If litellm doesn't document exceptions, catch `(litellm.APIError, asyncio.TimeoutError, RuntimeError)` or similar. Add `# pragma: no cover` comment only for truly external/untestable errors.\n- (high) In `_request_with_retry`, the assertion `assert last_error is not None` at line 143 could fail if the loop exits without entering the exception handler, though the logic appears sound. More importantly, the error message doesn't preserve the original exception type information. [line 143] Suggested fix: Replace assertion with explicit check. Also consider wrapping the original exception type in the message: `raise EvaluationError(f'LLM evaluation failed after {self._max_retries} attempts: {type(last_error).__name__}') from last_error`\n- (medium) The `_extract_max_score` method defaults to 100.0 silently if regex doesn't match. This could mask malformed rubric scales without warning. [line 234] Suggested fix: Log a warning when defaulting to 100.0: `self._logger.warning('Could not parse max score from scale: %s. Defaulting to 100.0', scale)`. Consider raising an error for invalid rubrics in strict mode.\n- (low) The `_extract_response_content` method has complex nested conditionals checking both Mapping and object attributes. This is fragile and hard to test. Suggested fix: Extract a helper method `_safe_get(obj, key, default=None)` that handles both dict and object attribute access. This improves testability and reduces duplication.\n- (medium) In `_load_json`, the markdown fence stripping logic assumes a specific format and could fail silently or produce unexpected results with unusual formatting. Suggested fix: Use a more robust regex: `stripped = re.sub(r'^```(?:json)?\\n?|\\n?```$', '', stripped)`. Add logging for when fences are detected and stripped.\n- (medium) The `_parse_dimension_scores` method accesses `rubric[name].get('scale', '1-10')` but RubricDimension is a TypedDict that should have 'scale' as a required key. The `.get()` with default suggests uncertainty about the type contract. [line 245] Suggested fix: Either ensure RubricDimension always has 'scale' (remove `.get()` default), or add explicit validation in `__init__` to validate rubric structure. Document the expected RubricDimension structure.\n- (low) The `_format_rubric` method iterates through rubric items and builds strings. For large rubrics, this is fine, but the pattern could be more Pythonic. [line 174] Suggested fix: Use list comprehension: `return '\\n'.join(f'- {name}: {metadata[\"description\"]} (Scale: {metadata[\"scale\"]})' for name, metadata in dimensions)`\n- (medium) The `_coerce_numeric` method checks `math.isfinite()` but doesn't handle NaN or Infinity strings like 'inf', 'nan' that `float()` accepts. Suggested fix: After `numeric = float(stripped)`, explicitly check: `if math.isnan(numeric) or math.isinf(numeric): raise ValueError('Score must be finite')`\n- (low) The `evaluate` method has many parameters and complex logic but lacks detailed docstring explaining cache behavior, return value structure, and exception conditions. [line 48] Suggested fix: Add comprehensive docstring with Args, Returns, Raises sections. Document the metadata structure in QualityScore. Explain cache key resolution logic.\n- (medium) The `# pragma: no cover` comment on line 116 is too broad. It covers the entire exception handler, making it impossible to test error paths. [line 116] Suggested fix: Move pragma comment to specific lines or remove it. Test the retry logic with mocked litellm that raises exceptions. The retry mechanism is testable and important.\nSuggestions:\n- {'category': 'Type Safety', 'description': 'Add type hints to return types of private methods like `_extract_response_content() -> str`, `_load_json() -> Mapping[str, Any]`, etc. This improves IDE support and catches errors early.'}\n- {'category': 'Error Messages', 'description': \"Include more context in error messages. For example, when JSON parsing fails, include a snippet of the content: `raise EvaluationError(f'Invalid JSON: {content[:100]}...')`\"}\n- {'category': 'Logging', 'description': \"Add debug logging for successful evaluations showing dimension scores. This helps with troubleshooting: `self._logger.debug('Evaluation complete: %s', {name: score.score for name, score in dimension_scores.items()})`\"}\n- {'category': 'Configuration', 'description': 'Consider making the JSON response template configurable or extractable as a constant. This would allow customization for different use cases.'}\n- {'category': 'Validation', 'description': 'Add validation that `rubric_definition` is not empty and contains valid dimension names (no special characters, reasonable length).'}\n- {'category': 'Caching', 'description': 'Document the cache key format and stability guarantees. If `build_cache_key` changes, old cache entries become invalid. Consider versioning the cache key.'}\n- {'category': 'Testing', 'description': 'The `_request_with_retry` method should be testable. Create a test that verifies exponential backoff and jitter are applied correctly by mocking `asyncio.sleep`.'}\n- {'category': 'Robustness', 'description': 'Add a timeout parameter to `litellm.acompletion` call to prevent indefinite hangs. Consider making it configurable in `__init__`.'}",
      "metadata": {
        "strategy": "standard",
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "temperature": 0.3,
        "context_size": "file_only",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Review limited to the provided file.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"Well-structured LLM-powered evaluator with solid error handling and caching support. Code is generally clean and maintainable, but has several issues around type safety, assertion usage, and edge case handling that should be addressed.\",\n  \"issues\": [\n    {\n      \"type\": \"Type Safety\",\n      \"severity\": \"medium\",\n      \"description\": \"Use of `assert` statements for type narrowing in production code. Assertions can be disabled with Python's -O flag, making them unreliable for runtime validation.\",\n      \"line_number\": [82, 107, 143],\n      \"suggestion\": \"Replace `assert resolved_cache_key is not None` and `assert content is not None` with explicit `if` checks that raise `EvaluationError`. Example: `if resolved_cache_key is None: raise EvaluationError('Cache key must be set')`\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"Bare `except Exception` clause in `_request_with_retry` catches all exceptions including KeyboardInterrupt and SystemExit, which should not be caught.\",\n      \"line_number\": 116,\n      \"suggestion\": \"Catch specific exceptions from litellm instead. If litellm doesn't document exceptions, catch `(litellm.APIError, asyncio.TimeoutError, RuntimeError)` or similar. Add `# pragma: no cover` comment only for truly external/untestable errors.\"\n    },\n    {\n      \"type\": \"Logic Error\",\n      \"severity\": \"high\",\n      \"description\": \"In `_request_with_retry`, the assertion `assert last_error is not None` at line 143 could fail if the loop exits without entering the exception handler, though the logic appears sound. More importantly, the error message doesn't preserve the original exception type information.\",\n      \"line_number\": 143,\n      \"suggestion\": \"Replace assertion with explicit check. Also consider wrapping the original exception type in the message: `raise EvaluationError(f'LLM evaluation failed after {self._max_retries} attempts: {type(last_error).__name__}') from last_error`\"\n    },\n    {\n      \"type\": \"Input Validation\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_extract_max_score` method defaults to 100.0 silently if regex doesn't match. This could mask malformed rubric scales without warning.\",\n      \"line_number\": 234,\n      \"suggestion\": \"Log a warning when defaulting to 100.0: `self._logger.warning('Could not parse max score from scale: %s. Defaulting to 100.0', scale)`. Consider raising an error for invalid rubrics in strict mode.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"The `_extract_response_content` method has complex nested conditionals checking both Mapping and object attributes. This is fragile and hard to test.\",\n      \"line_number\": [190, 200, 208],\n      \"suggestion\": \"Extract a helper method `_safe_get(obj, key, default=None)` that handles both dict and object attribute access. This improves testability and reduces duplication.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"In `_load_json`, the markdown fence stripping logic assumes a specific format and could fail silently or produce unexpected results with unusual formatting.\",\n      \"line_number\": [220, 224],\n      \"suggestion\": \"Use a more robust regex: `stripped = re.sub(r'^```(?:json)?\\\\n?|\\\\n?```$', '', stripped)`. Add logging for when fences are detected and stripped.\"\n    },\n    {\n      \"type\": \"Type Safety\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_parse_dimension_scores` method accesses `rubric[name].get('scale', '1-10')` but RubricDimension is a TypedDict that should have 'scale' as a required key. The `.get()` with default suggests uncertainty about the type contract.\",\n      \"line_number\": 245,\n      \"suggestion\": \"Either ensure RubricDimension always has 'scale' (remove `.get()` default), or add explicit validation in `__init__` to validate rubric structure. Document the expected RubricDimension structure.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"The `_format_rubric` method iterates through rubric items and builds strings. For large rubrics, this is fine, but the pattern could be more Pythonic.\",\n      \"line_number\": 174,\n      \"suggestion\": \"Use list comprehension: `return '\\\\n'.join(f'- {name}: {metadata[\\\"description\\\"]} (Scale: {metadata[\\\"scale\\\"]})' for name, metadata in dimensions)`\"\n    },\n    {\n      \"type\": \"Robustness\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_coerce_numeric` method checks `math.isfinite()` but doesn't handle NaN or Infinity strings like 'inf', 'nan' that `float()` accepts.\",\n      \"line_number\": [258, 265],\n      \"suggestion\": \"After `numeric = float(stripped)`, explicitly check: `if math.isnan(numeric) or math.isinf(numeric): raise ValueError('Score must be finite')`\"\n    },\n    {\n      \"type\": \"Documentation\",\n      \"severity\": \"low\",\n      \"description\": \"The `evaluate` method has many parameters and complex logic but lacks detailed docstring explaining cache behavior, return value structure, and exception conditions.\",\n      \"line_number\": 48,\n      \"suggestion\": \"Add comprehensive docstring with Args, Returns, Raises sections. Document the metadata structure in QualityScore. Explain cache key resolution logic.\"\n    },\n    {\n      \"type\": \"Testing\",\n      \"severity\": \"medium\",\n      \"description\": \"The `# pragma: no cover` comment on line 116 is too broad. It covers the entire exception handler, making it impossible to test error paths.\",\n      \"line_number\": 116,\n      \"suggestion\": \"Move pragma comment to specific lines or remove it. Test the retry logic with mocked litellm that raises exceptions. The retry mechanism is testable and important.\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"category\": \"Type Safety\",\n      \"description\": \"Add type hints to return types of private methods like `_extract_response_content() -> str`, `_load_json() -> Mapping[str, Any]`, etc. This improves IDE support and catches errors early.\"\n    },\n    {\n      \"category\": \"Error Messages\",\n      \"description\": \"Include more context in error messages. For example, when JSON parsing fails, include a snippet of the content: `raise EvaluationError(f'Invalid JSON: {content[:100]}...')`\"\n    },\n    {\n      \"category\": \"Logging\",\n      \"description\": \"Add debug logging for successful evaluations showing dimension scores. This helps with troubleshooting: `self._logger.debug('Evaluation complete: %s', {name: score.score for name, score in dimension_scores.items()})`\"\n    },\n    {\n      \"category\": \"Configuration\",\n      \"description\": \"Consider making the JSON response template configurable or extractable as a constant. This would allow customization for different use cases.\"\n    },\n    {\n      \"category\": \"Validation\",\n      \"description\": \"Add validation that `rubric_definition` is not empty and contains valid dimension names (no special characters, reasonable length).\"\n    },\n    {\n      \"category\": \"Caching\",\n      \"description\": \"Document the cache key format and stability guarantees. If `build_cache_key` changes, old cache entries become invalid. Consider versioning the cache key.\"\n    },\n    {\n      \"category\": \"Testing\",\n      \"description\": \"The `_request_with_retry` method should be testable. Create a test that verifies exponential backoff and jitter are applied correctly by mocking `asyncio.sleep`.\"\n    },\n    {\n      \"category\": \"Robustness\",\n      \"description\": \"Add a timeout parameter to `litellm.acompletion` call to prevent indefinite hangs. Consider making it configurable in `__init__`.\"\n    }\n  ]\n}\n```",
        "test_config": {
          "test_number": 2,
          "config": {
            "temperature": 0.3,
            "context_size": "file_only",
            "generation_strategy": "standard",
            "model": "openrouter/anthropic/claude-haiku-4.5"
          }
        },
        "workflow": {
          "started_at": "2025-10-28T01:51:07.280397Z",
          "completed_at": "2025-10-28T01:51:26.247878Z",
          "duration_seconds": 18.967505541979335
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 75,
              "reasoning": "The code generally accomplishes its intended purpose of evaluating workflow outputs, but there are notable issues with edge case handling, type safety, and assertion usage that could lead to incorrect behavior in production. The score extraction and normalization logic is mostly robust, but the default rubric and handling of malformed responses could be improved."
            },
            "error_handling": {
              "score": 70,
              "reasoning": "The code has some error handling in place, but it is not comprehensive. The use of bare `except` clauses and assertions for runtime validation is problematic. Error messages could provide more context, and logging for debugging is insufficient. The handling of LLM failures is somewhat graceful but could be more robust."
            },
            "code_quality": {
              "score": 65,
              "reasoning": "The code is generally clean and maintainable, but there are several issues with type safety, assertion usage, and complex nested conditionals. The separation of concerns is clear, but the use of type hints could be improved. The code could benefit from cleaner abstractions and better adherence to the Single Responsibility Principle."
            },
            "performance": {
              "score": 80,
              "reasoning": "The code is efficient in terms of minimizing unnecessary LLM calls and using an appropriate caching strategy. However, there are some inefficiencies in regex patterns and potential performance bottlenecks in certain methods. Overall, the performance is good but could be optimized further."
            },
            "testability": {
              "score": 60,
              "reasoning": "The code has potential for good test coverage, but the use of assertions and broad exception handlers makes it harder to test. The interfaces for mocking are clear, but some methods are not pure functions, making them harder to test. The testability of edge cases could be improved."
            },
            "documentation": {
              "score": 70,
              "reasoning": "The code lacks comprehensive docstrings for public APIs and detailed inline comments for complex logic. Type hints are present but could be more accurate. Variable and method names are clear, but the documentation could benefit from examples and more detailed explanations of the code's behavior."
            }
          },
          "temperature": 0.3,
          "cache_key": "363da3fe988f649e9b4343f61a3902300dfb742d75ec6dc9021156c1b910dc8d",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "timestamp": "2025-10-28T01:51:34.764910Z"
    },
    {
      "test_number": 3,
      "config": {
        "test_number": 3,
        "config_values": {
          "temperature": 0.3,
          "context_size": "full_module",
          "generation_strategy": "chain_of_thought",
          "model": "openrouter/deepseek/deepseek-chat"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.75,
            "reasoning": "The code generally accomplishes its intended purpose but has some weaknesses in handling edge cases and robustness, such as assumptions about response structures and lack of comprehensive input validation."
          },
          "error_handling": {
            "score": 0.7,
            "reasoning": "The code includes some error handling but lacks comprehensive validation and graceful handling of potential issues like malformed responses and missing keys. Logging for debugging is also insufficient."
          },
          "code_quality": {
            "score": 0.8,
            "reasoning": "The code is well-structured with good use of type hints and modularity. However, there are areas for improvement in readability and maintainability, such as string concatenation and method assumptions."
          },
          "performance": {
            "score": 0.85,
            "reasoning": "The code is generally efficient with minimal unnecessary LLM calls and reasonable token usage. However, the use of `random.uniform` for jitter could be optimized for more predictable behavior."
          },
          "testability": {
            "score": 0.7,
            "reasoning": "The code has potential for good test coverage but lacks unit tests for edge cases. Clear interfaces for mocking and testable methods are present, but more comprehensive testing is needed."
          },
          "documentation": {
            "score": 0.75,
            "reasoning": "The code has clear docstrings and type hints, but inline comments for complex logic and examples could be improved. Variable and method names are generally clear but could be more descriptive in some cases."
          }
        },
        "overall_score": 0.7583333333333333,
        "evaluator_model": "openrouter/deepseek/deepseek-chat",
        "timestamp": "2025-10-28T01:51:47.721046Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 75,
              "reasoning": "The code generally accomplishes its intended purpose but has some weaknesses in handling edge cases and robustness, such as assumptions about response structures and lack of comprehensive input validation."
            },
            "error_handling": {
              "score": 70,
              "reasoning": "The code includes some error handling but lacks comprehensive validation and graceful handling of potential issues like malformed responses and missing keys. Logging for debugging is also insufficient."
            },
            "code_quality": {
              "score": 80,
              "reasoning": "The code is well-structured with good use of type hints and modularity. However, there are areas for improvement in readability and maintainability, such as string concatenation and method assumptions."
            },
            "performance": {
              "score": 85,
              "reasoning": "The code is generally efficient with minimal unnecessary LLM calls and reasonable token usage. However, the use of `random.uniform` for jitter could be optimized for more predictable behavior."
            },
            "testability": {
              "score": 70,
              "reasoning": "The code has potential for good test coverage but lacks unit tests for edge cases. Clear interfaces for mocking and testable methods are present, but more comprehensive testing is needed."
            },
            "documentation": {
              "score": 75,
              "reasoning": "The code has clear docstrings and type hints, but inline comments for complex logic and examples could be improved. Variable and method names are generally clear but could be more descriptive in some cases."
            }
          },
          "temperature": 0.3,
          "cache_key": "c23226ab26cc2f95c1100d0471efa153b71d95a60023a31f9a05515c61628f85",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "cost": 0.0,
      "latency": 7720.785707992036,
      "utility": 0.5308333333333333,
      "workflow_output": "The code is well-structured and follows best practices in many areas, including error handling, type hints, and modularity. However, there are some areas for improvement in terms of readability, error handling, and potential security concerns.\nIssues:\n- (medium) The `_extract_response_content` method assumes the response structure from LiteLLM, which could change and break the code. [line 244] Suggested fix: Add more robust checks for the response structure and handle potential variations or missing fields gracefully.\n- (medium) The `_load_json` method strips markdown code fences but does not validate the content for potential malicious input. [line 274] Suggested fix: Consider adding input validation or sanitization to ensure the JSON content is safe to parse.\n- (low) The `_build_prompt` method constructs a large string with multiple concatenations, which can be hard to read and maintain. [line 208] Suggested fix: Use a multi-line string or template literals to improve readability.\n- (medium) The `_parse_dimension_scores` method assumes the payload is a dictionary with specific keys, which could lead to KeyError if the structure is incorrect. [line 290] Suggested fix: Add checks to ensure the payload structure is as expected and handle missing keys gracefully.\n- (low) The `_compute_retry_delay` method uses `random.uniform` for jitter, which is fine but could be optimized for more predictable behavior. [line 199] Suggested fix: Consider using a fixed jitter factor or a more predictable randomization method.\nSuggestions:\n- Consider adding more detailed logging for debugging purposes, especially around the LLM API calls and response parsing.\n- Implement unit tests to cover edge cases, especially around response parsing and error handling.\n- Consider using a configuration class or dictionary to manage default values and configurations, making the code more modular and easier to maintain.\n- Add more comprehensive input validation, especially for the `workflow_output` and `rubric` parameters, to ensure they meet expected formats.\n- Consider adding a timeout mechanism for the LLM API calls to prevent hanging in case of network issues.",
      "metadata": {
        "strategy": "chain_of_thought",
        "model": "openrouter/deepseek/deepseek-chat",
        "temperature": 0.3,
        "context_size": "full_module",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Consider surrounding module context for the review.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"The code is well-structured and follows best practices in many areas, including error handling, type hints, and modularity. However, there are some areas for improvement in terms of readability, error handling, and potential security concerns.\",\n  \"issues\": [\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_extract_response_content` method assumes the response structure from LiteLLM, which could change and break the code.\",\n      \"line_number\": 244,\n      \"suggestion\": \"Add more robust checks for the response structure and handle potential variations or missing fields gracefully.\"\n    },\n    {\n      \"type\": \"Security\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_load_json` method strips markdown code fences but does not validate the content for potential malicious input.\",\n      \"line_number\": 274,\n      \"suggestion\": \"Consider adding input validation or sanitization to ensure the JSON content is safe to parse.\"\n    },\n    {\n      \"type\": \"Readability\",\n      \"severity\": \"low\",\n      \"description\": \"The `_build_prompt` method constructs a large string with multiple concatenations, which can be hard to read and maintain.\",\n      \"line_number\": 208,\n      \"suggestion\": \"Use a multi-line string or template literals to improve readability.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_parse_dimension_scores` method assumes the payload is a dictionary with specific keys, which could lead to KeyError if the structure is incorrect.\",\n      \"line_number\": 290,\n      \"suggestion\": \"Add checks to ensure the payload structure is as expected and handle missing keys gracefully.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"The `_compute_retry_delay` method uses `random.uniform` for jitter, which is fine but could be optimized for more predictable behavior.\",\n      \"line_number\": 199,\n      \"suggestion\": \"Consider using a fixed jitter factor or a more predictable randomization method.\"\n    }\n  ],\n  \"suggestions\": [\n    \"Consider adding more detailed logging for debugging purposes, especially around the LLM API calls and response parsing.\",\n    \"Implement unit tests to cover edge cases, especially around response parsing and error handling.\",\n    \"Consider using a configuration class or dictionary to manage default values and configurations, making the code more modular and easier to maintain.\",\n    \"Add more comprehensive input validation, especially for the `workflow_output` and `rubric` parameters, to ensure they meet expected formats.\",\n    \"Consider adding a timeout mechanism for the LLM API calls to prevent hanging in case of network issues.\"\n  ]\n}\n```",
        "test_config": {
          "test_number": 3,
          "config": {
            "temperature": 0.3,
            "context_size": "full_module",
            "generation_strategy": "chain_of_thought",
            "model": "openrouter/deepseek/deepseek-chat"
          }
        },
        "workflow": {
          "started_at": "2025-10-28T01:51:34.767164Z",
          "completed_at": "2025-10-28T01:51:42.487943Z",
          "duration_seconds": 7.720785707992036
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 75,
              "reasoning": "The code generally accomplishes its intended purpose but has some weaknesses in handling edge cases and robustness, such as assumptions about response structures and lack of comprehensive input validation."
            },
            "error_handling": {
              "score": 70,
              "reasoning": "The code includes some error handling but lacks comprehensive validation and graceful handling of potential issues like malformed responses and missing keys. Logging for debugging is also insufficient."
            },
            "code_quality": {
              "score": 80,
              "reasoning": "The code is well-structured with good use of type hints and modularity. However, there are areas for improvement in readability and maintainability, such as string concatenation and method assumptions."
            },
            "performance": {
              "score": 85,
              "reasoning": "The code is generally efficient with minimal unnecessary LLM calls and reasonable token usage. However, the use of `random.uniform` for jitter could be optimized for more predictable behavior."
            },
            "testability": {
              "score": 70,
              "reasoning": "The code has potential for good test coverage but lacks unit tests for edge cases. Clear interfaces for mocking and testable methods are present, but more comprehensive testing is needed."
            },
            "documentation": {
              "score": 75,
              "reasoning": "The code has clear docstrings and type hints, but inline comments for complex logic and examples could be improved. Variable and method names are generally clear but could be more descriptive in some cases."
            }
          },
          "temperature": 0.3,
          "cache_key": "c23226ab26cc2f95c1100d0471efa153b71d95a60023a31f9a05515c61628f85",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "timestamp": "2025-10-28T01:51:47.721306Z"
    },
    {
      "test_number": 4,
      "config": {
        "test_number": 4,
        "config_values": {
          "temperature": 0.3,
          "context_size": "full_module",
          "generation_strategy": "chain_of_thought",
          "model": "openrouter/anthropic/claude-haiku-4.5"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.75,
            "reasoning": "The code generally accomplishes its intended purpose of evaluating workflow outputs, but there are notable gaps in handling edge cases, such as malformed LLM responses and incomplete input validation. The score extraction logic is somewhat robust but could be improved with better bounds checking and JSON parsing."
          },
          "error_handling": {
            "score": 0.7,
            "reasoning": "The code includes some error handling, but it is not comprehensive. High-priority issues like the lack of bounds checking in `_extract_response_content` and the broad `except Exception` catch reduce its effectiveness. Custom exceptions and more specific error handling would improve this dimension."
          },
          "code_quality": {
            "score": 0.8,
            "reasoning": "The code follows best practices with clear separation of concerns and generally good design. However, there are inconsistencies in type hints and the use of `assert` statements, which detract from overall quality. Improvements in these areas would enhance maintainability."
          },
          "performance": {
            "score": 0.85,
            "reasoning": "The code is generally efficient, with minimal unnecessary LLM calls and reasonable token usage. However, there are minor inefficiencies, such as the repeated conversion of `messages` to a list inside the retry loop, which could be optimized."
          },
          "testability": {
            "score": 0.7,
            "reasoning": "The code has potential for good test coverage, but there are gaps, particularly in testing external LiteLLM errors. The use of `# pragma: no cover` suggests critical paths are not fully tested. Comprehensive unit tests for edge cases would improve this dimension."
          },
          "documentation": {
            "score": 0.75,
            "reasoning": "The code includes some documentation, but it could be more comprehensive, especially for private methods. Clearer docstrings and examples would improve understanding and usability. The type hints are generally accurate but could be more consistent."
          }
        },
        "overall_score": 0.7583333333333333,
        "evaluator_model": "openrouter/deepseek/deepseek-chat",
        "timestamp": "2025-10-28T01:52:28.038625Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 75,
              "reasoning": "The code generally accomplishes its intended purpose of evaluating workflow outputs, but there are notable gaps in handling edge cases, such as malformed LLM responses and incomplete input validation. The score extraction logic is somewhat robust but could be improved with better bounds checking and JSON parsing."
            },
            "error_handling": {
              "score": 70,
              "reasoning": "The code includes some error handling, but it is not comprehensive. High-priority issues like the lack of bounds checking in `_extract_response_content` and the broad `except Exception` catch reduce its effectiveness. Custom exceptions and more specific error handling would improve this dimension."
            },
            "code_quality": {
              "score": 80,
              "reasoning": "The code follows best practices with clear separation of concerns and generally good design. However, there are inconsistencies in type hints and the use of `assert` statements, which detract from overall quality. Improvements in these areas would enhance maintainability."
            },
            "performance": {
              "score": 85,
              "reasoning": "The code is generally efficient, with minimal unnecessary LLM calls and reasonable token usage. However, there are minor inefficiencies, such as the repeated conversion of `messages` to a list inside the retry loop, which could be optimized."
            },
            "testability": {
              "score": 70,
              "reasoning": "The code has potential for good test coverage, but there are gaps, particularly in testing external LiteLLM errors. The use of `# pragma: no cover` suggests critical paths are not fully tested. Comprehensive unit tests for edge cases would improve this dimension."
            },
            "documentation": {
              "score": 75,
              "reasoning": "The code includes some documentation, but it could be more comprehensive, especially for private methods. Clearer docstrings and examples would improve understanding and usability. The type hints are generally accurate but could be more consistent."
            }
          },
          "temperature": 0.3,
          "cache_key": "14d023d6dcc3daa44d313fb5b6f9a61b38a4df69165f23fbc7e8b8265e29b02d",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "cost": 0.0,
      "latency": 22310.304207989248,
      "utility": 0.4308333333333333,
      "workflow_output": "Well-structured LLM evaluation code with solid error handling and async support. Generally follows best practices with clear separation of concerns. Main concerns are around robustness of LLM response parsing, incomplete input validation, and some edge cases in retry logic.\nIssues:\n- (high) The `_extract_response_content` method assumes `choices[0]` exists without bounds checking. If LiteLLM returns an empty choices array, this will raise IndexError instead of a proper EvaluationError. [line 219] Suggested fix: Add explicit check: `if not choices or len(choices) == 0:` before accessing `choices[0]`\n- (high) The JSON markdown fence stripping logic in `_load_json` is fragile. It assumes a specific format and doesn't handle edge cases like nested code blocks or malformed fences gracefully. [line 237] Suggested fix: Use a more robust regex pattern: `re.sub(r'^```(?:json)?\\n?|\\n?```$', '', stripped)` to handle variations\n- (medium) The `extra_instructions` parameter is not validated. If it's an empty string, the code still adds a newline, creating unnecessary whitespace in the prompt. [line 155] Suggested fix: Change to: `instructions = f\"\\n{extra_instructions.strip()}\" if extra_instructions and extra_instructions.strip() else \"\"`\n- (medium) In `_request_with_retry`, the `payload = list(messages)` conversion happens inside the loop but should happen once before the loop. Currently inefficient and could cause issues if messages are modified. [line 194] Suggested fix: Move `payload = list(messages)` to before the while loop initialization\n- (medium) Multiple uses of `assert` for type-checking (lines 180, 195, 216). While marked for type-checkers, asserts can be disabled with Python's `-O` flag, making code fragile in production. Suggested fix: Replace asserts with explicit `if` checks and raise EvaluationError: `if resolved_cache_key is None: raise EvaluationError(...)`\n- (medium) The broad `except Exception` in `_request_with_retry` catches all exceptions including KeyboardInterrupt and SystemExit (in Python <3.11). Should catch specific LiteLLM exceptions. [line 201] Suggested fix: Catch specific exceptions: `except (litellm.APIError, litellm.RateLimitError, asyncio.TimeoutError) as exc:`\n- (medium) The `_parse_dimension_scores` method doesn't validate that `raw_score` exists in the entry mapping before calling `_normalize_score`. If 'score' key is missing, it passes `None` to `_coerce_numeric`. [line 265] Suggested fix: Add explicit check: `if raw_score is None: raise EvaluationError(f\"Score missing for dimension '{name}'\")`\n- (low) The `_extract_max_score` regex doesn't handle edge cases like scientific notation (1e2) or negative numbers in scales. The default fallback to 100 could mask parsing errors. [line 283] Suggested fix: Add logging when falling back to default: `self._logger.warning(f\"Could not parse scale '{scale}', defaulting to 100\")`\n- (low) Inconsistent use of `Mapping` vs `Dict` type hints. `Mapping` is used for parameters but `Dict` for return types, making it unclear when mutability matters. Suggested fix: Standardize: use `Mapping` for inputs (read-only), `Dict` for outputs (mutable)\n- (low) The `# pragma: no cover` comment on line 201 suggests external LiteLLM errors aren't tested. This is a critical path that should have test coverage with mocked LiteLLM. [line 201] Suggested fix: Create integration tests with mocked litellm.acompletion to verify retry logic and error handling\n- (low) The `_format_rubric` method iterates through rubric items twice (once implicitly in the loop, once in join). Minor inefficiency but could matter with large rubrics. [line 227] Suggested fix: Use list comprehension: `lines = [f\"- {name}: ...\" for name, metadata in dimensions]`\n- (low) User-provided `workflow_output` and `extra_instructions` are directly interpolated into prompts without sanitization. While not a direct injection risk with LLMs, could leak sensitive data or cause prompt injection. Suggested fix: Add optional parameter to sanitize/redact sensitive patterns, or document that users should pre-sanitize inputs\nSuggestions:\n- {'category': 'Error Handling', 'priority': 'high', 'description': 'Create a custom exception hierarchy (e.g., `CacheError`, `LLMResponseError`, `ValidationError`) instead of generic `EvaluationError` for all cases. This enables better error recovery and logging.', 'example': 'class LLMResponseError(EvaluationError): pass'}\n- {'category': 'Testing', 'priority': 'high', 'description': 'Add comprehensive unit tests for edge cases: empty rubric, malformed JSON responses, out-of-range scores, missing dimensions, and various LLM response formats.', 'example': 'Create test fixtures for different LiteLLM response structures (dict vs object, list vs string content)'}\n- {'category': 'Logging', 'priority': 'medium', 'description': 'Add structured logging with context (cache_key, model, attempt) to improve observability. Current logging is sparse for debugging production issues.', 'example': \"Use `self._logger.info('Evaluation complete', extra={'model': selected_model, 'dimensions': len(dimension_scores)})`\"}\n- {'category': 'Documentation', 'priority': 'medium', 'description': 'Add docstrings to private methods explaining expected input/output formats, especially for `_extract_response_content` and `_parse_dimension_scores`.', 'example': 'Document that `_extract_response_content` expects LiteLLM response format with specific structure'}\n- {'category': 'Configuration', 'priority': 'medium', 'description': 'Make retry strategy configurable (exponential vs linear backoff, jitter percentage) rather than hardcoded. Consider using `tenacity` library for more robust retry handling.', 'example': 'Add `retry_strategy` parameter to __init__ accepting a callable or enum'}\n- {'category': 'Type Safety', 'priority': 'medium', 'description': 'Add `@overload` decorators for methods with optional parameters to improve IDE support and type checking. Consider using `TypedDict` for rubric structure.', 'example': \"Define `RubricDimension = TypedDict('RubricDimension', {'description': str, 'scale': str})`\"}\n- {'category': 'Performance', 'priority': 'low', 'description': 'Consider caching the compiled regex pattern in `_extract_max_score` as a class variable to avoid recompilation on each call.', 'example': \"_SCALE_PATTERN = re.compile(r'(\\\\d+(?:\\\\.\\\\d+)?)\\\\s*-\\\\s*(\\\\d+(?:\\\\.\\\\d+)?)')\"}\n- {'category': 'Maintainability', 'priority': 'low', 'description': 'Extract the JSON response template string to a constant to reduce duplication and make it easier to modify the expected format.', 'example': 'Move the template to a class constant `_JSON_RESPONSE_TEMPLATE`'}",
      "metadata": {
        "strategy": "chain_of_thought",
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "temperature": 0.3,
        "context_size": "full_module",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Consider surrounding module context for the review.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"Well-structured LLM evaluation code with solid error handling and async support. Generally follows best practices with clear separation of concerns. Main concerns are around robustness of LLM response parsing, incomplete input validation, and some edge cases in retry logic.\",\n  \"issues\": [\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"high\",\n      \"description\": \"The `_extract_response_content` method assumes `choices[0]` exists without bounds checking. If LiteLLM returns an empty choices array, this will raise IndexError instead of a proper EvaluationError.\",\n      \"line_number\": 219,\n      \"suggestion\": \"Add explicit check: `if not choices or len(choices) == 0:` before accessing `choices[0]`\"\n    },\n    {\n      \"type\": \"Robustness\",\n      \"severity\": \"high\",\n      \"description\": \"The JSON markdown fence stripping logic in `_load_json` is fragile. It assumes a specific format and doesn't handle edge cases like nested code blocks or malformed fences gracefully.\",\n      \"line_number\": 237,\n      \"suggestion\": \"Use a more robust regex pattern: `re.sub(r'^```(?:json)?\\\\n?|\\\\n?```$', '', stripped)` to handle variations\"\n    },\n    {\n      \"type\": \"Input Validation\",\n      \"severity\": \"medium\",\n      \"description\": \"The `extra_instructions` parameter is not validated. If it's an empty string, the code still adds a newline, creating unnecessary whitespace in the prompt.\",\n      \"line_number\": 155,\n      \"suggestion\": \"Change to: `instructions = f\\\"\\\\n{extra_instructions.strip()}\\\" if extra_instructions and extra_instructions.strip() else \\\"\\\"`\"\n    },\n    {\n      \"type\": \"Logic Error\",\n      \"severity\": \"medium\",\n      \"description\": \"In `_request_with_retry`, the `payload = list(messages)` conversion happens inside the loop but should happen once before the loop. Currently inefficient and could cause issues if messages are modified.\",\n      \"line_number\": 194,\n      \"suggestion\": \"Move `payload = list(messages)` to before the while loop initialization\"\n    },\n    {\n      \"type\": \"Type Safety\",\n      \"severity\": \"medium\",\n      \"description\": \"Multiple uses of `assert` for type-checking (lines 180, 195, 216). While marked for type-checkers, asserts can be disabled with Python's `-O` flag, making code fragile in production.\",\n      \"line_number\": [180, 195, 216],\n      \"suggestion\": \"Replace asserts with explicit `if` checks and raise EvaluationError: `if resolved_cache_key is None: raise EvaluationError(...)`\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"The broad `except Exception` in `_request_with_retry` catches all exceptions including KeyboardInterrupt and SystemExit (in Python <3.11). Should catch specific LiteLLM exceptions.\",\n      \"line_number\": 201,\n      \"suggestion\": \"Catch specific exceptions: `except (litellm.APIError, litellm.RateLimitError, asyncio.TimeoutError) as exc:`\"\n    },\n    {\n      \"type\": \"Data Validation\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_parse_dimension_scores` method doesn't validate that `raw_score` exists in the entry mapping before calling `_normalize_score`. If 'score' key is missing, it passes `None` to `_coerce_numeric`.\",\n      \"line_number\": 265,\n      \"suggestion\": \"Add explicit check: `if raw_score is None: raise EvaluationError(f\\\"Score missing for dimension '{name}'\\\")`\"\n    },\n    {\n      \"type\": \"Robustness\",\n      \"severity\": \"low\",\n      \"description\": \"The `_extract_max_score` regex doesn't handle edge cases like scientific notation (1e2) or negative numbers in scales. The default fallback to 100 could mask parsing errors.\",\n      \"line_number\": 283,\n      \"suggestion\": \"Add logging when falling back to default: `self._logger.warning(f\\\"Could not parse scale '{scale}', defaulting to 100\\\")`\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Inconsistent use of `Mapping` vs `Dict` type hints. `Mapping` is used for parameters but `Dict` for return types, making it unclear when mutability matters.\",\n      \"line_number\": [47, 127, 260],\n      \"suggestion\": \"Standardize: use `Mapping` for inputs (read-only), `Dict` for outputs (mutable)\"\n    },\n    {\n      \"type\": \"Testing\",\n      \"severity\": \"low\",\n      \"description\": \"The `# pragma: no cover` comment on line 201 suggests external LiteLLM errors aren't tested. This is a critical path that should have test coverage with mocked LiteLLM.\",\n      \"line_number\": 201,\n      \"suggestion\": \"Create integration tests with mocked litellm.acompletion to verify retry logic and error handling\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"The `_format_rubric` method iterates through rubric items twice (once implicitly in the loop, once in join). Minor inefficiency but could matter with large rubrics.\",\n      \"line_number\": 227,\n      \"suggestion\": \"Use list comprehension: `lines = [f\\\"- {name}: ...\\\" for name, metadata in dimensions]`\"\n    },\n    {\n      \"type\": \"Security\",\n      \"severity\": \"low\",\n      \"description\": \"User-provided `workflow_output` and `extra_instructions` are directly interpolated into prompts without sanitization. While not a direct injection risk with LLMs, could leak sensitive data or cause prompt injection.\",\n      \"line_number\": [158, 159],\n      \"suggestion\": \"Add optional parameter to sanitize/redact sensitive patterns, or document that users should pre-sanitize inputs\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"category\": \"Error Handling\",\n      \"priority\": \"high\",\n      \"description\": \"Create a custom exception hierarchy (e.g., `CacheError`, `LLMResponseError`, `ValidationError`) instead of generic `EvaluationError` for all cases. This enables better error recovery and logging.\",\n      \"example\": \"class LLMResponseError(EvaluationError): pass\"\n    },\n    {\n      \"category\": \"Testing\",\n      \"priority\": \"high\",\n      \"description\": \"Add comprehensive unit tests for edge cases: empty rubric, malformed JSON responses, out-of-range scores, missing dimensions, and various LLM response formats.\",\n      \"example\": \"Create test fixtures for different LiteLLM response structures (dict vs object, list vs string content)\"\n    },\n    {\n      \"category\": \"Logging\",\n      \"priority\": \"medium\",\n      \"description\": \"Add structured logging with context (cache_key, model, attempt) to improve observability. Current logging is sparse for debugging production issues.\",\n      \"example\": \"Use `self._logger.info('Evaluation complete', extra={'model': selected_model, 'dimensions': len(dimension_scores)})`\"\n    },\n    {\n      \"category\": \"Documentation\",\n      \"priority\": \"medium\",\n      \"description\": \"Add docstrings to private methods explaining expected input/output formats, especially for `_extract_response_content` and `_parse_dimension_scores`.\",\n      \"example\": \"Document that `_extract_response_content` expects LiteLLM response format with specific structure\"\n    },\n    {\n      \"category\": \"Configuration\",\n      \"priority\": \"medium\",\n      \"description\": \"Make retry strategy configurable (exponential vs linear backoff, jitter percentage) rather than hardcoded. Consider using `tenacity` library for more robust retry handling.\",\n      \"example\": \"Add `retry_strategy` parameter to __init__ accepting a callable or enum\"\n    },\n    {\n      \"category\": \"Type Safety\",\n      \"priority\": \"medium\",\n      \"description\": \"Add `@overload` decorators for methods with optional parameters to improve IDE support and type checking. Consider using `TypedDict` for rubric structure.\",\n      \"example\": \"Define `RubricDimension = TypedDict('RubricDimension', {'description': str, 'scale': str})`\"\n    },\n    {\n      \"category\": \"Performance\",\n      \"priority\": \"low\",\n      \"description\": \"Consider caching the compiled regex pattern in `_extract_max_score` as a class variable to avoid recompilation on each call.\",\n      \"example\": \"_SCALE_PATTERN = re.compile(r'(\\\\d+(?:\\\\.\\\\d+)?)\\\\s*-\\\\s*(\\\\d+(?:\\\\.\\\\d+)?)')\"\n    },\n    {\n      \"category\": \"Maintainability\",\n      \"priority\": \"low\",\n      \"description\": \"Extract the JSON response template string to a constant to reduce duplication and make it easier to modify the expected format.\",\n      \"example\": \"Move the template to a class constant `_JSON_RESPONSE_TEMPLATE`\"\n    }\n  ]\n}\n```",
        "test_config": {
          "test_number": 4,
          "config": {
            "temperature": 0.3,
            "context_size": "full_module",
            "generation_strategy": "chain_of_thought",
            "model": "openrouter/anthropic/claude-haiku-4.5"
          }
        },
        "workflow": {
          "started_at": "2025-10-28T01:51:47.723803Z",
          "completed_at": "2025-10-28T01:52:10.034330Z",
          "duration_seconds": 22.310304207989248
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 75,
              "reasoning": "The code generally accomplishes its intended purpose of evaluating workflow outputs, but there are notable gaps in handling edge cases, such as malformed LLM responses and incomplete input validation. The score extraction logic is somewhat robust but could be improved with better bounds checking and JSON parsing."
            },
            "error_handling": {
              "score": 70,
              "reasoning": "The code includes some error handling, but it is not comprehensive. High-priority issues like the lack of bounds checking in `_extract_response_content` and the broad `except Exception` catch reduce its effectiveness. Custom exceptions and more specific error handling would improve this dimension."
            },
            "code_quality": {
              "score": 80,
              "reasoning": "The code follows best practices with clear separation of concerns and generally good design. However, there are inconsistencies in type hints and the use of `assert` statements, which detract from overall quality. Improvements in these areas would enhance maintainability."
            },
            "performance": {
              "score": 85,
              "reasoning": "The code is generally efficient, with minimal unnecessary LLM calls and reasonable token usage. However, there are minor inefficiencies, such as the repeated conversion of `messages` to a list inside the retry loop, which could be optimized."
            },
            "testability": {
              "score": 70,
              "reasoning": "The code has potential for good test coverage, but there are gaps, particularly in testing external LiteLLM errors. The use of `# pragma: no cover` suggests critical paths are not fully tested. Comprehensive unit tests for edge cases would improve this dimension."
            },
            "documentation": {
              "score": 75,
              "reasoning": "The code includes some documentation, but it could be more comprehensive, especially for private methods. Clearer docstrings and examples would improve understanding and usability. The type hints are generally accurate but could be more consistent."
            }
          },
          "temperature": 0.3,
          "cache_key": "14d023d6dcc3daa44d313fb5b6f9a61b38a4df69165f23fbc7e8b8265e29b02d",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "timestamp": "2025-10-28T01:52:28.039127Z"
    },
    {
      "test_number": 5,
      "config": {
        "test_number": 5,
        "config_values": {
          "temperature": 0.7,
          "context_size": "file_only",
          "generation_strategy": "chain_of_thought",
          "model": "openrouter/deepseek/deepseek-chat"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.85,
            "reasoning": "The code generally accomplishes its intended purpose correctly, but there are identified issues with handling edge cases like malformed JSON responses and unexpected data structures. The score extraction and normalization logic are not explicitly mentioned, but the overall functionality is solid."
          },
          "error_handling": {
            "score": 0.8,
            "reasoning": "The code demonstrates good error handling practices, but there are gaps in validating JSON responses and handling unexpected data structures. The error messages and logging could be more detailed to aid in debugging."
          },
          "code_quality": {
            "score": 0.9,
            "reasoning": "The code is well-structured with good use of type hints and modular design. However, there are minor issues with string concatenation and method assumptions that could be improved for better readability and maintainability."
          },
          "performance": {
            "score": 0.75,
            "reasoning": "The code has some performance inefficiencies, such as recalculating the base delay and jitter on every retry attempt. There is room for optimization in terms of caching and reducing computational overhead."
          },
          "testability": {
            "score": 0.7,
            "reasoning": "The code lacks comprehensive test coverage for edge cases. While the interfaces are clear, the absence of explicit test cases for scenarios like malformed JSON responses and invalid cache keys reduces the testability score."
          },
          "documentation": {
            "score": 0.85,
            "reasoning": "The code is well-documented with clear docstrings and type hints. However, there could be more inline comments for complex logic and examples to further enhance clarity."
          }
        },
        "overall_score": 0.8083333333333332,
        "evaluator_model": "openrouter/deepseek/deepseek-chat",
        "timestamp": "2025-10-28T01:52:41.415961Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 85,
              "reasoning": "The code generally accomplishes its intended purpose correctly, but there are identified issues with handling edge cases like malformed JSON responses and unexpected data structures. The score extraction and normalization logic are not explicitly mentioned, but the overall functionality is solid."
            },
            "error_handling": {
              "score": 80,
              "reasoning": "The code demonstrates good error handling practices, but there are gaps in validating JSON responses and handling unexpected data structures. The error messages and logging could be more detailed to aid in debugging."
            },
            "code_quality": {
              "score": 90,
              "reasoning": "The code is well-structured with good use of type hints and modular design. However, there are minor issues with string concatenation and method assumptions that could be improved for better readability and maintainability."
            },
            "performance": {
              "score": 75,
              "reasoning": "The code has some performance inefficiencies, such as recalculating the base delay and jitter on every retry attempt. There is room for optimization in terms of caching and reducing computational overhead."
            },
            "testability": {
              "score": 70,
              "reasoning": "The code lacks comprehensive test coverage for edge cases. While the interfaces are clear, the absence of explicit test cases for scenarios like malformed JSON responses and invalid cache keys reduces the testability score."
            },
            "documentation": {
              "score": 85,
              "reasoning": "The code is well-documented with clear docstrings and type hints. However, there could be more inline comments for complex logic and examples to further enhance clarity."
            }
          },
          "temperature": 0.3,
          "cache_key": "daf5d674ca5a57917e2f37b88a30755288a75b46fb04914f7e2ce73f5e88e91a",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "cost": 0.0,
      "latency": 8220.873541984474,
      "utility": 0.5624056133986929,
      "workflow_output": "The code is generally well-structured and follows best practices. It demonstrates good use of type hints, modular design, and thorough error handling. However, there are areas for improvement in terms of performance optimization, security considerations, and testing coverage.\nIssues:\n- (medium) The `_request_with_retry` method uses exponential backoff with jitter, but the retry delay calculation could be optimized further. The current implementation recalculates the base delay and jitter on every attempt, which is unnecessary. [line 167] Suggested fix: Consider precomputing the base delay and jitter values outside the loop to reduce computational overhead.\n- (medium) The code does not validate the content of the JSON response from the LLM for potential malicious payloads or unexpected data structures. [line 254] Suggested fix: Implement additional validation to ensure the JSON response adheres to the expected schema and does not contain harmful content.\n- (low) The `_extract_response_content` method assumes that the response structure will always contain 'choices' and 'message' fields. This could lead to runtime errors if the response structure changes. [line 214] Suggested fix: Add more robust checks to handle cases where the response structure is unexpected or malformed.\n- (low) The `_build_prompt` method concatenates strings using the `+` operator, which can be less efficient and harder to read for long strings. [line 179] Suggested fix: Use f-strings or a multi-line string with `.format()` for better readability and maintainability.\n- (medium) The code lacks explicit test cases for edge cases such as malformed JSON responses, invalid cache keys, and extreme temperature values. Suggested fix: Add comprehensive unit tests to cover edge cases and ensure robustness.\nSuggestions:\n- Consider using a JSON schema validator to ensure the LLM response adheres to the expected format.\n- Implement caching for the retry delay calculation in `_request_with_retry` to improve performance.\n- Add more detailed logging for debugging purposes, especially around cache operations and retry attempts.\n- Use f-strings or `.format()` for string concatenation in `_build_prompt` to improve readability.\n- Expand the test suite to cover edge cases and improve overall test coverage.",
      "metadata": {
        "strategy": "chain_of_thought",
        "model": "openrouter/deepseek/deepseek-chat",
        "temperature": 0.7,
        "context_size": "file_only",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Review limited to the provided file.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"The code is generally well-structured and follows best practices. It demonstrates good use of type hints, modular design, and thorough error handling. However, there are areas for improvement in terms of performance optimization, security considerations, and testing coverage.\",\n  \"issues\": [\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_request_with_retry` method uses exponential backoff with jitter, but the retry delay calculation could be optimized further. The current implementation recalculates the base delay and jitter on every attempt, which is unnecessary.\",\n      \"line_number\": 167,\n      \"suggestion\": \"Consider precomputing the base delay and jitter values outside the loop to reduce computational overhead.\"\n    },\n    {\n      \"type\": \"Security\",\n      \"severity\": \"medium\",\n      \"description\": \"The code does not validate the content of the JSON response from the LLM for potential malicious payloads or unexpected data structures.\",\n      \"line_number\": 254,\n      \"suggestion\": \"Implement additional validation to ensure the JSON response adheres to the expected schema and does not contain harmful content.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"low\",\n      \"description\": \"The `_extract_response_content` method assumes that the response structure will always contain 'choices' and 'message' fields. This could lead to runtime errors if the response structure changes.\",\n      \"line_number\": 214,\n      \"suggestion\": \"Add more robust checks to handle cases where the response structure is unexpected or malformed.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"The `_build_prompt` method concatenates strings using the `+` operator, which can be less efficient and harder to read for long strings.\",\n      \"line_number\": 179,\n      \"suggestion\": \"Use f-strings or a multi-line string with `.format()` for better readability and maintainability.\"\n    },\n    {\n      \"type\": \"Testing\",\n      \"severity\": \"medium\",\n      \"description\": \"The code lacks explicit test cases for edge cases such as malformed JSON responses, invalid cache keys, and extreme temperature values.\",\n      \"suggestion\": \"Add comprehensive unit tests to cover edge cases and ensure robustness.\"\n    }\n  ],\n  \"suggestions\": [\n    \"Consider using a JSON schema validator to ensure the LLM response adheres to the expected format.\",\n    \"Implement caching for the retry delay calculation in `_request_with_retry` to improve performance.\",\n    \"Add more detailed logging for debugging purposes, especially around cache operations and retry attempts.\",\n    \"Use f-strings or `.format()` for string concatenation in `_build_prompt` to improve readability.\",\n    \"Expand the test suite to cover edge cases and improve overall test coverage.\"\n  ]\n}\n```",
        "test_config": {
          "test_number": 5,
          "config": {
            "temperature": 0.7,
            "context_size": "file_only",
            "generation_strategy": "chain_of_thought",
            "model": "openrouter/deepseek/deepseek-chat"
          }
        },
        "workflow": {
          "started_at": "2025-10-28T01:52:28.043138Z",
          "completed_at": "2025-10-28T01:52:36.265131Z",
          "duration_seconds": 8.220873541984474
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 85,
              "reasoning": "The code generally accomplishes its intended purpose correctly, but there are identified issues with handling edge cases like malformed JSON responses and unexpected data structures. The score extraction and normalization logic are not explicitly mentioned, but the overall functionality is solid."
            },
            "error_handling": {
              "score": 80,
              "reasoning": "The code demonstrates good error handling practices, but there are gaps in validating JSON responses and handling unexpected data structures. The error messages and logging could be more detailed to aid in debugging."
            },
            "code_quality": {
              "score": 90,
              "reasoning": "The code is well-structured with good use of type hints and modular design. However, there are minor issues with string concatenation and method assumptions that could be improved for better readability and maintainability."
            },
            "performance": {
              "score": 75,
              "reasoning": "The code has some performance inefficiencies, such as recalculating the base delay and jitter on every retry attempt. There is room for optimization in terms of caching and reducing computational overhead."
            },
            "testability": {
              "score": 70,
              "reasoning": "The code lacks comprehensive test coverage for edge cases. While the interfaces are clear, the absence of explicit test cases for scenarios like malformed JSON responses and invalid cache keys reduces the testability score."
            },
            "documentation": {
              "score": 85,
              "reasoning": "The code is well-documented with clear docstrings and type hints. However, there could be more inline comments for complex logic and examples to further enhance clarity."
            }
          },
          "temperature": 0.3,
          "cache_key": "daf5d674ca5a57917e2f37b88a30755288a75b46fb04914f7e2ce73f5e88e91a",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "timestamp": "2025-10-28T01:52:41.416227Z"
    },
    {
      "test_number": 6,
      "config": {
        "test_number": 6,
        "config_values": {
          "temperature": 0.7,
          "context_size": "file_only",
          "generation_strategy": "chain_of_thought",
          "model": "openrouter/anthropic/claude-haiku-4.5"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.85,
            "reasoning": "The code accomplishes its intended purpose of evaluating workflow outputs effectively. It handles most edge cases and has robust score extraction and normalization logic. However, there are concerns around type safety and some edge case handling in JSON parsing that could lead to unexpected behavior."
          },
          "error_handling": {
            "score": 0.8,
            "reasoning": "The code demonstrates strong error handling with appropriate exception handling and defensive programming. However, the use of bare `except Exception` and `assert` statements for type narrowing are problematic. Error messages could include more context to aid debugging."
          },
          "code_quality": {
            "score": 0.75,
            "reasoning": "The code is well-structured with clear separation of concerns and good method naming. However, there are issues with type hints, dependency injection, and the use of `assert` statements. The code could benefit from better abstractions and more explicit type safety measures."
          },
          "performance": {
            "score": 0.9,
            "reasoning": "The code is efficient with minimal unnecessary LLM calls and an appropriate caching strategy. There are no obvious performance bottlenecks, and the use of regex patterns is efficient. However, there is room for improvement in caching formatted rubric strings and handling large inputs."
          },
          "testability": {
            "score": 0.7,
            "reasoning": "The code is somewhat testable but lacks clear interfaces for mocking and dependency injection. The heavy reliance on `litellm.acompletion` makes unit testing difficult. There is potential for good test coverage, but the code needs better contracts and fixtures for common test data."
          },
          "documentation": {
            "score": 0.75,
            "reasoning": "The code has clear docstrings for public APIs and inline comments for complex logic. However, private methods lack detailed docstrings, and there is room for improvement in documenting the JSON response schema more formally. Type hints could be more accurate and comprehensive."
          }
        },
        "overall_score": 0.7916666666666666,
        "evaluator_model": "openrouter/deepseek/deepseek-chat",
        "timestamp": "2025-10-28T01:53:12.254105Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 85,
              "reasoning": "The code accomplishes its intended purpose of evaluating workflow outputs effectively. It handles most edge cases and has robust score extraction and normalization logic. However, there are concerns around type safety and some edge case handling in JSON parsing that could lead to unexpected behavior."
            },
            "error_handling": {
              "score": 80,
              "reasoning": "The code demonstrates strong error handling with appropriate exception handling and defensive programming. However, the use of bare `except Exception` and `assert` statements for type narrowing are problematic. Error messages could include more context to aid debugging."
            },
            "code_quality": {
              "score": 75,
              "reasoning": "The code is well-structured with clear separation of concerns and good method naming. However, there are issues with type hints, dependency injection, and the use of `assert` statements. The code could benefit from better abstractions and more explicit type safety measures."
            },
            "performance": {
              "score": 90,
              "reasoning": "The code is efficient with minimal unnecessary LLM calls and an appropriate caching strategy. There are no obvious performance bottlenecks, and the use of regex patterns is efficient. However, there is room for improvement in caching formatted rubric strings and handling large inputs."
            },
            "testability": {
              "score": 70,
              "reasoning": "The code is somewhat testable but lacks clear interfaces for mocking and dependency injection. The heavy reliance on `litellm.acompletion` makes unit testing difficult. There is potential for good test coverage, but the code needs better contracts and fixtures for common test data."
            },
            "documentation": {
              "score": 75,
              "reasoning": "The code has clear docstrings for public APIs and inline comments for complex logic. However, private methods lack detailed docstrings, and there is room for improvement in documenting the JSON response schema more formally. Type hints could be more accurate and comprehensive."
            }
          },
          "temperature": 0.3,
          "cache_key": "8f690e768bf431df73e5159132e334b1b08d888b88ed9c57ee302d3612c73909",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "cost": 0.0,
      "latency": 15746.166541997809,
      "utility": 0.4991588140496847,
      "workflow_output": "Well-structured code with strong error handling and validation. The rubric evaluator demonstrates good async patterns, caching support, and retry logic. Main concerns are around type safety, testability, and some edge case handling in JSON parsing. Overall quality is above average with room for improvement in a few areas.\nIssues:\n- (medium) Use of `assert` statements for type narrowing (lines 156, 180, 197) is problematic in production code. Assertions can be disabled with Python's -O flag, leaving code in unsafe state. Suggested fix: Replace `assert` with explicit type checks and raise `EvaluationError` if conditions aren't met. Example: `if resolved_cache_key is None: raise EvaluationError(\"Cache key required\")`\n- (high) Bare `except Exception` on line 171 catches all exceptions including KeyboardInterrupt and SystemExit (in Python 3.7+). This masks critical errors. [line 171] Suggested fix: Catch specific exceptions: `except (litellm.APIError, asyncio.TimeoutError, ValueError) as exc:` or similar based on LiteLLM's actual exception types.\n- (medium) The `_extract_response_content` method (lines 227-249) has complex conditional logic that could fail silently or return empty strings. Line 246 concatenates text from list items without validation. Suggested fix: Add explicit validation: ensure `combined` is non-empty before returning. Add logging for debugging when content is transformed.\n- (medium) In `_parse_dimension_scores` (line 289), if `entry` is neither a Mapping nor has a numeric score, the code may fail downstream. The fallback to treat `entry` as raw_score is risky. [line 289] Suggested fix: Validate that `raw_score` is numeric before normalization. Add explicit error message: `if not isinstance(entry, Mapping): raise EvaluationError(f\"Expected mapping for dimension {name}, got {type(entry)}\")`\n- (medium) Heavy dependency on `litellm.acompletion` makes unit testing difficult. No dependency injection or interface abstraction for the LLM client. [line 179] Suggested fix: Create an abstract `LLMClient` interface and inject it via constructor. This enables easy mocking in tests.\n- (low) The `_build_prompt` method (lines 209-226) builds a large multi-line string with string concatenation. Hard to maintain and prone to formatting errors. Suggested fix: Use `textwrap.dedent()` or f-string with triple quotes for better readability and maintainability.\n- (low) In `_extract_max_score` (line 303), the regex only handles numeric scales. Scales like 'A-F' or 'poor-excellent' would silently default to 100. [line 303] Suggested fix: Document the limitation or add support for categorical scales. Log a warning when falling back to default.\n- (low) The `_format_rubric` method rebuilds the rubric text on every evaluation. If the same rubric is used repeatedly, this is wasteful. [line 202] Suggested fix: Cache the formatted rubric string or compute it once during initialization if using default rubric.\n- (low) No explicit validation that `workflow_output` doesn't exceed reasonable size limits. Very large inputs could cause memory issues or API failures. [line 113] Suggested fix: Add a max length check: `if len(workflow_output) > MAX_OUTPUT_LENGTH: raise EvaluationError(\"Output too large\")`\n- (low) The `_request_with_retry` method logs at WARNING level for retries (line 189), but this is normal behavior, not necessarily a warning. [line 189] Suggested fix: Use INFO level for retries and WARNING only for unexpected failures close to max retries.\nSuggestions:\n- {'category': 'Type Safety', 'description': 'Add type hints to return types of helper methods like `_extract_response_content` and `_load_json`. Consider using `TypedDict` for rubric dimensions and response structures.', 'priority': 'medium'}\n- {'category': 'Testing', 'description': 'Create fixtures for common test data (sample rubrics, mock responses, etc.). Add parametrized tests for edge cases like malformed JSON, missing dimensions, out-of-range scores.', 'priority': 'high'}\n- {'category': 'Documentation', 'description': 'Add docstrings to private methods explaining expected input/output formats and exceptions. Document the JSON response schema more formally.', 'priority': 'medium'}\n- {'category': 'Constants', 'description': 'Extract magic strings and numbers to named constants (e.g., `JSON_RESPONSE_TEMPLATE`, `MAX_RETRIES`, `DEFAULT_MAX_SCORE`). Makes configuration clearer.', 'priority': 'low'}\n- {'category': 'Validation', 'description': 'Consider validating the rubric structure in `__init__` or `evaluate` to fail fast if dimensions are malformed.', 'priority': 'low'}\n- {'category': 'Async Patterns', 'description': 'Consider using `asyncio.wait_for()` to add timeout protection around `litellm.acompletion()` calls to prevent indefinite hangs.', 'priority': 'medium'}\n- {'category': 'Error Messages', 'description': 'Include more context in error messages (e.g., actual vs. expected types, received values when safe). This aids debugging.', 'priority': 'low'}",
      "metadata": {
        "strategy": "chain_of_thought",
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "temperature": 0.7,
        "context_size": "file_only",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Review limited to the provided file.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"Well-structured code with strong error handling and validation. The rubric evaluator demonstrates good async patterns, caching support, and retry logic. Main concerns are around type safety, testability, and some edge case handling in JSON parsing. Overall quality is above average with room for improvement in a few areas.\",\n  \"issues\": [\n    {\n      \"type\": \"Type Safety\",\n      \"severity\": \"medium\",\n      \"description\": \"Use of `assert` statements for type narrowing (lines 156, 180, 197) is problematic in production code. Assertions can be disabled with Python's -O flag, leaving code in unsafe state.\",\n      \"line_number\": [156, 180, 197],\n      \"suggestion\": \"Replace `assert` with explicit type checks and raise `EvaluationError` if conditions aren't met. Example: `if resolved_cache_key is None: raise EvaluationError(\\\"Cache key required\\\")`\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"high\",\n      \"description\": \"Bare `except Exception` on line 171 catches all exceptions including KeyboardInterrupt and SystemExit (in Python 3.7+). This masks critical errors.\",\n      \"line_number\": 171,\n      \"suggestion\": \"Catch specific exceptions: `except (litellm.APIError, asyncio.TimeoutError, ValueError) as exc:` or similar based on LiteLLM's actual exception types.\"\n    },\n    {\n      \"type\": \"Input Validation\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_extract_response_content` method (lines 227-249) has complex conditional logic that could fail silently or return empty strings. Line 246 concatenates text from list items without validation.\",\n      \"line_number\": [227, 246],\n      \"suggestion\": \"Add explicit validation: ensure `combined` is non-empty before returning. Add logging for debugging when content is transformed.\"\n    },\n    {\n      \"type\": \"Robustness\",\n      \"severity\": \"medium\",\n      \"description\": \"In `_parse_dimension_scores` (line 289), if `entry` is neither a Mapping nor has a numeric score, the code may fail downstream. The fallback to treat `entry` as raw_score is risky.\",\n      \"line_number\": 289,\n      \"suggestion\": \"Validate that `raw_score` is numeric before normalization. Add explicit error message: `if not isinstance(entry, Mapping): raise EvaluationError(f\\\"Expected mapping for dimension {name}, got {type(entry)}\\\")`\"\n    },\n    {\n      \"type\": \"Testability\",\n      \"severity\": \"medium\",\n      \"description\": \"Heavy dependency on `litellm.acompletion` makes unit testing difficult. No dependency injection or interface abstraction for the LLM client.\",\n      \"line_number\": 179,\n      \"suggestion\": \"Create an abstract `LLMClient` interface and inject it via constructor. This enables easy mocking in tests.\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"The `_build_prompt` method (lines 209-226) builds a large multi-line string with string concatenation. Hard to maintain and prone to formatting errors.\",\n      \"line_number\": [209, 226],\n      \"suggestion\": \"Use `textwrap.dedent()` or f-string with triple quotes for better readability and maintainability.\"\n    },\n    {\n      \"type\": \"Edge Case\",\n      \"severity\": \"low\",\n      \"description\": \"In `_extract_max_score` (line 303), the regex only handles numeric scales. Scales like 'A-F' or 'poor-excellent' would silently default to 100.\",\n      \"line_number\": 303,\n      \"suggestion\": \"Document the limitation or add support for categorical scales. Log a warning when falling back to default.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"The `_format_rubric` method rebuilds the rubric text on every evaluation. If the same rubric is used repeatedly, this is wasteful.\",\n      \"line_number\": 202,\n      \"suggestion\": \"Cache the formatted rubric string or compute it once during initialization if using default rubric.\"\n    },\n    {\n      \"type\": \"Security\",\n      \"severity\": \"low\",\n      \"description\": \"No explicit validation that `workflow_output` doesn't exceed reasonable size limits. Very large inputs could cause memory issues or API failures.\",\n      \"line_number\": 113,\n      \"suggestion\": \"Add a max length check: `if len(workflow_output) > MAX_OUTPUT_LENGTH: raise EvaluationError(\\\"Output too large\\\")`\"\n    },\n    {\n      \"type\": \"Logging\",\n      \"severity\": \"low\",\n      \"description\": \"The `_request_with_retry` method logs at WARNING level for retries (line 189), but this is normal behavior, not necessarily a warning.\",\n      \"line_number\": 189,\n      \"suggestion\": \"Use INFO level for retries and WARNING only for unexpected failures close to max retries.\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"category\": \"Type Safety\",\n      \"description\": \"Add type hints to return types of helper methods like `_extract_response_content` and `_load_json`. Consider using `TypedDict` for rubric dimensions and response structures.\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"category\": \"Testing\",\n      \"description\": \"Create fixtures for common test data (sample rubrics, mock responses, etc.). Add parametrized tests for edge cases like malformed JSON, missing dimensions, out-of-range scores.\",\n      \"priority\": \"high\"\n    },\n    {\n      \"category\": \"Documentation\",\n      \"description\": \"Add docstrings to private methods explaining expected input/output formats and exceptions. Document the JSON response schema more formally.\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"category\": \"Constants\",\n      \"description\": \"Extract magic strings and numbers to named constants (e.g., `JSON_RESPONSE_TEMPLATE`, `MAX_RETRIES`, `DEFAULT_MAX_SCORE`). Makes configuration clearer.\",\n      \"priority\": \"low\"\n    },\n    {\n      \"category\": \"Validation\",\n      \"description\": \"Consider validating the rubric structure in `__init__` or `evaluate` to fail fast if dimensions are malformed.\",\n      \"priority\": \"low\"\n    },\n    {\n      \"category\": \"Async Patterns\",\n      \"description\": \"Consider using `asyncio.wait_for()` to add timeout protection around `litellm.acompletion()` calls to prevent indefinite hangs.\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"category\": \"Error Messages\",\n      \"description\": \"Include more context in error messages (e.g., actual vs. expected types, received values when safe). This aids debugging.\",\n      \"priority\": \"low\"\n    }\n  ]\n}\n```",
        "test_config": {
          "test_number": 6,
          "config": {
            "temperature": 0.7,
            "context_size": "file_only",
            "generation_strategy": "chain_of_thought",
            "model": "openrouter/anthropic/claude-haiku-4.5"
          }
        },
        "workflow": {
          "started_at": "2025-10-28T01:52:41.418744Z",
          "completed_at": "2025-10-28T01:52:57.165656Z",
          "duration_seconds": 15.746166541997809
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 85,
              "reasoning": "The code accomplishes its intended purpose of evaluating workflow outputs effectively. It handles most edge cases and has robust score extraction and normalization logic. However, there are concerns around type safety and some edge case handling in JSON parsing that could lead to unexpected behavior."
            },
            "error_handling": {
              "score": 80,
              "reasoning": "The code demonstrates strong error handling with appropriate exception handling and defensive programming. However, the use of bare `except Exception` and `assert` statements for type narrowing are problematic. Error messages could include more context to aid debugging."
            },
            "code_quality": {
              "score": 75,
              "reasoning": "The code is well-structured with clear separation of concerns and good method naming. However, there are issues with type hints, dependency injection, and the use of `assert` statements. The code could benefit from better abstractions and more explicit type safety measures."
            },
            "performance": {
              "score": 90,
              "reasoning": "The code is efficient with minimal unnecessary LLM calls and an appropriate caching strategy. There are no obvious performance bottlenecks, and the use of regex patterns is efficient. However, there is room for improvement in caching formatted rubric strings and handling large inputs."
            },
            "testability": {
              "score": 70,
              "reasoning": "The code is somewhat testable but lacks clear interfaces for mocking and dependency injection. The heavy reliance on `litellm.acompletion` makes unit testing difficult. There is potential for good test coverage, but the code needs better contracts and fixtures for common test data."
            },
            "documentation": {
              "score": 75,
              "reasoning": "The code has clear docstrings for public APIs and inline comments for complex logic. However, private methods lack detailed docstrings, and there is room for improvement in documenting the JSON response schema more formally. Type hints could be more accurate and comprehensive."
            }
          },
          "temperature": 0.3,
          "cache_key": "8f690e768bf431df73e5159132e334b1b08d888b88ed9c57ee302d3612c73909",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "timestamp": "2025-10-28T01:53:12.254953Z"
    },
    {
      "test_number": 7,
      "config": {
        "test_number": 7,
        "config_values": {
          "temperature": 0.7,
          "context_size": "full_module",
          "generation_strategy": "standard",
          "model": "openrouter/deepseek/deepseek-chat"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.75,
            "reasoning": "The code generally accomplishes its intended purpose but has notable gaps in handling edge cases, such as malformed LLM responses and missing scores. The normalization logic and default rubric are well-designed, but the score extraction logic could be more robust."
          },
          "error_handling": {
            "score": 0.7,
            "reasoning": "The code includes some error handling but lacks comprehensive validation for inputs and responses. Clear error messages and logging are present, but the handling of LLM failures and unexpected response formats could be improved."
          },
          "code_quality": {
            "score": 0.8,
            "reasoning": "The code is well-structured with good use of type hints and modular design. However, there are areas where readability and maintainability could be enhanced, such as using multi-line strings for better clarity and simplifying retry delay calculations."
          },
          "performance": {
            "score": 0.75,
            "reasoning": "The code is generally efficient but could benefit from a more robust caching strategy and optimization of retry delay calculations. There are no obvious performance bottlenecks, but some areas could be streamlined for better efficiency."
          },
          "testability": {
            "score": 0.7,
            "reasoning": "The code has clear interfaces and testable methods, but there is room for improvement in test coverage, especially for edge cases. Adding unit tests for methods like `_extract_response_content` and `_parse_dimension_scores` would enhance testability."
          },
          "documentation": {
            "score": 0.8,
            "reasoning": "The code is well-documented with clear docstrings and type hints. Inline comments and examples are present where helpful, and variable/method names are clear. However, more detailed documentation for complex logic could be beneficial."
          }
        },
        "overall_score": 0.75,
        "evaluator_model": "openrouter/deepseek/deepseek-chat",
        "timestamp": "2025-10-28T01:53:29.092661Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 75,
              "reasoning": "The code generally accomplishes its intended purpose but has notable gaps in handling edge cases, such as malformed LLM responses and missing scores. The normalization logic and default rubric are well-designed, but the score extraction logic could be more robust."
            },
            "error_handling": {
              "score": 70,
              "reasoning": "The code includes some error handling but lacks comprehensive validation for inputs and responses. Clear error messages and logging are present, but the handling of LLM failures and unexpected response formats could be improved."
            },
            "code_quality": {
              "score": 80,
              "reasoning": "The code is well-structured with good use of type hints and modular design. However, there are areas where readability and maintainability could be enhanced, such as using multi-line strings for better clarity and simplifying retry delay calculations."
            },
            "performance": {
              "score": 75,
              "reasoning": "The code is generally efficient but could benefit from a more robust caching strategy and optimization of retry delay calculations. There are no obvious performance bottlenecks, but some areas could be streamlined for better efficiency."
            },
            "testability": {
              "score": 70,
              "reasoning": "The code has clear interfaces and testable methods, but there is room for improvement in test coverage, especially for edge cases. Adding unit tests for methods like `_extract_response_content` and `_parse_dimension_scores` would enhance testability."
            },
            "documentation": {
              "score": 80,
              "reasoning": "The code is well-documented with clear docstrings and type hints. Inline comments and examples are present where helpful, and variable/method names are clear. However, more detailed documentation for complex logic could be beneficial."
            }
          },
          "temperature": 0.3,
          "cache_key": "91a405ab8267e5d9018561136778b72ce2eeaf0bdface768050a279a3c3c0904",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "cost": 0.0,
      "latency": 9054.915499989875,
      "utility": 0.5158555598185224,
      "workflow_output": "The code is well-structured and follows Python best practices, with good use of type hints, error handling, and modular design. However, there are areas for improvement in error handling, readability, and performance.\nIssues:\n- (medium) The `_extract_response_content` method assumes the response structure but does not handle unexpected formats gracefully. [line 239] Suggested fix: Add more robust validation for the response structure, including handling nested attributes more safely.\n- (low) The `_build_prompt` method constructs a large string using concatenation, which can be hard to read and maintain. [line 207] Suggested fix: Use a multi-line string (f-string) or template literals to improve readability.\n- (low) The `_compute_retry_delay` method uses `random.uniform` for jitter, which may not be necessary for all retry strategies. [line 193] Suggested fix: Consider simplifying the retry delay calculation unless jitter is explicitly required.\n- (medium) The `_parse_dimension_scores` method assumes the payload contains all rubric dimensions but does not handle missing or unexpected dimensions. [line 290] Suggested fix: Add validation to ensure all required dimensions are present in the payload and handle missing dimensions gracefully.\nSuggestions:\n- Consider using a more structured approach for handling prompt construction, such as using a template engine or a dedicated class.\n- Add more comprehensive logging for debugging purposes, especially in the `_request_with_retry` method to track retry attempts and failures.\n- Implement a more robust caching strategy, including cache invalidation and handling cache expiration.\n- Consider adding unit tests for edge cases, especially for methods like `_extract_response_content` and `_parse_dimension_scores`.\n- Optimize the `_compute_retry_delay` method by removing unnecessary jitter unless it is explicitly required for the retry strategy.",
      "metadata": {
        "strategy": "standard",
        "model": "openrouter/deepseek/deepseek-chat",
        "temperature": 0.7,
        "context_size": "full_module",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Consider surrounding module context for the review.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"The code is well-structured and follows Python best practices, with good use of type hints, error handling, and modular design. However, there are areas for improvement in error handling, readability, and performance.\",\n  \"issues\": [\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_extract_response_content` method assumes the response structure but does not handle unexpected formats gracefully.\",\n      \"line_number\": 239,\n      \"suggestion\": \"Add more robust validation for the response structure, including handling nested attributes more safely.\"\n    },\n    {\n      \"type\": \"Readability\",\n      \"severity\": \"low\",\n      \"description\": \"The `_build_prompt` method constructs a large string using concatenation, which can be hard to read and maintain.\",\n      \"line_number\": 207,\n      \"suggestion\": \"Use a multi-line string (f-string) or template literals to improve readability.\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"The `_compute_retry_delay` method uses `random.uniform` for jitter, which may not be necessary for all retry strategies.\",\n      \"line_number\": 193,\n      \"suggestion\": \"Consider simplifying the retry delay calculation unless jitter is explicitly required.\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"medium\",\n      \"description\": \"The `_parse_dimension_scores` method assumes the payload contains all rubric dimensions but does not handle missing or unexpected dimensions.\",\n      \"line_number\": 290,\n      \"suggestion\": \"Add validation to ensure all required dimensions are present in the payload and handle missing dimensions gracefully.\"\n    }\n  ],\n  \"suggestions\": [\n    \"Consider using a more structured approach for handling prompt construction, such as using a template engine or a dedicated class.\",\n    \"Add more comprehensive logging for debugging purposes, especially in the `_request_with_retry` method to track retry attempts and failures.\",\n    \"Implement a more robust caching strategy, including cache invalidation and handling cache expiration.\",\n    \"Consider adding unit tests for edge cases, especially for methods like `_extract_response_content` and `_parse_dimension_scores`.\",\n    \"Optimize the `_compute_retry_delay` method by removing unnecessary jitter unless it is explicitly required for the retry strategy.\"\n  ]\n}\n```",
        "test_config": {
          "test_number": 7,
          "config": {
            "temperature": 0.7,
            "context_size": "full_module",
            "generation_strategy": "standard",
            "model": "openrouter/deepseek/deepseek-chat"
          }
        },
        "workflow": {
          "started_at": "2025-10-28T01:53:12.259576Z",
          "completed_at": "2025-10-28T01:53:21.314573Z",
          "duration_seconds": 9.054915499989875
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 75,
              "reasoning": "The code generally accomplishes its intended purpose but has notable gaps in handling edge cases, such as malformed LLM responses and missing scores. The normalization logic and default rubric are well-designed, but the score extraction logic could be more robust."
            },
            "error_handling": {
              "score": 70,
              "reasoning": "The code includes some error handling but lacks comprehensive validation for inputs and responses. Clear error messages and logging are present, but the handling of LLM failures and unexpected response formats could be improved."
            },
            "code_quality": {
              "score": 80,
              "reasoning": "The code is well-structured with good use of type hints and modular design. However, there are areas where readability and maintainability could be enhanced, such as using multi-line strings for better clarity and simplifying retry delay calculations."
            },
            "performance": {
              "score": 75,
              "reasoning": "The code is generally efficient but could benefit from a more robust caching strategy and optimization of retry delay calculations. There are no obvious performance bottlenecks, but some areas could be streamlined for better efficiency."
            },
            "testability": {
              "score": 70,
              "reasoning": "The code has clear interfaces and testable methods, but there is room for improvement in test coverage, especially for edge cases. Adding unit tests for methods like `_extract_response_content` and `_parse_dimension_scores` would enhance testability."
            },
            "documentation": {
              "score": 80,
              "reasoning": "The code is well-documented with clear docstrings and type hints. Inline comments and examples are present where helpful, and variable/method names are clear. However, more detailed documentation for complex logic could be beneficial."
            }
          },
          "temperature": 0.3,
          "cache_key": "91a405ab8267e5d9018561136778b72ce2eeaf0bdface768050a279a3c3c0904",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "timestamp": "2025-10-28T01:53:29.092905Z"
    },
    {
      "test_number": 8,
      "config": {
        "test_number": 8,
        "config_values": {
          "temperature": 0.7,
          "context_size": "full_module",
          "generation_strategy": "standard",
          "model": "openrouter/anthropic/claude-haiku-4.5"
        },
        "workflow": "code_review"
      },
      "quality_score": {
        "dimension_scores": {
          "functionality": {
            "score": 0.75,
            "reasoning": "The code generally accomplishes its intended purpose of evaluating workflow outputs, but there are notable issues with edge case handling, such as unvalidated list indexing and missing score checks. The normalization logic appears correct, but the default rubric could be improved with better type safety and validation."
          },
          "error_handling": {
            "score": 0.65,
            "reasoning": "Error handling is present but has significant gaps, such as catching generic exceptions and insufficient validation of inputs and outputs. Error messages are inconsistent, and logging could be improved for better debugging."
          },
          "code_quality": {
            "score": 0.7,
            "reasoning": "The code follows best practices like separation of concerns and type hints, but there are issues with type safety, inconsistent error message formatting, and the use of `assert` statements. The code could benefit from better abstractions and more explicit None checks."
          },
          "performance": {
            "score": 0.8,
            "reasoning": "The code is generally efficient with minimal unnecessary LLM calls and an appropriate caching strategy. However, there are minor inefficiencies like uncapped retry delays and uncompiled regex patterns that could be optimized."
          },
          "testability": {
            "score": 0.6,
            "reasoning": "The code has potential for good test coverage, but the presence of `# pragma: no cover` comments suggests untested error paths. Edge cases like malformed JSON and empty choices are not adequately tested, and the interfaces could be clearer for mocking."
          },
          "documentation": {
            "score": 0.65,
            "reasoning": "The code lacks docstrings for private methods and could benefit from more inline comments for complex logic. Type hints are used but could be more precise, and variable/method names are generally clear but could be more descriptive in some cases."
          }
        },
        "overall_score": 0.6916666666666668,
        "evaluator_model": "openrouter/deepseek/deepseek-chat",
        "timestamp": "2025-10-28T01:53:50.292440Z",
        "metadata": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 75,
              "reasoning": "The code generally accomplishes its intended purpose of evaluating workflow outputs, but there are notable issues with edge case handling, such as unvalidated list indexing and missing score checks. The normalization logic appears correct, but the default rubric could be improved with better type safety and validation."
            },
            "error_handling": {
              "score": 65,
              "reasoning": "Error handling is present but has significant gaps, such as catching generic exceptions and insufficient validation of inputs and outputs. Error messages are inconsistent, and logging could be improved for better debugging."
            },
            "code_quality": {
              "score": 70,
              "reasoning": "The code follows best practices like separation of concerns and type hints, but there are issues with type safety, inconsistent error message formatting, and the use of `assert` statements. The code could benefit from better abstractions and more explicit None checks."
            },
            "performance": {
              "score": 80,
              "reasoning": "The code is generally efficient with minimal unnecessary LLM calls and an appropriate caching strategy. However, there are minor inefficiencies like uncapped retry delays and uncompiled regex patterns that could be optimized."
            },
            "testability": {
              "score": 60,
              "reasoning": "The code has potential for good test coverage, but the presence of `# pragma: no cover` comments suggests untested error paths. Edge cases like malformed JSON and empty choices are not adequately tested, and the interfaces could be clearer for mocking."
            },
            "documentation": {
              "score": 65,
              "reasoning": "The code lacks docstrings for private methods and could benefit from more inline comments for complex logic. Type hints are used but could be more precise, and variable/method names are generally clear but could be more descriptive in some cases."
            }
          },
          "temperature": 0.3,
          "cache_key": "9f93a9ebef20e6528e67cc6edc424849735a07f9603011139000ffda1d79dab3",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "cost": 0.0,
      "latency": 15583.33316698554,
      "utility": 0.43027491239804566,
      "workflow_output": "Well-structured LLM evaluation code with good error handling and async support. Generally follows best practices with clear separation of concerns. Main concerns are around error recovery, type safety, and some edge case handling in JSON parsing.\nIssues:\n- (high) Generic `Exception` catch in `_request_with_retry` may mask programming errors. Catches all exceptions including KeyboardInterrupt, SystemExit, etc. [line 169] Suggested fix: Catch specific exceptions from litellm (e.g., `litellm.APIError`, `litellm.RateLimitError`, `asyncio.TimeoutError`) instead of generic `Exception`. This allows genuine bugs to propagate.\n- (medium) Multiple `assert` statements used for type narrowing (lines 156, 169, 189). These are removed in optimized Python and provide no runtime guarantee. Suggested fix: Replace `assert` statements with explicit type checks and raise `EvaluationError` instead. Example: `if content is None: raise EvaluationError('Content should not be None')`\n- (medium) Cache key validation happens after cache backend check, but `build_cache_key()` is called without validating its return value. Could return empty string. [line 139] Suggested fix: Validate the result of `build_cache_key()` before using it. Add: `if not resolved_cache_key.strip(): raise EvaluationError('Cache key generation failed')`\n- (medium) `_extract_response_content()` handles both Mapping and object attribute access, but doesn't validate list indexing safety. `choices[0]` may fail if choices is empty. [line 204] Suggested fix: Add explicit length check: `if not choices or len(choices) == 0: raise EvaluationError('No choices in response')`\n- (low) Inconsistent error message formatting. Some use f-strings, others use string formatting with `%s`. Mix of styles reduces consistency. Suggested fix: Standardize on f-strings throughout for consistency and readability.\n- (medium) `_load_json()` markdown fence stripping is simplistic. Multi-line content with triple backticks in the middle could be incorrectly parsed. [line 213] Suggested fix: Use regex to match opening/closing fence pairs more robustly: `re.sub(r'^```(?:json)?\\n|\\n?```$', '', content, flags=re.MULTILINE)`\n- (low) `_parse_dimension_scores()` doesn't validate that `raw_score` exists before passing to `_normalize_score()`. Entry could have no 'score' key. [line 238] Suggested fix: Add explicit check: `if raw_score is None: raise EvaluationError(f'Dimension {name} missing score value')`\n- (low) Retry delay computation uses exponential backoff but doesn't cap maximum delay. With high max_retries, delays could become very large. [line 193] Suggested fix: Add a maximum delay cap: `return min(base_delay + jitter, self._max_retry_delay)` and accept `max_retry_delay` parameter in `__init__`.\n- (low) Magic strings in prompt building (system prompt, instructions) are hardcoded. Makes it difficult to customize evaluation behavior. Suggested fix: Consider moving system prompt and instruction templates to configurable class attributes or parameters for better extensibility.\n- (low) Type hints use `Optional[str]` but some parameters accept `None` and use truthiness checks without explicit None handling in all paths. Suggested fix: Be more explicit with None checks. Example: `if extra_instructions is not None and extra_instructions.strip():`\n- (low) No logging when cache key is provided but cache backend is None. User might not realize caching is disabled. [line 124] Suggested fix: Add debug log: `if cache_key is not None and cache_backend is None: self._logger.warning('Cache key provided but no cache backend configured')`\nSuggestions:\n- {'category': 'Error Recovery', 'priority': 'high', 'description': 'Implement exponential backoff with jitter more robustly. Consider using `tenacity` library for production-grade retry logic with better exception handling.'}\n- {'category': 'Testing', 'priority': 'high', 'description': \"Add unit tests for edge cases: empty choices, malformed JSON, out-of-range scores, markdown-wrapped JSON. The `# pragma: no cover` comment suggests external error paths aren't tested.\"}\n- {'category': 'Security', 'priority': 'medium', 'description': 'Add input length validation for `workflow_output` and `extra_instructions` to prevent prompt injection or excessive token usage.'}\n- {'category': 'Documentation', 'priority': 'medium', 'description': 'Add docstrings to private methods (`_build_prompt`, `_extract_response_content`, etc.) explaining expected formats and error conditions.'}\n- {'category': 'Observability', 'priority': 'medium', 'description': 'Add structured logging with context (e.g., evaluation ID, dimensions being scored) to improve debuggability in production.'}\n- {'category': 'Type Safety', 'priority': 'medium', 'description': 'Replace `Mapping[str, Any]` with `TypedDict` for `RubricDimension` and response payloads to enable better static type checking.'}\n- {'category': 'Code Quality', 'priority': 'low', 'description': 'Extract the JSON response template from `_build_prompt()` to a class constant for reusability and easier testing.'}\n- {'category': 'Performance', 'priority': 'low', 'description': 'Consider caching the compiled regex pattern in `_extract_max_score()` as a class-level constant to avoid recompilation on each call.'}",
      "metadata": {
        "strategy": "standard",
        "model": "openrouter/anthropic/claude-haiku-4.5",
        "temperature": 0.7,
        "context_size": "full_module",
        "language": "python",
        "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
        "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Consider surrounding module context for the review.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
        "analysis_raw": "```json\n{\n  \"summary\": \"Well-structured LLM evaluation code with good error handling and async support. Generally follows best practices with clear separation of concerns. Main concerns are around error recovery, type safety, and some edge case handling in JSON parsing.\",\n  \"issues\": [\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"high\",\n      \"description\": \"Generic `Exception` catch in `_request_with_retry` may mask programming errors. Catches all exceptions including KeyboardInterrupt, SystemExit, etc.\",\n      \"line_number\": 169,\n      \"suggestion\": \"Catch specific exceptions from litellm (e.g., `litellm.APIError`, `litellm.RateLimitError`, `asyncio.TimeoutError`) instead of generic `Exception`. This allows genuine bugs to propagate.\"\n    },\n    {\n      \"type\": \"Type Safety\",\n      \"severity\": \"medium\",\n      \"description\": \"Multiple `assert` statements used for type narrowing (lines 156, 169, 189). These are removed in optimized Python and provide no runtime guarantee.\",\n      \"line_number\": [156, 169, 189],\n      \"suggestion\": \"Replace `assert` statements with explicit type checks and raise `EvaluationError` instead. Example: `if content is None: raise EvaluationError('Content should not be None')`\"\n    },\n    {\n      \"type\": \"Input Validation\",\n      \"severity\": \"medium\",\n      \"description\": \"Cache key validation happens after cache backend check, but `build_cache_key()` is called without validating its return value. Could return empty string.\",\n      \"line_number\": 139,\n      \"suggestion\": \"Validate the result of `build_cache_key()` before using it. Add: `if not resolved_cache_key.strip(): raise EvaluationError('Cache key generation failed')`\"\n    },\n    {\n      \"type\": \"Robustness\",\n      \"severity\": \"medium\",\n      \"description\": \"`_extract_response_content()` handles both Mapping and object attribute access, but doesn't validate list indexing safety. `choices[0]` may fail if choices is empty.\",\n      \"line_number\": 204,\n      \"suggestion\": \"Add explicit length check: `if not choices or len(choices) == 0: raise EvaluationError('No choices in response')`\"\n    },\n    {\n      \"type\": \"Code Quality\",\n      \"severity\": \"low\",\n      \"description\": \"Inconsistent error message formatting. Some use f-strings, others use string formatting with `%s`. Mix of styles reduces consistency.\",\n      \"line_number\": [49, 75, 178, 182],\n      \"suggestion\": \"Standardize on f-strings throughout for consistency and readability.\"\n    },\n    {\n      \"type\": \"Edge Case\",\n      \"severity\": \"medium\",\n      \"description\": \"`_load_json()` markdown fence stripping is simplistic. Multi-line content with triple backticks in the middle could be incorrectly parsed.\",\n      \"line_number\": 213,\n      \"suggestion\": \"Use regex to match opening/closing fence pairs more robustly: `re.sub(r'^```(?:json)?\\\\n|\\\\n?```$', '', content, flags=re.MULTILINE)`\"\n    },\n    {\n      \"type\": \"Error Handling\",\n      \"severity\": \"low\",\n      \"description\": \"`_parse_dimension_scores()` doesn't validate that `raw_score` exists before passing to `_normalize_score()`. Entry could have no 'score' key.\",\n      \"line_number\": 238,\n      \"suggestion\": \"Add explicit check: `if raw_score is None: raise EvaluationError(f'Dimension {name} missing score value')`\"\n    },\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"low\",\n      \"description\": \"Retry delay computation uses exponential backoff but doesn't cap maximum delay. With high max_retries, delays could become very large.\",\n      \"line_number\": 193,\n      \"suggestion\": \"Add a maximum delay cap: `return min(base_delay + jitter, self._max_retry_delay)` and accept `max_retry_delay` parameter in `__init__`.\"\n    },\n    {\n      \"type\": \"Maintainability\",\n      \"severity\": \"low\",\n      \"description\": \"Magic strings in prompt building (system prompt, instructions) are hardcoded. Makes it difficult to customize evaluation behavior.\",\n      \"line_number\": [224, 247],\n      \"suggestion\": \"Consider moving system prompt and instruction templates to configurable class attributes or parameters for better extensibility.\"\n    },\n    {\n      \"type\": \"Type Safety\",\n      \"severity\": \"low\",\n      \"description\": \"Type hints use `Optional[str]` but some parameters accept `None` and use truthiness checks without explicit None handling in all paths.\",\n      \"line_number\": [100, 101],\n      \"suggestion\": \"Be more explicit with None checks. Example: `if extra_instructions is not None and extra_instructions.strip():`\"\n    },\n    {\n      \"type\": \"Logging\",\n      \"severity\": \"low\",\n      \"description\": \"No logging when cache key is provided but cache backend is None. User might not realize caching is disabled.\",\n      \"line_number\": 124,\n      \"suggestion\": \"Add debug log: `if cache_key is not None and cache_backend is None: self._logger.warning('Cache key provided but no cache backend configured')`\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"category\": \"Error Recovery\",\n      \"priority\": \"high\",\n      \"description\": \"Implement exponential backoff with jitter more robustly. Consider using `tenacity` library for production-grade retry logic with better exception handling.\"\n    },\n    {\n      \"category\": \"Testing\",\n      \"priority\": \"high\",\n      \"description\": \"Add unit tests for edge cases: empty choices, malformed JSON, out-of-range scores, markdown-wrapped JSON. The `# pragma: no cover` comment suggests external error paths aren't tested.\"\n    },\n    {\n      \"category\": \"Security\",\n      \"priority\": \"medium\",\n      \"description\": \"Add input length validation for `workflow_output` and `extra_instructions` to prevent prompt injection or excessive token usage.\"\n    },\n    {\n      \"category\": \"Documentation\",\n      \"priority\": \"medium\",\n      \"description\": \"Add docstrings to private methods (`_build_prompt`, `_extract_response_content`, etc.) explaining expected formats and error conditions.\"\n    },\n    {\n      \"category\": \"Observability\",\n      \"priority\": \"medium\",\n      \"description\": \"Add structured logging with context (e.g., evaluation ID, dimensions being scored) to improve debuggability in production.\"\n    },\n    {\n      \"category\": \"Type Safety\",\n      \"priority\": \"medium\",\n      \"description\": \"Replace `Mapping[str, Any]` with `TypedDict` for `RubricDimension` and response payloads to enable better static type checking.\"\n    },\n    {\n      \"category\": \"Code Quality\",\n      \"priority\": \"low\",\n      \"description\": \"Extract the JSON response template from `_build_prompt()` to a class constant for reusability and easier testing.\"\n    },\n    {\n      \"category\": \"Performance\",\n      \"priority\": \"low\",\n      \"description\": \"Consider caching the compiled regex pattern in `_extract_max_score()` as a class-level constant to avoid recompilation on each call.\"\n    }\n  ]\n}\n```",
        "test_config": {
          "test_number": 8,
          "config": {
            "temperature": 0.7,
            "context_size": "full_module",
            "generation_strategy": "standard",
            "model": "openrouter/anthropic/claude-haiku-4.5"
          }
        },
        "workflow": {
          "started_at": "2025-10-28T01:53:29.097086Z",
          "completed_at": "2025-10-28T01:53:44.680475Z",
          "duration_seconds": 15.58333316698554
        },
        "evaluation": {
          "rubric": {
            "functionality": {
              "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
              "scale": "0-100 points",
              "weight": 0.25
            },
            "error_handling": {
              "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "code_quality": {
              "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
              "scale": "0-100 points",
              "weight": 0.2
            },
            "performance": {
              "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
              "scale": "0-100 points",
              "weight": 0.15
            },
            "testability": {
              "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
              "scale": "0-100 points",
              "weight": 0.1
            },
            "documentation": {
              "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
              "scale": "0-100 points",
              "weight": 0.1
            }
          },
          "raw_response": {
            "functionality": {
              "score": 75,
              "reasoning": "The code generally accomplishes its intended purpose of evaluating workflow outputs, but there are notable issues with edge case handling, such as unvalidated list indexing and missing score checks. The normalization logic appears correct, but the default rubric could be improved with better type safety and validation."
            },
            "error_handling": {
              "score": 65,
              "reasoning": "Error handling is present but has significant gaps, such as catching generic exceptions and insufficient validation of inputs and outputs. Error messages are inconsistent, and logging could be improved for better debugging."
            },
            "code_quality": {
              "score": 70,
              "reasoning": "The code follows best practices like separation of concerns and type hints, but there are issues with type safety, inconsistent error message formatting, and the use of `assert` statements. The code could benefit from better abstractions and more explicit None checks."
            },
            "performance": {
              "score": 80,
              "reasoning": "The code is generally efficient with minimal unnecessary LLM calls and an appropriate caching strategy. However, there are minor inefficiencies like uncapped retry delays and uncompiled regex patterns that could be optimized."
            },
            "testability": {
              "score": 60,
              "reasoning": "The code has potential for good test coverage, but the presence of `# pragma: no cover` comments suggests untested error paths. Edge cases like malformed JSON and empty choices are not adequately tested, and the interfaces could be clearer for mocking."
            },
            "documentation": {
              "score": 65,
              "reasoning": "The code lacks docstrings for private methods and could benefit from more inline comments for complex logic. Type hints are used but could be more precise, and variable/method names are generally clear but could be more descriptive in some cases."
            }
          },
          "temperature": 0.3,
          "cache_key": "9f93a9ebef20e6528e67cc6edc424849735a07f9603011139000ffda1d79dab3",
          "cache_hit": false,
          "cache_recorded": true,
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "timestamp": "2025-10-28T01:53:50.292704Z"
    }
  ],
  "status": "COMPLETED",
  "started_at": "2025-10-28T01:50:42.102220Z",
  "completed_at": "2025-10-28T01:53:50.295232Z",
  "experiment_metadata": {
    "config_hash": "ccba6ec788fce577de4a40ac930b2581a4fc9e5d0011bb85b7f6154710f7c465",
    "created_at": "2025-10-28T01:50:42.102159Z",
    "dependencies": {
      "pydantic": "2.12.3",
      "langgraph": "1.0.1",
      "litellm": "1.79.0"
    },
    "non_deterministic_sources": [
      "llm_sampling"
    ]
  },
  "metadata": {
    "normalization": {
      "cost": {
        "min": 0.0,
        "max": 0.0
      },
      "latency": {
        "min": 7720.785707992036,
        "max": 22310.304207989248
      }
    }
  },
  "baseline_test_number": 1,
  "baseline_config": {
    "test_number": 1,
    "config_values": {
      "temperature": 0.3,
      "context_size": "file_only",
      "generation_strategy": "standard",
      "model": "openrouter/deepseek/deepseek-chat"
    },
    "workflow": "code_review"
  },
  "baseline_result": {
    "test_number": 1,
    "config": {
      "test_number": 1,
      "config_values": {
        "temperature": 0.3,
        "context_size": "file_only",
        "generation_strategy": "standard",
        "model": "openrouter/deepseek/deepseek-chat"
      },
      "workflow": "code_review"
    },
    "quality_score": {
      "dimension_scores": {
        "functionality": {
          "score": 0.85,
          "reasoning": "The code accomplishes its intended purpose and handles most edge cases effectively. However, the lack of validation for JSON responses and incomplete test coverage for certain edge cases slightly reduces the score."
        },
        "error_handling": {
          "score": 0.8,
          "reasoning": "The code includes proper exception handling and defensive programming, but it lacks validation for JSON responses and could benefit from more detailed logging and a timeout mechanism for API calls."
        },
        "code_quality": {
          "score": 0.9,
          "reasoning": "The code is well-structured, follows best practices, and maintains readability. There are minor areas for improvement, such as reducing method complexity and improving abstractions."
        },
        "performance": {
          "score": 0.75,
          "reasoning": "The code is generally efficient but could be optimized further, particularly in the retry mechanism's jitter calculation and caching strategy to reduce computational overhead."
        },
        "testability": {
          "score": 0.7,
          "reasoning": "The code has potential for good test coverage but lacks explicit test cases for certain edge cases, such as malformed JSON responses and cache key collisions."
        },
        "documentation": {
          "score": 0.85,
          "reasoning": "The code is well-documented with clear docstrings and inline comments. However, adding more examples and improving variable/method naming could enhance clarity further."
        }
      },
      "overall_score": 0.8083333333333332,
      "evaluator_model": "openrouter/deepseek/deepseek-chat",
      "timestamp": "2025-10-28T01:51:07.277619Z",
      "metadata": {
        "rubric": {
          "functionality": {
            "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
            "scale": "0-100 points",
            "weight": 0.25
          },
          "error_handling": {
            "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
            "scale": "0-100 points",
            "weight": 0.2
          },
          "code_quality": {
            "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
            "scale": "0-100 points",
            "weight": 0.2
          },
          "performance": {
            "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
            "scale": "0-100 points",
            "weight": 0.15
          },
          "testability": {
            "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
            "scale": "0-100 points",
            "weight": 0.1
          },
          "documentation": {
            "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
            "scale": "0-100 points",
            "weight": 0.1
          }
        },
        "raw_response": {
          "functionality": {
            "score": 85,
            "reasoning": "The code accomplishes its intended purpose and handles most edge cases effectively. However, the lack of validation for JSON responses and incomplete test coverage for certain edge cases slightly reduces the score."
          },
          "error_handling": {
            "score": 80,
            "reasoning": "The code includes proper exception handling and defensive programming, but it lacks validation for JSON responses and could benefit from more detailed logging and a timeout mechanism for API calls."
          },
          "code_quality": {
            "score": 90,
            "reasoning": "The code is well-structured, follows best practices, and maintains readability. There are minor areas for improvement, such as reducing method complexity and improving abstractions."
          },
          "performance": {
            "score": 75,
            "reasoning": "The code is generally efficient but could be optimized further, particularly in the retry mechanism's jitter calculation and caching strategy to reduce computational overhead."
          },
          "testability": {
            "score": 70,
            "reasoning": "The code has potential for good test coverage but lacks explicit test cases for certain edge cases, such as malformed JSON responses and cache key collisions."
          },
          "documentation": {
            "score": 85,
            "reasoning": "The code is well-documented with clear docstrings and inline comments. However, adding more examples and improving variable/method naming could enhance clarity further."
          }
        },
        "temperature": 0.3,
        "cache_key": "7c2e295193de5494bde9c44e4e685c75aefc4719e012ac359a42d255dd923654",
        "cache_hit": false,
        "cache_recorded": true,
        "model": "openrouter/deepseek/deepseek-chat"
      }
    },
    "cost": 0.0,
    "latency": 20200.36191600957,
    "utility": 0.5658333333333332,
    "workflow_output": "The code is well-structured and follows best practices in Python. It handles edge cases effectively, uses proper error handling, and is designed for readability and maintainability. However, there are some areas for improvement in terms of performance optimization, security, and testing coverage.\nIssues:\n- (medium) The retry mechanism uses exponential backoff with jitter, which is good, but the jitter calculation could be optimized to reduce unnecessary delays. [line 237] Suggested fix: Consider using a fixed jitter percentage or a more efficient randomization method to reduce computational overhead.\n- (medium) The code does not validate the content of the JSON response from the LLM, which could potentially lead to security vulnerabilities if the response is malformed or malicious. [line 276] Suggested fix: Implement additional validation to ensure the JSON response does not contain unexpected or harmful content.\n- (medium) The code lacks explicit test cases for edge cases such as malformed JSON responses, extreme temperature values, and cache key collisions. Suggested fix: Add comprehensive unit tests to cover these edge cases and ensure robustness.\nSuggestions:\n- {'description': 'Consider adding a timeout mechanism for the LLM API calls to prevent hanging indefinitely in case of network issues.', 'line_number': 194}\n- {'description': 'Add more detailed logging for cache operations to help with debugging and monitoring.', 'line_number': 156}\n- {'description': 'Refactor the `_extract_response_content` method to reduce complexity and improve readability.', 'line_number': 248}\n- {'description': 'Consider adding a method to validate the rubric dimensions to ensure they are correctly formatted before evaluation.', 'line_number': 284}",
    "metadata": {
      "strategy": "standard",
      "model": "openrouter/deepseek/deepseek-chat",
      "temperature": 0.3,
      "context_size": "file_only",
      "language": "python",
      "sample_code_path": "/Users/markr/Documents/aiwriting/TesseractFlow/tesseract_flow/evaluation/rubric.py",
      "analysis_prompt": "You are an expert python code reviewer following industry best practices.\nContext: Review limited to the provided file.\n\nReview the following code systematically across these dimensions:\n1. **Functionality**: Does it work correctly? Handle edge cases?\n2. **Code Quality**: Follow standards, naming conventions, design principles?\n3. **Readability**: Clear, maintainable, well-organized?\n4. **Error Handling**: Proper exceptions, validation, robustness?\n5. **Testing**: Testable design, adequate coverage?\n6. **Performance**: Efficient algorithms, no obvious bottlenecks?\n7. **Security**: Input validation, no vulnerabilities, safe practices?\n\n```python\n\"\"\"LLM-powered rubric evaluator for workflow outputs.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport math\nimport random\nimport re\nfrom typing import Any, Dict, Iterable, Mapping, Optional\n\nimport litellm\n\nfrom tesseract_flow.core.exceptions import EvaluationError\nfrom tesseract_flow.core.types import RubricDimension\n\nfrom .cache import CacheBackend, build_cache_key\nfrom .metrics import DimensionScore, QualityScore\n\n\nclass RubricEvaluator:\n    \"\"\"Evaluate workflow output quality using a rubric and LiteLLM.\"\"\"\n\n    DEFAULT_MODEL = \"openrouter/anthropic/claude-haiku-4.5\"\n    DEFAULT_TEMPERATURE = 0.3\n    DEFAULT_RUBRIC: Dict[str, RubricDimension] = {\n        \"clarity\": {\n            \"description\": \"Is the output clear and understandable?\",\n            \"scale\": \"0-100 where 0=incomprehensible, 100=crystal clear\",\n        },\n        \"accuracy\": {\n            \"description\": \"Is the output factually accurate?\",\n            \"scale\": \"0-100 where 0=many errors, 100=fully accurate\",\n        },\n        \"completeness\": {\n            \"description\": \"Does the output address all requirements?\",\n            \"scale\": \"0-100 where 0=missing major parts, 100=comprehensive\",\n        },\n        \"usefulness\": {\n            \"description\": \"Is the output actionable and useful?\",\n            \"scale\": \"0-100 where 0=not useful, 100=highly actionable\",\n        },\n    }\n\n    def __init__(\n        self,\n        *,\n        model: Optional[str] = None,\n        temperature: float = DEFAULT_TEMPERATURE,\n        logger: Optional[logging.Logger] = None,\n        cache: Optional[CacheBackend] = None,\n        use_cache: bool = False,\n        record_cache: bool = False,\n        max_retries: int = 3,\n        retry_base_delay: float = 1.0,\n    ) -> None:\n        self._model = (model or self.DEFAULT_MODEL).strip()\n        if not self._model:\n            msg = \"Evaluator model identifier must be non-empty.\"\n            raise ValueError(msg)\n\n        self._temperature = self._validate_temperature(temperature)\n        self._logger = logger or logging.getLogger(__name__)\n        self._cache = cache\n        self._default_use_cache = bool(use_cache)\n        self._default_record_cache = bool(record_cache)\n        if max_retries < 1:\n            msg = \"max_retries must be at least 1.\"\n            raise ValueError(msg)\n        if retry_base_delay <= 0:\n            msg = \"retry_base_delay must be greater than zero.\"\n            raise ValueError(msg)\n        self._max_retries = max_retries\n        self._retry_base_delay = retry_base_delay\n\n    async def evaluate(\n        self,\n        workflow_output: str,\n        rubric: Optional[Dict[str, RubricDimension]] = None,\n        *,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        extra_instructions: Optional[str] = None,\n        use_cache: Optional[bool] = None,\n        record_cache: Optional[bool] = None,\n        cache_key: Optional[str] = None,\n    ) -> QualityScore:\n        \"\"\"Evaluate workflow output using the provided rubric.\"\"\"\n\n        if not workflow_output or not workflow_output.strip():\n            msg = \"Workflow output must be a non-empty string for evaluation.\"\n            raise EvaluationError(msg)\n\n        rubric_definition = rubric or self.DEFAULT_RUBRIC\n        selected_model = (model or self._model).strip()\n        if not selected_model:\n            msg = \"Model identifier must be non-empty.\"\n            raise EvaluationError(msg)\n        selected_temperature = self._validate_temperature(\n            temperature if temperature is not None else self._temperature\n        )\n\n        effective_use_cache = self._default_use_cache if use_cache is None else bool(use_cache)\n        effective_record_cache = (\n            self._default_record_cache if record_cache is None else bool(record_cache)\n        )\n        cache_backend = self._cache\n        requested_cache = effective_use_cache or effective_record_cache or cache_key is not None\n        if requested_cache and cache_backend is None:\n            msg = \"Caching requested but no cache backend configured.\"\n            raise EvaluationError(msg)\n\n        prompt = self._build_prompt(workflow_output, rubric_definition, extra_instructions)\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        resolved_cache_key = None\n        if cache_backend is not None:\n            resolved_cache_key = (cache_key or build_cache_key(prompt, selected_model, selected_temperature)).strip()\n            if not resolved_cache_key:\n                msg = \"Cache key must be a non-empty string.\"\n                raise EvaluationError(msg)\n\n        cache_hit = False\n        recorded_cache = False\n        content: Optional[str] = None\n\n        if cache_backend is not None and effective_use_cache:\n            assert resolved_cache_key is not None\n            cached = cache_backend.get(resolved_cache_key)\n            if cached is not None:\n                cache_hit = True\n                content = cached\n                self._logger.debug(\"Cache hit for rubric evaluation with key %s\", resolved_cache_key)\n\n        if content is None:\n            response = await self._request_with_retry(\n                messages=messages,\n                model=selected_model,\n                temperature=selected_temperature,\n            )\n\n            content = self._extract_response_content(response)\n\n            if cache_backend is not None and effective_record_cache:\n                assert resolved_cache_key is not None\n                cache_backend.set(resolved_cache_key, content)\n                recorded_cache = True\n                self._logger.debug(\n                    \"Recorded rubric evaluation response to cache key %s\", resolved_cache_key\n                )\n\n        assert content is not None  # for type-checkers\n        parsed_payload = self._load_json(content)\n        dimension_scores = self._parse_dimension_scores(parsed_payload, rubric_definition)\n\n        quality = QualityScore(\n            dimension_scores=dimension_scores,\n            evaluator_model=selected_model,\n        )\n        metadata = {\n            \"rubric\": rubric_definition,\n            \"raw_response\": parsed_payload,\n            \"temperature\": selected_temperature,\n        }\n        if resolved_cache_key is not None:\n            metadata.update(\n                {\n                    \"cache_key\": resolved_cache_key,\n                    \"cache_hit\": cache_hit,\n                }\n            )\n            if recorded_cache:\n                metadata[\"cache_recorded\"] = True\n        return quality.with_metadata(**metadata)\n\n    async def _request_with_retry(\n        self,\n        *,\n        messages: Iterable[Mapping[str, Any]],\n        model: str,\n        temperature: float,\n    ) -> Any:\n        \"\"\"Call LiteLLM with exponential backoff and jitter.\"\"\"\n\n        attempt = 1\n        payload = list(messages)\n        last_error: Optional[Exception] = None\n        while attempt <= self._max_retries:\n            try:\n                self._logger.debug(\n                    \"Rubric evaluation attempt %s/%s with model %s\", attempt, self._max_retries, model\n                )\n                return await litellm.acompletion(\n                    model=model,\n                    messages=payload,\n                    temperature=temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n            except Exception as exc:  # pragma: no cover - LiteLLM errors are external\n                last_error = exc\n                if attempt >= self._max_retries:\n                    break\n                delay = self._compute_retry_delay(attempt)\n                self._logger.warning(\n                    \"Rubric evaluation attempt %s failed (%s). Retrying in %.2fs...\",\n                    attempt,\n                    exc,\n                    delay,\n                )\n                await asyncio.sleep(delay)\n                attempt += 1\n\n        assert last_error is not None  # for type-checkers\n        self._logger.exception(\n            \"Rubric evaluation failed after %s attempts: %s\", self._max_retries, last_error\n        )\n        raise EvaluationError(\n            f\"LLM evaluation request failed after {self._max_retries} attempts.\"\n        ) from last_error\n\n    def _compute_retry_delay(self, attempt: int) -> float:\n        base_delay = self._retry_base_delay * (2 ** (attempt - 1))\n        jitter = random.uniform(0, base_delay * 0.25)\n        return base_delay + jitter\n\n    def _build_prompt(\n        self,\n        workflow_output: str,\n        rubric: Mapping[str, RubricDimension],\n        extra_instructions: Optional[str],\n    ) -> str:\n        rubric_text = self._format_rubric(rubric.items())\n        instructions = \"\\n\" + extra_instructions.strip() if extra_instructions else \"\"\n        return (\n            \"You are an impartial expert evaluator.\"\n            \" Assess the workflow output using the rubric.\"\n            \" Think step-by-step before scoring each dimension.\"\n            \"\\n\\nOUTPUT TO EVALUATE:\\n\" + workflow_output.strip() +\n            \"\\n\\nRUBRIC:\\n\" + rubric_text +\n            \"\\nINSTRUCTIONS:\\n\"\n            \"1. Reason carefully about each dimension.\\n\"\n            \"2. Provide concise reasoning referencing the rubric.\\n\"\n            \"3. Assign a score following the specified scale.\\n\"\n            \"4. Respond ONLY in JSON with numeric scores.\"\n            \"\\n\\nJSON RESPONSE TEMPLATE:\\n\"\n            \"{\\n\"\n            \"  \\\"dimension_name\\\": {\\n\"\n            \"    \\\"score\\\": <number>,\\n\"\n            \"    \\\"reasoning\\\": \\\"<why you chose the score>\\\"\\n\"\n            \"  }, ...\\n\"\n            \"}\"\n            f\"{instructions}\\n\"\n        )\n\n    def _system_prompt(self) -> str:\n        return (\n            \"You are a meticulous and unbiased reviewer.\"\n            \" Provide honest assessments and avoid revealing deliberation summaries.\"\n        )\n\n    def _format_rubric(self, dimensions: Iterable[tuple[str, RubricDimension]]) -> str:\n        lines = []\n        for name, metadata in dimensions:\n            lines.append(\n                f\"- {name}: {metadata['description']} (Scale: {metadata['scale']})\"\n            )\n        return \"\\n\".join(lines)\n\n    def _extract_response_content(self, response: Any) -> str:\n        choices = None\n        if isinstance(response, Mapping):\n            choices = response.get(\"choices\")\n        else:\n            choices = getattr(response, \"choices\", None)\n\n        if not choices:\n            msg = \"Evaluator response did not include any choices.\"\n            raise EvaluationError(msg)\n\n        first_choice = choices[0]\n        message = (\n            first_choice.get(\"message\")\n            if isinstance(first_choice, Mapping)\n            else getattr(first_choice, \"message\", None)\n        )\n        if message is None:\n            msg = \"Evaluator response choice missing message content.\"\n            raise EvaluationError(msg)\n\n        content = (\n            message.get(\"content\")\n            if isinstance(message, Mapping)\n            else getattr(message, \"content\", None)\n        )\n\n        if isinstance(content, list):\n            combined = \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, Mapping))\n            content = combined\n\n        if not isinstance(content, str):\n            msg = \"Evaluator response content must be a string.\"\n            raise EvaluationError(msg)\n        return content\n\n    def _load_json(self, content: str) -> Mapping[str, Any]:\n        # Strip markdown code fences if present (some models wrap JSON in ```json ... ```)\n        stripped = content.strip()\n        if stripped.startswith(\"```\"):\n            # Remove opening fence (e.g., \"```json\" or \"```\")\n            lines = stripped.split(\"\\n\", 1)\n            if len(lines) > 1:\n                stripped = lines[1]\n            # Remove closing fence\n            if stripped.endswith(\"```\"):\n                stripped = stripped[:-3].rstrip()\n\n        try:\n            return json.loads(stripped)\n        except json.JSONDecodeError as exc:\n            msg = \"Evaluator response was not valid JSON.\"\n            raise EvaluationError(msg) from exc\n\n    def _parse_dimension_scores(\n        self,\n        payload: Mapping[str, Any],\n        rubric: Mapping[str, RubricDimension],\n    ) -> Dict[str, DimensionScore]:\n        scores: Dict[str, DimensionScore] = {}\n        for name in rubric:\n            entry = payload.get(name)\n            if entry is None:\n                msg = f\"Evaluator response missing score for dimension '{name}'.\"\n                raise EvaluationError(msg)\n            if isinstance(entry, Mapping):\n                raw_score = entry.get(\"score\")\n                reasoning = entry.get(\"reasoning\")\n            else:\n                raw_score = entry\n                reasoning = None\n\n            # Extract max score from rubric scale (e.g., \"0-20 points\" -> 20)\n            max_score = self._extract_max_score(rubric[name].get(\"scale\", \"1-10\"))\n            normalized_score = self._normalize_score(raw_score, max_score)\n            scores[name] = DimensionScore(score=normalized_score, reasoning=reasoning)\n        return scores\n\n    def _extract_max_score(self, scale: str) -> float:\n        \"\"\"Extract maximum score from scale string like '0-100 points' or '1-10'.\"\"\"\n        # Try to find a pattern like \"X-Y\" where Y is the max\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)', scale)\n        if match:\n            return float(match.group(2))\n        # Default to 100 if we can't parse the scale\n        return 100.0\n\n    def _normalize_score(self, raw_score: Any, max_score: float = 100.0) -> float:\n        try:\n            value = self._coerce_numeric(raw_score)\n        except (TypeError, ValueError) as exc:\n            msg = \"Dimension score must be a numeric value.\"\n            raise EvaluationError(msg) from exc\n\n        # Clamp to valid range (LLMs sometimes give scores slightly outside bounds)\n        if value < 0.0:\n            self._logger.warning(\n                \"Evaluator gave negative score %.2f (clamping to 0). Max scale was %.2f.\",\n                value,\n                max_score,\n            )\n            value = 0.0\n        elif value > max_score:\n            self._logger.warning(\n                \"Evaluator gave score %.2f exceeding max %.2f (clamping to max).\",\n                value,\n                max_score,\n            )\n            value = max_score\n\n        # Normalize to 0-1 range\n        return value / max_score\n\n    def _coerce_numeric(self, value: Any) -> float:\n        if isinstance(value, (int, float)) and math.isfinite(value):\n            return float(value)\n        if isinstance(value, str):\n            stripped = value.strip()\n            if not stripped:\n                msg = \"Dimension score string cannot be empty.\"\n                raise ValueError(msg)\n            numeric = float(stripped)\n            if not math.isfinite(numeric):\n                msg = \"Dimension score must be finite.\"\n                raise ValueError(msg)\n            return numeric\n        msg = \"Unsupported score type.\"\n        raise TypeError(msg)\n\n    def _validate_temperature(self, value: float) -> float:\n        if not 0.0 <= value <= 1.0:\n            msg = \"Temperature must be between 0.0 and 1.0.\"\n            raise ValueError(msg)\n        return float(value)\n```\n\nReturn JSON with:\n- 'summary': Brief overview of code quality\n- 'issues': Array of objects with {type, severity (low/medium/high/critical), description, line_number (optional), suggestion}\n- 'suggestions': Array of actionable improvements\n\nFocus on impactful issues. Be specific and constructive.",
      "analysis_raw": "```json\n{\n  \"summary\": \"The code is well-structured and follows best practices in Python. It handles edge cases effectively, uses proper error handling, and is designed for readability and maintainability. However, there are some areas for improvement in terms of performance optimization, security, and testing coverage.\",\n  \"issues\": [\n    {\n      \"type\": \"Performance\",\n      \"severity\": \"medium\",\n      \"description\": \"The retry mechanism uses exponential backoff with jitter, which is good, but the jitter calculation could be optimized to reduce unnecessary delays.\",\n      \"line_number\": 237,\n      \"suggestion\": \"Consider using a fixed jitter percentage or a more efficient randomization method to reduce computational overhead.\"\n    },\n    {\n      \"type\": \"Security\",\n      \"severity\": \"medium\",\n      \"description\": \"The code does not validate the content of the JSON response from the LLM, which could potentially lead to security vulnerabilities if the response is malformed or malicious.\",\n      \"line_number\": 276,\n      \"suggestion\": \"Implement additional validation to ensure the JSON response does not contain unexpected or harmful content.\"\n    },\n    {\n      \"type\": \"Testing\",\n      \"severity\": \"medium\",\n      \"description\": \"The code lacks explicit test cases for edge cases such as malformed JSON responses, extreme temperature values, and cache key collisions.\",\n      \"suggestion\": \"Add comprehensive unit tests to cover these edge cases and ensure robustness.\"\n    }\n  ],\n  \"suggestions\": [\n    {\n      \"description\": \"Consider adding a timeout mechanism for the LLM API calls to prevent hanging indefinitely in case of network issues.\",\n      \"line_number\": 194\n    },\n    {\n      \"description\": \"Add more detailed logging for cache operations to help with debugging and monitoring.\",\n      \"line_number\": 156\n    },\n    {\n      \"description\": \"Refactor the `_extract_response_content` method to reduce complexity and improve readability.\",\n      \"line_number\": 248\n    },\n    {\n      \"description\": \"Consider adding a method to validate the rubric dimensions to ensure they are correctly formatted before evaluation.\",\n      \"line_number\": 284\n    }\n  ]\n}\n```",
      "test_config": {
        "test_number": 1,
        "config": {
          "temperature": 0.3,
          "context_size": "file_only",
          "generation_strategy": "standard",
          "model": "openrouter/deepseek/deepseek-chat"
        }
      },
      "workflow": {
        "started_at": "2025-10-28T01:50:42.106193Z",
        "completed_at": "2025-10-28T01:51:02.306527Z",
        "duration_seconds": 20.20036191600957
      },
      "evaluation": {
        "rubric": {
          "functionality": {
            "description": "Does the code accomplish its intended purpose correctly?\n- RubricEvaluator properly evaluates workflow outputs\n- Handles all edge cases (malformed LLM responses, missing scores, etc.)\n- Score extraction logic is robust\n- Normalization logic is correct (0-1 range)\n- Default rubric is well-designed\n",
            "scale": "0-100 points",
            "weight": 0.25
          },
          "error_handling": {
            "description": "Does the code handle errors and edge cases gracefully?\n- Appropriate exception handling (EvaluationError)\n- Defensive programming (validate inputs)\n- Clear error messages with context\n- Handles LLM failures gracefully\n- Proper logging for debugging\n",
            "scale": "0-100 points",
            "weight": 0.2
          },
          "code_quality": {
            "description": "Is the code well-designed and maintainable?\n- Clear separation of concerns\n- Single Responsibility Principle\n- Appropriate use of type hints\n- Clean abstractions (no leaky abstractions)\n- Proper use of Pydantic for validation\n- Good class/method naming\n",
            "scale": "0-100 points",
            "weight": 0.2
          },
          "performance": {
            "description": "Is the code efficient and performant?\n- Minimal unnecessary LLM calls\n- Efficient regex patterns\n- Appropriate caching strategy\n- No obvious performance bottlenecks\n- Reasonable token usage\n",
            "scale": "0-100 points",
            "weight": 0.15
          },
          "testability": {
            "description": "Is the code easy to test and well-tested?\n- Clear interfaces for mocking\n- Testable methods (pure functions where possible)\n- Good test coverage potential\n- Easy to reproduce edge cases\n- Clear contracts (inputs/outputs)\n",
            "scale": "0-100 points",
            "weight": 0.1
          },
          "documentation": {
            "description": "Is the code well-documented and clear?\n- Clear docstrings for public APIs\n- Inline comments for complex logic\n- Type hints are accurate\n- Examples where helpful\n- Clear variable/method names\n",
            "scale": "0-100 points",
            "weight": 0.1
          }
        },
        "raw_response": {
          "functionality": {
            "score": 85,
            "reasoning": "The code accomplishes its intended purpose and handles most edge cases effectively. However, the lack of validation for JSON responses and incomplete test coverage for certain edge cases slightly reduces the score."
          },
          "error_handling": {
            "score": 80,
            "reasoning": "The code includes proper exception handling and defensive programming, but it lacks validation for JSON responses and could benefit from more detailed logging and a timeout mechanism for API calls."
          },
          "code_quality": {
            "score": 90,
            "reasoning": "The code is well-structured, follows best practices, and maintains readability. There are minor areas for improvement, such as reducing method complexity and improving abstractions."
          },
          "performance": {
            "score": 75,
            "reasoning": "The code is generally efficient but could be optimized further, particularly in the retry mechanism's jitter calculation and caching strategy to reduce computational overhead."
          },
          "testability": {
            "score": 70,
            "reasoning": "The code has potential for good test coverage but lacks explicit test cases for certain edge cases, such as malformed JSON responses and cache key collisions."
          },
          "documentation": {
            "score": 85,
            "reasoning": "The code is well-documented with clear docstrings and inline comments. However, adding more examples and improving variable/method naming could enhance clarity further."
          }
        },
        "temperature": 0.3,
        "cache_key": "7c2e295193de5494bde9c44e4e685c75aefc4719e012ac359a42d255dd923654",
        "cache_hit": false,
        "cache_recorded": true,
        "model": "openrouter/deepseek/deepseek-chat"
      }
    },
    "timestamp": "2025-10-28T01:51:07.278025Z"
  },
  "baseline_quality": 0.8083333333333332,
  "quality_improvement_pct": 0.0
}