# TesseractFlow Experiment: Production Code Review for rubric.py
# Generated by: tesseract-experiment-designer skill
# Created: 2025-10-26
# Target: tesseract_flow/evaluation/rubric.py (core evaluator module)

name: "rubric_module_production_review"
workflow: "code_review"

# Taguchi L8 Variables - Testing model quality vs cost trade-offs
variables:
  # Variable 1: Temperature (determinism vs edge case detection)
  - name: "temperature"
    level_1: 0.3  # Very deterministic, consistent feedback
    level_2: 0.6  # Balanced, may catch more edge cases

  # Variable 2: Model (cost vs quality for production code)
  - name: "model"
    level_1: "openrouter/anthropic/claude-haiku-4.5"   # Fast, cost-effective: $3/M tokens
    level_2: "openrouter/anthropic/claude-sonnet-4.5"  # Premium quality: $9/M tokens

  # Variable 3: Context Size (focused vs comprehensive)
  - name: "context_size"
    level_1: "file_only"      # Just rubric.py - faster, cheaper
    level_2: "full_module"    # Include evaluation/__init__.py, metrics.py, cache.py

  # Variable 4: Generation Strategy (direct vs reasoning)
  - name: "generation_strategy"
    level_1: "standard"          # Direct code review prompting
    level_2: "chain_of_thought"  # Reasoning-based review (often better quality)

# Utility Function Weights - Quality is paramount for production code
utility_weights:
  quality: 1.0    # Most important - this is critical infrastructure
  cost: 0.05      # Somewhat important - willing to pay for quality
  time: 0.02      # Least important - thoroughness > speed

# Workflow Configuration
workflow_config:
  # Production-grade rubric for code review (0-100 scale)
  rubric:
    # 1. Functionality & Correctness (25% weight)
    functionality:
      description: |
        Does the code accomplish its intended purpose correctly?
        - RubricEvaluator properly evaluates workflow outputs
        - Handles all edge cases (malformed LLM responses, missing scores, etc.)
        - Score extraction logic is robust
        - Normalization logic is correct (0-1 range)
        - Default rubric is well-designed
      scale: "0-100 points"
      weight: 0.25

    # 2. Error Handling & Robustness (20% weight)
    error_handling:
      description: |
        Does the code handle errors and edge cases gracefully?
        - Appropriate exception handling (EvaluationError)
        - Defensive programming (validate inputs)
        - Clear error messages with context
        - Handles LLM failures gracefully
        - Proper logging for debugging
      scale: "0-100 points"
      weight: 0.20

    # 3. Code Quality & Design (20% weight)
    code_quality:
      description: |
        Is the code well-designed and maintainable?
        - Clear separation of concerns
        - Single Responsibility Principle
        - Appropriate use of type hints
        - Clean abstractions (no leaky abstractions)
        - Proper use of Pydantic for validation
        - Good class/method naming
      scale: "0-100 points"
      weight: 0.20

    # 4. Performance & Efficiency (15% weight)
    performance:
      description: |
        Is the code efficient and performant?
        - Minimal unnecessary LLM calls
        - Efficient regex patterns
        - Appropriate caching strategy
        - No obvious performance bottlenecks
        - Reasonable token usage
      scale: "0-100 points"
      weight: 0.15

    # 5. Testing & Testability (10% weight)
    testability:
      description: |
        Is the code easy to test and well-tested?
        - Clear interfaces for mocking
        - Testable methods (pure functions where possible)
        - Good test coverage potential
        - Easy to reproduce edge cases
        - Clear contracts (inputs/outputs)
      scale: "0-100 points"
      weight: 0.10

    # 6. Documentation & Clarity (10% weight)
    documentation:
      description: |
        Is the code well-documented and clear?
        - Clear docstrings for public APIs
        - Inline comments for complex logic
        - Type hints are accurate
        - Examples where helpful
        - Clear variable/method names
      scale: "0-100 points"
      weight: 0.10

  # Code to review
  sample_code_path: "tesseract_flow/evaluation/rubric.py"
  language: "python"

  # Additional context files (for full_module context level)
  context_files:
    - "tesseract_flow/evaluation/__init__.py"
    - "tesseract_flow/evaluation/metrics.py"
    - "tesseract_flow/evaluation/cache.py"
    - "tesseract_flow/core/types.py"
    - "tesseract_flow/core/exceptions.py"

  # Evaluator Configuration - Use Haiku 4.5 for consistent, fast evaluation
  evaluator_model: "openrouter/anthropic/claude-haiku-4.5"
  evaluator_temperature: 0.3  # Deterministic evaluation

  # Context descriptions
  context_descriptions:
    file_only: "Review limited to rubric.py only - focused analysis."
    full_module: "Review considering full evaluation module context and dependencies."

  # Generation strategy descriptions
  strategy_descriptions:
    standard: "Direct code review with standard prompting."
    chain_of_thought: "Step-by-step reasoning approach to code review analysis."

# Expected Costs (for planning)
# Assumptions:
# - rubric.py: ~400 lines = ~2,000 input tokens
# - Review output: ~1,500 output tokens
# - Full module context: ~3,000 additional input tokens
# - Evaluation: ~3,500 input + 500 output tokens
# - Total per test: ~6,000-10,000 tokens depending on context

# Model Pair Costs (per L8):
# - Haiku 4.5 + Haiku 4.5: $0.24 (file_only) to $0.36 (full_module)
# - Sonnet 4.5 + Haiku 4.5: $0.60 (file_only) to $0.84 (full_module)
# - Mixed (4 Haiku, 4 Sonnet): ~$0.40-$0.60

# Recommended: Mixed approach - Total L8 cost: ~$0.50

# Expected Outcomes:
# 1. Temperature: Learn if 0.6 catches more issues than 0.3 for production code
# 2. Model: Validate if Sonnet 4.5 quality justifies 3x cost for code review
# 3. Context: Determine if full module context improves review depth/accuracy
# 4. Strategy: Test if chain-of-thought enhances code review quality
